---
layout: post
title:  "Yet another list of cybersecurity and devsecops questions #yalcdq"
date:   2018-11-30 14:09:28 -0500
author: David Xiao
categories: cybersecurity devops devsecops
tags:  cybersecurity devsecops
---

A few weeks ago, I decided to put together some of the cybersecurity questions I was asked over the past few years, so that I can come back and review the questions and answers from time to time. This post is my initial deliverable.

By no means I am suggesting this post is complete and comprehensive, as my intention is to advise on security basics. Whenever reference was provided, I did my best to give credits, but if you believe that I missed something, just let me know.

As always, it is highly appreciated if you would share your comments, suggestions or corrections.


### SSH and TLS

They both run on top of TCP/IP. But the protocols differ in many ways. To name a few:

**Authentication**

SSH supports multiple authentication methods, most common ones are password and publickey. Another method called keyboard-interactive is used less.

TLS supports both server authentication (more common) and client authentication. When both are used, it is called TLS mutual authentication. TLS authentication typically involves requesting digital certificate from the other end and validating the certificate against the host.

**Functionality**

SSH protocol provides extra functionalities such as user authentication, tunneling. The functionality of the SSH transport layer alone is comparable to TLS. [wikipedia] See SSH protocol layers.

**Port**

The well known port for SSH is 22. TLS protocol itself does not specify any port number, so it is up to the application to specify a port, e.g. HTTPS typically uses port 443.


### SFTP

SFTP stands for Secure FTP. SFTP by itself does not provide authentication and security; it expects the underlying protocol to secure this. SFTP is most often used as subsystem of SSH.

### Some commonly known ports and services

It is a typical interview question for pentest related positions. If metasploit scans over 200 commonly known ports for discovery, you need to know at least a few of them:

DNS  53 tcp/udp

FTP 21 (command port)

RDP 3389 tcp/udp

SSH 22

SFTP - SSH File Transfer Protocol  22

FTPS - FTP extension with SSL/TLS   990 *not to be confused with FTP over SSH, which is the practice of tunneling FTP through an SSH connection*

NTP 123 tcp

SNMP  161 udp 162 udp

SMB 445,139 tcp   137 udp 

SMTP(sending) 25 465 587

POP3(retrieving) 110

IMAP(retrieving) 143 993 

Postgres  5432

MS SQL Server 1433

### SSH protocol layers

SSH protocol has an internal architecture with several layers, including:

The transport layer (RFC 4253). Key exchange method, public key algorithm, symmetric encryption algorithm, message authentication algorithm, and hash algorithm are all negotiated. This is also where session key is negotiated. 

The user authentication layer. It supports methods such as password, publickey etc.

The connection layer. It provides the ability to multiplex many secondary sessions into a single SSH connection. Consider local port forwarding, remote port forwarding etc.

### Why publickey method is considered better over password in SSH

For one reason, password method can be compromised by MTLM (man in the middle attack) in some case. 

Consider this situation: client failed to authenticate the server properly so that an attacker managed to trick the client into connecting to attacker's server. 

If password method is used, it's pretty much game over at that point. 

But in publickey method, client sends signature that is encrypted by its privatekey, the session identifier is encrypted by signature. The attacker has no way to extract the session identifier from the signature without knowing the client's publickey, and if the attacker attempts to trick the server into using the attacker's own session identifier by forging and sending its own signature, that will also fail when server is verifying the signature against the client's public key which was pre-shared out-of-band.

### How is SSH connection encrypted?

At the beginning of a session, symmetric encryption algorithm is negotiated, and a session key is also negotiated using Diffie-Hellman. The session key is then used to encrypt the entire session. [Understanding the SSH Encryption and Connection Process](https://www.digitalocean.com/community/tutorials/understanding-the-ssh-encryption-and-connection-process?utm_content=understanding-the-ssh-encryption-and-connection-process)

It is worth noting that in a SSH session, establishment of such a secure channel happens BEFORE user authentication.


### Diffie-Hellman

This [Youtube video][diffie hellman video] from Khan Academy explains how it works.

In a nutshell, Diffie-Hellman provides a solution to an essential cryptographic requirement: To allow two parties who have never met to agree on a shared secret key.

There is an important distinction in terms of shared secret: The two parties are not sharing an scret key during the communication, they are creating a secret key together by agreeing on it.

Even if an attacker observes and records the communication, the secret can't be seen by the attacker.

[diffie hellman video](https://youtu.be/YEBfamv-_do?list=PL87386AD236727A1B){:target="_blank"}

Elliptic Curve Diffie–Hellman (ECDH) is a variation of DH which provides better security. Latest OpenSSH has started using ECDH. See more details at [weakdh.org](https://weakdh.org/){:target="_blank"} and [wikipedia](https://en.wikipedia.org/wiki/Logjam_(computer_security)){:target="_blank"}

### Forward secrecy

Forward secrecy is a feature of specific key agreement protocols that gives assurances your session keys will not be compromised even if the private key of the server is compromised. 

Key exchange, hash, MAC, asymmetric and symmetric enyption all play their own part altogether in the effort of providing FS capabilities .

<https://en.wikipedia.org/wiki/Forward_secrecy>

### Screened host architecture and screened subnet architecture 

First off, the following diagrams illustrate the different architectures of the two.

![alt text](/pic/screened host.png)

Screened host architecture (credit: internet)

![alt text](/pic/screened subnet.png)

Screened Subnet Architecture (credit: internet)

**Advantages with screened subnet architecture**

- It adds an extra layer of security by adding a perimeter network that further isolates the internal network from the Internet. Bastion hosts are considered more vulnerable on the network, therefore by isolating the bastion host(s) on a perimeter network, it reduces the impact of a break-in on the bastion host. The break-in gives an intruder some access, but not full access.

**Disadvantages with screened host architecture**

- Layered security wise, if an attacker manages to break in to the bastion host, there is nothing left in the way of network security between the bastion host and the rest of the internal hosts;
- The interior router presents a single point of failure: if the router is compromised, the entire network is available to an attacker.


### Hash, MAC and Digital Signature

I found the following table best in describing the differences in a concise way. [[source]][hash mac]

[hash mac]: https://crypto.stackexchange.com/a/5647 


| Cryptographic security goal | Hash | MAC            | Digital signature |
|-----------------------------|------|----------------|-------------------|
| Integrity                   | Yes  | Yes            | Yes               |
| Authentication              | No   | Yes            | Yes               |
| Non-repudiation             | No   | No             | Yes               |
|-----------------------------|------|----------------|-------------------|
| Kind of keys                | none | symmetric keys | asymmetric keys   |


### Hardware MAC spoofing

This wikpedia page explains MAC spoofing. <https://en.wikipedia.org/wiki/MAC_spoofing>{:target="_blank"} 

A few takeways:

- Hardware MAC is 48 bit(6 bytes) long, the first 3 bytes indicate the manufacturer

- Hardware MAC should not be considered globally unique, given that it can be changed on various levels

- Privacy concerns. A device may be passively tracked by its Wifi MAC address. Randomizing the MAC address was indended to address the concern, but articles such as [A Study of MAC Address Randomization in Mobile Devices and When it Fails](https://arxiv.org/abs/1703.02874){:target="_blank"} argue that MAC address randomization policies are neither universally implemented nor effective at eliminating such concerns

### Message Authentication Code MAC and HMAC

HMAC is one type of MAC that uses hash algorithm as part of the algorithm set.

`HMAC(key, message) = H(key | H(key | message))`

- H is a cryptographic hash function
- Key is the secret key
- Message is the message to be authenticated
- | denotes concatenation

The following Java code shows a great example of code

```java
import javax.crypto.SecretKeyFactory;
public static byte[] HmacSHA256(String data, byte[] key) throws Exception {
    String algorithm=”HmacSHA256";
    Mac mac = Mac.getInstance(algorithm);
    SecretKeySpec secretKey = new SecretKeySpec(key, algorithm); 
    mac.init(secretKey);
    return mac.doFinal(data.getBytes(“UTF8”));
}
```

### Google Authenticator

Based on RFC 6238 and RFC 4226, [Google authenticator](<https://en.wikipedia.org/wiki/Google_Authenticator>){:target="_blank"} uses TOTP - time based one time password. The service provider generates an 80-bit secret key for each user (whereas RFC 4226 requires 128 bits and recommends 160 bits). This is provided as a 16, 26 or 32 character base32 string or as a QR code. The client creates an HMAC-SHA1 using this secret key. The message that is HMAC-ed can be:

- the number of 30-second periods having elapsed since the Unix epoch (TOTP), or the counter that is incremented with each new code (HOTP)

- A portion of the HMAC is extracted, converted to a six-digit code and returned to user

Note: Many other 2FA software tokens support the same RFC and therefore provide similiar services. 

### Name a few 'good' and 'not so good' cryptographic algorithms and protocols

Some 'not so good' ones:

- SSL: all versions

- TLS: v1.1 and lower

- SNMP: any version lower than v3. SNMP v1 public community string is considered big security hole 

- Hash algorithm: MD5, SHA1, RC4

- Symmetric algorithm: DES, 3DES

- Asymmetric algorithm: DSS, DSA (both are invented by NSA)

- Block cipher mode: ECB is a no-no. CBC in some cases are not good. [Wikipedia](https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation){:target="_blank"} explains why ECB and CBC are not advisable cipher mode.

- TLS cipher suite: CBC mode is considered weaker than GCM mode regardless of the cipher size

Now let's look at a couple good examples:

- TLS cipher suite TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384. This is the Safari browser current version's first choice of cipher suite in 'Client Hello'. The suite says ECDHE is key agreement algorithm, ECDSA is asymmetric algorithm, AES_256_GCM is symmetric algorithm, and SHA-2 384 is hash algorithm. 

- TLS cipher suite TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384. This is Chrome browser's first choice. The only difference between Chrome and Safari is asymmetric algorithtm.


### Name a few data breaches that went public lately

They are just too many LOL. The key words here are "went public", since there are many breaches that either gone unnoticed or have not gone public just yet. For starters, here is a list of known [data breaches][data breach]. In addition, Verizon data breach investigation report is also worth reading. Be advised that there are many reports published by both government and private sectors that discuss certain breaches and cyber attacks in greater detail at various levels.

[data breach]:https://en.wikipedia.org/wiki/List_of_data_breaches

### Cyber Kill Chain

aka [cyber intrusion chain](https://en.wikipedia.org/wiki/Kill_chain), is a framework developed by Lockheed-Martin. It intends to reveal the phases of a typical cyber attack: 7 steps from early reconnaissance to the goal of data exfiltration. The framework is used by organizations to define phases of a cyber attack.

### SSH port forwarding and tunneling

It is widely used on bastion hosts (a.k.a. jump server). There is a [youtube video](https://youtu.be/JKrO5WABdoY) that explains SSH local port forwarding and remote port forwarding.

### Bastion host

a.k.a. jump server, it is typically deployed in the DMZ or perimeter network so that it is exposed to internet.

* Single point of failure, given there is only one bastion host for a specific service
* Performance compromise. Bastion host itself can be the bottleneck of the end-to-end performance in terms of bandwidth, network latency etc.


### TCP handshake steps

To establish a connection, the 3-step handshake occurs: SYN, SYN-ACK, ACK.

Step 1. Client sends a TCP segment with SYN = 1, ACK = 0, ISN (Initial Sequence Number) = 2000, a randomly chosen number.

Step 2. Server receives client's TCP segment and returns a TCP segment with SYN = 1, ACK = 1, ISN = 5000, server's randomly chosen Initial Sequence Number, Acknowledgment Number = 2001 (2000 + 1)

Step 3. Client sends a TCP segment to server that acknowledges receipt of server's ISN, with flags set as SYN = 0, ACK = 1, sequence number = 2001, Acknowledgment number = 5001 (5000 + 1)

### Forward proxy and Reverse proxy

Reverse proxy. A common use-case is setting the reverse proxy up as the TLS gateway so you can communicate via HTTP behind the firewall. Be advised that some load balancers can act as TLS termination as well, such as AWS ELB.

Failover for non-responding servers is another application of reverse proxy.

### SSL inspection and TLS inspection

SSL inspection (aka TLS inspection, SSL/TLS decryption, SSL analysis, or deep packet inspection). The technology is typically used to intercept and inspect HTTPS traffic at enterprise level.

There are research articles such as [The Security Impact of HTTPS Interception](https://jhalderm.com/pub/papers/interception-ndss17.pdf){:target="_blank"} arguing that SSL Inspection is doing more harm than good. 

### SSL termination proxy and TLS termination proxy

to be added

### SAML

To be added

### OAuth 2.0 and OpenID Connect (OIDC)

To be added.

### Authentication and Authorization

To be added.

### Compiled language and script language

To be added.

### Stateful firewall and stateless firewall

To be added.

### Name a few layers and typical applications in OSI model

To be added.

### DDoS

[Denial of Service Attack Mitigation on AWS](https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/)

[AWS Best Practices for DDoS Resiliency - Whitepaper](https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf)

### What protection WAF can provide

WAF is a protocol layer 7 defense , and is not designed to defend against all types of attacks. This method of attack mitigation is usually part of a suite of tools which together create a holistic defense against a range of attack vectors.

By deploying a WAF in front of a web application, a shield is placed between the web application and the Internet. While a proxy server protects a client machine's identity by using an intermediary, a WAF is a type of reverse proxy, protecting the server from exposure by having requests pass through the WAF before reaching the server.

A WAF operates through a set of rules often called policies. These policies aim to protect against vulnerabilities in the application by filtering out malicious traffic. 

WAF helps protect web applications by filtering and monitoring HTTP traffic between a web application and the Internet. It typically protects web applications from attacks such as cross-site forgery, cross-site-scripting (XSS), file inclusion, and SQL injection, among others. See OWASP topics in this post for more detail.

The value of a WAF comes in part from the speed and ease with which policy modification can be implemented, allowing for faster response to varying attack vectors; during a DDoS attack, rate limiting can be quickly implemented by modifying WAF policies.

<https://www.cloudflare.com/learning/ddos/glossary/web-application-firewall-waf/>


### IDS and IPS

The Intrusion Detection System (IDS) watches and listens, then logs an alert, but unlike IPS, an IDS does nothing to stop the traffic from continuing on its way, in other words, network traffic is not dependent on the IDS working to flow properly. IDS works by taking a SPAN port or network tap, so it does not present a risk of single point of failure. 

Intrusion Prevention System (IPS) executes real-time responses to active attacks and violations. An IPS is the same as an IDS but with some sort of active defense. Since traffic flows through the IPS, network traffic can be stopped, block and reported when IPS decided it is violating policies, rules, security, etc. 

IPS are positioned behind firewalls and provide an additional layer of security by scanning and analyzing suspicious content for potential threats. Placed in the direct communication path, an IPS will take automatic action on suspicious traffic within the network. 

IPS has its own challenges such as having too many false positives and causing a disruptive user experience. Or the parameters sometimes results in an IPS generating a large number of alerts that can't be responded to effectively.

### FISMA and FedRAMP

[FISMA](https://www.dhs.gov/fisma){:target="_blank"}, or "Federal Information Security Modernization Act", is a law enacted in 2002 and updated in 2014, that mandates a process to strengthen the security posture of US government’s information systems.

When most agencies and their vendors discuss being “FISMA compliant”, they are
usually referring to meeting the controls identified in NIST SP 800-53, “Recommended Security Controls for Federal Information Systems”.

[FedRAMP](https://www.fedramp.gov/){:target="_blank"} is a result of the ”Cloud First” policy issued in 2010 requiring the use of FedRAMP authorized cloud services by agencies in an effort to reduce costs and to streamline the IT procurement process. This
policy requires that government agencies move IT services to cloud solutions. FedRAMP has been developed as a program for CSPs to receive an independent security assessment from Third Party Assessment Organizations (3PAOs).

The ["Cloud Smart" policy](https://cloud.cio.gov/strategy/) is released in 2018 as an amendment to "Cloud First".

Since these assessments are also based on NIST SP 800-53 Rev 4, FedRAMP can be thought of a “FISMA for the cloud” as it inherits the NIST baseline of controls and is tailored for cloud computing initiatives.

FedRAMP defines 4 security baselines, High, Medium, Low, LI-SaaS (low impact - SaaS) and each has its own security controls selected from NIST 800-53.

| FedRAMP security baseline | Number of controls|
|-----|---|
| High | 421 |
| Medium | 325 |
| Low | 125 |
| LI-SaaS (low impact SaaS)| 38 |


### RDBMS and NoSQL

tbc

AWS DynamoDB

AWS Mongodb

### Describe typical NoSQL storage

To be added. JSON format / in memory 

[https://www.devbridge.com/articles/benefits-of-nosql/](https://www.devbridge.com/articles/benefits-of-nosql/) 


### Durability and availability for data storage

Availability and durability are two different aspects of data accessibility. Availability refers to system uptime, i.e. the storage system is operational and can deliver data upon request. 

Durability concerns more about long-term data protection, i.e. the stored data does not suffer from bit rot, degradation or other corruption.


### NIST SP-800

NIST Special Publication is a series of guidelines, technical spec, recommendations reference model for all U.S. federal information systems. Worth noting ones include SP 800-53 Security and Privacy Controls for Federal Information Systems and Organizations

### HIPAA

Health Insurance Portability and Accountability Act. it protects protected health information, known as PHI which is individually identifiable health information.

### Two factor authentication or 2FA

Here is some commonly used 2FA methods and security concerns:

- SMS 2FA
   - For privacy reason and abuse concern, user may be relectant to give out phone number
   - By design security flaws in the [SS7 telephony protocol](https://en.wikipedia.org/wiki/Signalling_System_No._7#Protocol_security_vulnerabilities){:target="_blank"}. Wired has covered [an indicent](https://www.wired.com/2017/05/fix-ss7-two-factor-authentication-bank-accounts/){:target="_blank"} in 2017. 
   - Phone number takeover attack
   - Service provider needs to protect phone numbers 

- Authenticator App / TOTP 2FA
   - If your phone dies or gets stolen, and you don’t have printed backup codes or a saved copy of the original QR code, you can lose access to your account

- Push-based 2FA
   - Like Apple’s Trusted Devices method, can send a prompt to one of your devices during login. This prompt will indicate that someone (possibly you) is trying to log in, and an estimated location for the login attempt
   - It’s not standardized, so you can’t choose from a variety of authenticator apps, and can’t consolidate all your push-based credentials in a single app. Also, it requires a working data connection on your phone

- [Universal Second Factor (U2F)](https://en.wikipedia.org/wiki/Universal_2nd_Factor)
   - Relatively new, typically using small USB or NFC called “security keys”. Once set up, the site will prompt you to connect your device and tap it to allow the login.
   - The main downsides of U2F are support and cost. The standard is hosted by FIDO Alliance. The W3C is working on further standardizing the U2F protocol for the web, which should lead to further adoption. 
   - Products like [yubikey](https://www.yubico.com/) are available on the market

### 2FA and security questions

Security questions alone is "what you know" factor, thus it is by nature less secure than 2FA.

### PKI

A public key infrastructure is a set of roles, policies, and procedures needed for create, manage, distribute, use, store & revoke digital certificates and manage public-key encryption.

A PKI is an arrangement that binds public keys with respective identities of entities (like people and organizations). 

* Certificate authority (CA). The binding is established through a process of registration and issuance of certificates at and by a CA. It may be carried out by an automated process or under human supervision.

* Registration authority (RA). It assumes the role of assuring valid and correct registration. An RA is responsible for accepting requests for digital certificates and authenticating the entity making the request.

* Validation authority (VA). An entity must be uniquely identifiable within each CA domain on the basis of information about that entity. VA can provide this entity information on behalf of the CA.

### GDPR

A bunch of AWS related GRPR resources here. More to add.

[AWS compliance against GDPR](https://aws.amazon.com/compliance/gdpr-center/>) and [whitepaper](https://d1.awsstatic.com/whitepapers/compliance/GDPR_Compliance_on_AWS.pdf)

[AWS security blogs](https://aws.amazon.com/blogs/security/tag/gdpr/)




### SOC1, SOC2, SOC3

Simply put, SOC1 SOC2 and SOC3 are reports issued by auditors for service organizations. SOC 2 report focuses on non-financial reporting controls as they relate to security, availability, processing integrity, confidentiality, and privacy, as opposed to SOC 1/SSAE 18 which is focused on the financial reporting controls. SOC3 is a simplified form of SOC2.

**Type 1 and type 2**

Type 1 report provides a report of procedures / controls an organization has put in place as of *a point in time.*

Type 2 report has an audit period and provides evidence of how an organization operated its controls *over a period of time.* It describes how a company's control environment operated over its audit period. During the reporting period, if there were significant changes, the before and after both need to be tested in SOC2 case, but only the after needs to be tested if it is in SOC1 case.


### PCI DSS

Here are a couple things that you should know about:

**Compliance level and QSA** 

There are 4 compliance levels. 

Level 1 is defined as merchat that has greater than 6,000,000 Mastercard or Visa transactions annually, OR, a merchant that has experienced an attack resulting in compromised card data, OR, a merchant deemed level 1 by a card association.

For level 1 compliance, the merchant is required to have yearly assessments of compliance by a Qualified Security Assessor (QSA), in addition to the requirements for levels 2, 3, and 4.

Levels 2, 3 and 4 all have the same validation requirements - yearly self-assessment, a quarterly network scan by an approved scanning vendor, and an attestation of compliance form.

**CHD**, **SAD**, **CDE**

Cardhold data environment(CDE) is the systems that processes, stores, transmits CHD or SAD. It also includes any component that directly connects to or supports this network. See more detail at [PCI DSS data storage](https://www.pcisecuritystandards.org/pdfs/pci_fs_data_storage.pdf)

|CHD|SAD|
|---|---|
|Cardholder data|Sensitive authentication data|
|Primary Account Number or PAN, cardholder name, expiration date|full magnetic stripe data, CAV/CVV2, and PIN|
|allowed to be stored|NOT allowed to be stored|

**Network segmentation**

Since it is critical to effectively manage the scope of CDE in reality, Network segmentation is a best practice to significantly reduce the scope of the PCI environment. As reference, PCI DSS [mandated](https://www.pcisecuritystandards.org/documents/Guidance-PCI-DSS-Scoping-and-Segmentation_v1.pdf) the scoping concepts as follows:

> Systems located within the CDE are in scope, irrespective of the functionality or the reason why they are in the CDE
>
> Systems that connect to a system in the CDE are in scope
>
> In a flat network, all systems are in scope if any one single system stores, processes, or transmits account data

**AWS and PCI DSS**

AWS has provided a compliance checklist which provides a number of accelerators by mapping PCI-DSS requirements to AWS services and functionalities.

### Stage 1 and stage 2 in an ISO 27001 audit

The certification audit will be conducted by an independent certification body (selected by you), and consists of ‘Stage 1’ and ‘Stage 2’ audits. 

*Documentation review* is the Stage 1 audit, whereby the auditor does a high-level review of your ISMS and establishes whether the internal audit programme is in place.  Stage 1 is completed on-site to determine whether your ISMS has met the minimum requirements of the Standard and is ready for a certification audit. The auditor will point out any areas of nonconformity and potential improvements of the management system.

*Certification audit* is the Stage 2 audit. The auditor will conduct a thorough assessment to establish whether the organisation’s ISMS is compliant with the ISO 27001 standard and seek evidence that the organisation is following the policies and procedures in practice. The auditor will review their audit checklists and provide feedback to the client regarding any nonconformities.


### What is segmentation in PCI-DSS And what is the advantage of that?

To be added.


### Defense in depth

To be added.

### Security by obscurity

To be added.


### Pentest and Red team

To be added.


### What is container

To be added.


### CIS Benchmarks and hardening

To be added.


### Type 1 and type 2 hypervisor in virtualization

Type 1 runs on hardware directly. So called "bare metal". e.g. VMware ESX, Citrix Xen

Type 2 runs on a host OS that provides virtualization services such as I/O and memory management. E.g. VMware workstation

### NIST Cybersecurity Framework

Five high level functions: identify, protect, detect, respond, recover.

### AWS security

Read AWS whitepapers pertaining to security. Visit the [website](https://aws.amazon.com/whitepapers/#security) for latest publications. 

[AWS Security Workshops](https://awssecworkshops.com) follow the NIST Cybersecurity Framework and provide some useful materials.


### AWS Security Hub

It is a security and compliance check tool across multiple AWS accounts.
<https://aws.amazon.com/security-hub/>

### Amazon GuardDuty

Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise. GuardDuty also detects potentially compromised instances or reconnaissance by attackers.

<https://aws.amazon.com/guardduty/>


### Amazon Inspector 

Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. After performing an assessment, Amazon Inspector produces a detailed list of security findings prioritized by level of severity.

Amazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. 

The basic building blocks of Inspector include Assessment template, rules packages, inspector agent and so force. 

<https://aws.amazon.com/inspector/>

### What is Federation

<https://aws.amazon.com/blogs/security/aws-federated-authentication-with-active-directory-federation-services-ad-fs/> 

<https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html>

Use case: Windows AD, ADFS, AWS 

ADFS acts as broker in this use case.

Prior to the following use case, configuration steps are required in AD, ADFS, and AWS. 

AWS SAML identity provider (IdP) can be used to establish trust between AWS and ADFS. One of the steps is to determine IAM Role Naming Convention for User Access. In a federated authentication scenario, users (as defined in the IdP) assume an AWS role during the sign-in process. 

![alt text](/pic/aws federation.png)

### AWS IAM

[Policy Evaluation Logic](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html?icmpid=docs_iam_console#policy-eval-basics){:target="_blank"}

[AWS IAM delegates access](Understanding the API Options for Securely Delegating Access to Your AWS Account){:target="_blank"}

<https://aws.amazon.com/blogs/security/understanding-the-api-options-for-securely-delegating-access-to-your-aws-account/>

> GetFederationToken. The resulting permissions inherit the permissions of the caller, scoped down by the permissions attached in the request (if you don’t attach permissions, the resulting permissions will be *deny*).  The actual permissions associated with the session are the intersection of the caller’s base permissions and the permissions attached as an API parameter.
> 
> Unlike GetFederationToken, AssumeRole sessions derive their permissions from the role policies that you’ve pre-defined, scoped down by the optional permissions attached in the request.  The actual permissions associated with the session are the intersection of the role’s permissions and the permissions attached as an API parameter. Only an IAM user or another role with permissions to call AssumeRole can assume a role.
>
> It’s important to understand that using either GetFederationToken or AssumeRole, the permissions are evaluated each time an AWS API call is made.  That means that even though you cannot revoke the session, you can always modify the permissions associated with a session even after the session has been issued.  Simply modify the permissions on the IAM user (for GetFederationToken) or IAM role (for AssumeRole), and the permissions on the session will automatically be affected as well.

### AWS KMS and CloudHSM

KMS: managed service. Based on multi tenant infrastructure. 

CloudHSM: dedicated hardware. 

KMS: master keys. Only used to encrypt and decrypt up to 4KB data, use it to generate, encrypt and decrypt the data keys. It is so called envelope encryption <https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html>

3 types of CMK: customer managed MK (typical use). AWS managed CMKs (owned by an AWS services on your behalf), AWS owned MKs, AWS itself owns and for use in multiple AWS account

Master key never goes out of KMS unencrypted. 

Data keys are generated by master key but it is not managed, stored or tracked in KMS. 

Use case

Create a data key

To create a data key, AWS KMS uses the CMK that you specify to generate a data key. The operation returns a plaintext copy of the data key and a copy of the data key encrypted under the CMK, as shown in the following image. 

Encrypt the data

Client uses the plaintext data key to encrypt data, and ditch the plaintext key but retain the encrypted data key alongside the encrypted data.

Decrypt the data

Client requests the plaintext data key by authenticating through KMS alongside the encrypted data key, the KMS returns the plaintext data key.


### AWS auto scaling group

To be added.


### DNS and AWS Route 53

To be added.

This is a 30,000 feet view of how DNS works

![](/pic/aws dns.png)

hosted zone: On Name server for example.com, it creates records that point out to the individual servers within the domain.

DNS Records (NS, A, MX, CNAME)

The most widely used DNS record types, and their purpose are as follows:

A - specifies IP addresses corresponding to your domain and its subdomains;

NS - lists which name servers can answer lookups on a DNS zone;

MX - specifies where the emails for your domain should be delivered;

CNAME - Canonical name record, is a type of resource record which maps one domain name to another. CNAME records must always point to another domain name, never directly to an IP address.

DNAME - A DNAME record creates an alias for an entire subtree of the domain name tree. In contrast, the CNAME record creates an alias for a single name and not its subdomains. Like the CNAME record, the DNS lookup will continue by retrying the lookup with the new name.

Alias - In AWS you can create alias records for ELB, S3 etc.

Use case

CNAME can prove convenient when running multiple services (like an FTP server and a web server) on a single IP address. One can have two CNAME records for ftp.example.com and www.example.com both pointing to the DNS entry for example.com, which in turn has an A record which points to the IP address. Then, if the IP address ever changes, one only has to record the change in one place within the network: in the DNS A record for example.com.

Advantages of managed DNS such as AWS route 53:

*   Redundant locations
*   LBR, Geo, Failover
*   Managed via APIs

This [youtube video](https://www.youtube.com/watch?v=e2xLV7pCOLI) from AWS covers some basics of DNS and Route 53. 

### AWS ELB and ALB

Amazon offers two types of load balancers, each with its own strengths. 

Classic load balancers, called ELB *(elastic load balancer)* operates at Layer 4 - transport layer, and is controlled by the protocol being used to transmit the request. It reads the protocol and port of the incoming request, and then routes it to one or more backend servers. ELB supports SSL Termination and pass-thru.

In contrast, the ALB *(application load balancer)* operates at Layer 7. Whereas a request to a specific URL backed by a Classic ELB would only enable routing to a particular pool of homogeneous */ˌhōməˈjēnēəs/* servers, the ALB offers the ability to route requests based on their content, can route based on the content of the UR and direct to a specific subgroup of backing servers existing in a heterogeneous */ˌhedərəˈjēnēəs/* collection registered with the load balancer.

If your environment consists of clearly defined services which can each be mapped to a specific address, then the Classic ELB is the logical choice.

Termination of Idle Sessions
Completion of Requests to Instances In The Process Of Being Marked Unhealthy.


### Cloud Custodian and Anchore

To be added.


### AWS VPCs and subnets 

Security groups supports only Allow rules.

[https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html) 

VPC Flow Logs shows the egress and ingress traffic logs. You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in the VPC or subnet is monitored. A flow log record represents a network flow in your flow log. Each record captures the network flow for a specific 5-tuple, for a specific capture window, such as IP address, port, protocol, time etc.


### AWS security group and Network ACL

Security groups supports only Allow rules.

Network ACL supports Allow and Deny rules.

By default, a security group includes an outbound rule that allows all outbound traffic.

The default network ACL comes with VPC by default allows all inbound and outbound IPv4 traffic. New custom network ACL is deny all by default. A subnet can be associated with only one network ACL at a time. A network ACL contains a numbered list of rules that we evaluate in order, starting with the lowest numbered rule.

Security Group is stateful, e.g. responses to allowed inbound traffic are allowed to flow out, regardless of outbound rules. 

Network ACL is stateless, responses to allowed inbound traffic are subject to the rules for outbound traffic.

Security groups are tied to an instance.

Network ACL are tied to the subnet. 


### Blockchain technology 

Oct 2018. NIST. [NISTIR 8202 Blockchain Technology Overview](https://nvlpubs.nist.gov/nistpubs/ir/2018/NIST.IR.8202.pdf) explains blockchain in some detail there. In the report there is also a flowchart that helps one determine whether a blockchain may be needed for a development initiative.
![](/pic/nist blockchain.png)

If you know a good article regarding technical implementation of blockchain, please feel free to share it with the author.


### OWASP Top 10 and AWS WAF

[OWASP top 10](https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project) for web application security represents a broad consensus about the most critical security risks to web applications.

|OWASP Top 10|
|-------|
|A1 – Injection, especially SQL injection|
|A2 – Broken Authentication and Session Management|
|A3 – Cross-Site Scripting (XSS)|
|A4 – Broken Access Control|
|A5 – Security Misconfiguration|
|A6 – Sensitive Data Exposure|
|A7 – Insufficient Attack Protection|
|A8 – Cross-Site Request Forgery (CSRF)|
|A9 – Using Components with Known Vulnerabilities|
|A10 – Underprotected API|


In AWS OWASP and AWS WAF context, AWS has published a whitepaper in regards to [using AWS WAF to Mitigate OWASP’s Top 10 Web Application Vulnerabilities][aws waf owasp]. 

From an automation perspective, you can leverage other AWS services to build comprehensive security automations. A set of such tools is available on our website in the form of the [AWS WAF Security Automations](https://aws.amazon.com/answers/security/aws-waf-security-automations/). 

Threat Modeling is related to OWASP and application security.

**A1 – Injection, especially SQL injection**

AWS WAF can match and mitigate SQL injection attacks. More sophisticated attack detection require additional work at app level.

**A2 – Broken Authentication and Session Management** 

AWS WAF takes on a reactive role by blocking the known stolen tokens, because illicit requests with stolen authorization credentials or tokens are hard to distinguish from legitimate ones. 

**A3 – Cross-Site Scripting (XSS)** 

For a use case, consider the example of a blog that accepts user comments. When user comments are are sent to the browser without proper sanitization, malicious user can embed a malicious script in the comments and that code then gets executed anytime a legitimate user loads that same blog article. 

XSS attacks are relatively easy to mitigate since they require specific key HTML tag names in the HTTP request.

**A4 – Broken Access Control** 

WAF can be effective against certain types of the attack, such as to filter dangerous HTTP request patterns that can indicate [path traversal attack](https://en.wikipedia.org/wiki/Directory_traversal_attack) and [remote and local file inclusion (RFI/LFI)](https://en.wikipedia.org/wiki/File_inclusion_vulnerability)

**A5 – Security Misconfiguration** 

It can happen at any level of your stack. e.g. It can apply to the OS, middleware, platform services, application code, or database layers of your application. A few examples such as leaving default directory listings enabled on production web servers, Leaving the Apache web server exposing the exact versions of the web server and associated modules in any server-generated responses. 

In AWS WAF, you should consider:
* block access to the paths to administrative consoles, configuration, or status pages that are installed or enabled by default.
* restrict access to trusted source IP addresses, if they’re in use. 
* define match patterns to protect against known attack patterns that are specific to your platform, especially if you have legacy applications that rely on old platform behavior.

**A6 – Sensitive Data Exposure** 

HTTP requests that can lead to sensitive data exposure have detectable patterns, you can mitigate them by using string match conditions that target those patterns. WAF searches the first 8 KB of the HTTP request body or less. 
* WAF is not effective against encryption protocols and ciphers that are used at the connection level. Other AWS services may be helpful. e.g. ELB security policies can mandate protocols must be used, CloudFront can set minimum SSL protocol

**A7 – Insufficient Attack Protection** 

Malicious actors are able to adapt their toolsets quickly to exploit new vulnerabilities and launch large-scale automated attacks to detect vulnerable systems. This category focuses strongly on your ability to react in a timely manner to new attack vectors and abnormal request patterns, or to application flaws that are discovered.
* Use AWS WAF to enforce a level of hygiene for inbound HTTP requests. Size constraint conditions help you build rules that ensure that components of HTTP requests fall within specifically defined ranges. You can use them to avoid processing abnormal requests. 
* Scanner and probe mitigation. Malicious sources scan and probe internet-facing web applications for vulnerabilities. They send a series of requests that generate HTTP 4xx error codes. You can use this history to help identify and block IP addresses from malicious sources. This solution creates an AWS Lambda function that automatically parses access logs, counts the number of bad requests from unique source IP addresses, and updates AWS WAF to block further scans from those addresses.
* Known attacker origin mitigation. A number of organizations maintain reputation lists of IP addresses that are operated by known attackers, such as spammers, malware distributors, and botnets. 

**A8 – Cross-Site Request Forgery (CSRF)** 

Consider any URL path and HTTP request that is intended to cause a state change (for example, form submission requests). Are there any mechanisms in place to ensure the user intended to take that action? Without such mechanisms, there isn’t an effective way to determine whether the request is legitimate and wasn’t forged by a malicious party. Depending solely on client-side attributes, such as session tokens or source IP addresses, isn’t an effective strategy because malicious actors can manipulate and replicate these values.

CSRF attacks take advantage of the fact that all details of a particular action are predictable (form fields, query string parameters).

* WAF can check and verify if unique id is included in HTTP requests. As an example, the server sends a simple form to the client browser along with the embedded unique token as a hidden field. At the same time, it retains in the current server-side session store the token value it expects the browser to supply when the user submits the form. After the user submits the form, a POST request is made to the server that includes the unique hidden token. The server can safely discard any POST requests that don’t contain the expected value for the supplied session. It should clear the value from the session store after it’s used up, which ensures that the value doesn’t get reused.

**A9 – Using Components with Known Vulnerabilities**

Most web applications are highly composed. They use frameworks and libraries from a variety of sources, commercial or open source. The primary mechanism to mitigate known vulnerabilities in components is to have a comprehensive process in place that addresses the lifecycle of such components. You should have a way to identify and track the dependencies of your application and the dependencies of the underlying components. Also, you should have a monitoring process in place to track the security of these components.
* You can use AWS WAF to filter and block HTTP requests to functionality of such components that you aren’t using in your applications. This helps reduce the attack surface of those components if vulnerabilities are discovered in functionality you’re not using.
* Block requests that attempts to access server-side components, e.g. attempt to access `/include/` directory where the application has its PHP source code files

Note from the author: Software Composition Analysis (SCA) is another set of tool that you may want to look into. 

**A10 – Underprotected API** 

Attack vectors for APIs are often the same as for traditional web applications, the mitigation mechanisms discussed throughout this document also apply to APIs in a similar manner. You can use AWS WAF in a variety of ways to mitigate these different attack vectors.
Here is another great [AWS blog post](https://aws.amazon.com/blogs/compute/protecting-your-api-using-amazon-api-gateway-and-aws-waf-part-i/) that discuss protecting API using API Gateway and WAF

[aws waf owasp]: https://d0.awsstatic.com/whitepapers/Security/aws-waf-owasp.pdf

### Equifax data breach 

[The U.S. government](https://www.gao.gov/products/GAO-18-559) released a report in Sept. 2018 that discussed the breach in detail.

[This post](https://castraconsulting.com/GAO-Equifax-Breach-Report) shared a few takeaway as follows:

> The report provides the most detail such as how the attack started on the Equifax dispute portal servers, how they gained access to 51 databases and extracted data from them for over 76 days in small increments to avoid detection
> 
> The report details that the attackers used network scanning tools on March 10, 2017, to detect and confirm the vulnerability in the Apache Struts Web Framework software only 2 days after the vulnerability was described and published by the US CERT group
> 
> It took another 2 months after this vulnerability was confirmed by the attackers before they began to actively work on exploiting what they had gained access to, initially gaining access to the 3 dispute portal-related databases and extending their reach to 48 other databases. Over the course of the breach, the attackers ran ~9,000 queries. Had the network at Equifax been more segmented, the attackers would not have been able to get to so many other databases and widen the scope of the breach
> 
> Attackers were able to find clear-text credentials to gain access to additional databases on the systems they originally breached, which allowed them to run their queries on additional databases. Had better data governance procedures been in place, credentials would have been encrypted and sensitive information more restricted, further limiting the scope of the breach.
> 
> Equifax admitted that because of an expired digital certificate on the affected systems, their Intrusion Detection System failed to work properly and did not detect the attackers activities until the digital certificate was updated – had they maintained their digital certificates, the initial attacker activity would have been detected much earlier.
> 
> It's comforting to know that it took 1 day for the attack to be detected after the system administrator updated the expired digital certificate, and only 2 days after that for the company to formally engage the FBI to assist on August 2, 2017. It appears that their Incident Response Procedures were implemented and carried out properly.
> 
> We know so much now about the scope and details of the attack because the attackers thankfully did not erase the system logs that recorded their commands and activity.


### URL reputation protection 

tbc

### AWS ARN

*offtopic:* There is a rather generic requirement in many domains: The need to uniquely identify an individual entity among others with no ambiguity. Different approaches have been devised (and re-invented over and over again) to fulfil it in their respective domain. To name a few, social security number, credit number, phone number, GUID, URI, IP address etc. ARN is an approach that Amazon uses within the AWS world.

Amazon Resource Names (ARNs) uniquely identify AWS resources. A typical ARN is string that consists of a number of values that concatenated with each other following a pattern as follows: `arn:partition:service:region:account-id:resource` 

**partition**

The partition that the resource is in. For standard AWS regions, the partition is aws. If you have resources in other partitions, the partition is aws-partitionname. For example, the partition for resources in the China (Beijing) region is aws-cn.

**service**

The service namespace that identifies the AWS product (for example, Amazon S3, IAM, or Amazon RDS). For a list of namespaces, see AWS Service Namespaces.

**region**

The region the resource resides in. Note that the ARNs for some resources do not require a region, so this component might be omitted.

**account-id**

The ID of the AWS account that owns the resource, without the hyphens. For example, 123456789012. Note that the ARNs for some resources don't require an account number, so this component might be omitted.

**resource**

The content of this part of the ARN varies by service, such as `resourcetype:resource`, or `resourcetype/resource`.


### Security challenges when moving on-prem workloads to cloud

cloud transformation
TBC

### AWS Account Strategy 

[AWS Account Structure — Strategies for Enterprises](https://medium.com/stax-blog/aws-account-structure-strategies-for-enterprises-a6bae76be487)

> Organisations typically need a multi-account strategy to support the requirements of running their AWS ecosystem. It's important to setup the accounts in a consistent fashion at the start, to ensure simple management and tracking of many AWS accounts in the future.


### AWS Service Control Policies (SCP) 

SCPs are similar to IAM permission policies and use almost the exact same syntax. However, a SCP never grants permissions. Instead, think of an SCP as a filter that enables you to restrict what service and actions can be accessed by users and roles in the accounts that you attach the SCP to. 

An SCP that is applied at the root cascades its permissions to the OUs below it. Any account has only those permissions permitted by every parent above it. If a permission is blocked at any level above the account, either implicitly (by not being included in an Allow policy statement) or explicitly (by being included in a Deny policy statement), a user or role in the affected account cannot use that permission, even if the account administrator attaches the Administrator Access IAM policy with */* permissions to the user.

### AWS Inspector

It finds security issues in EC2 instances and the software running on them. When you change an instance or when there is an critical vulnerability.

tbc

### AWS Shield

AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.


### Terraform

tbc

[Why use Terraform](https://www.oreilly.com/learning/why-use-terraform)


### AWS cloudwatch and cloudtrail

Cloudtrial is primarily used when you want to monitor the API calls made to a particular service or Application (e.g. The number of get requests on S3 bucket made from an application written using AWS SDKs). It is primarily used to monitor API calls and is applicable for a select services only.

CloudWatch is used for logging events that happen on any particular AWS service. It is the default logging service provided by AWS and can be configured to detect Alarm conditions such as High CPU, Low Disk Space, Network Parameters etc.

CloudWatch can be used to monitor Cloudtrial service but not the other way around. *[credit: Sriram Kuravi on Quora]*

### AWS systems manager 

**Dashboard**

It has a dashboard that shows centralized information pulling from other services, such as API call logs from CloudTrail, Cloudwatch metrics, config changes, software inventory, patch compliance etc. 

Run commands on instances. An agent must be installed on each instance prior to being able to support commends.

**Session Manager**

It provides a browser-based interactive shell for Linux and Windows  

Patch Manager. AWS Systems Manager helps you select and deploy operating system and software patches automatically across large groups of Amazon EC2 or on-premises instances.

**Distributor**

AWS Systems Manager helps you securely distribute and install software packages, such as software agents. 

State Manager - just like what Configuration management does

AWS Systems Manager provides configuration management, which helps you maintain consistent configuration of your Amazon EC2 or on-premises instances. You can define configuration policies for your servers through the AWS Management Console or use existing scripts, PowerShell modules, or Ansible playbooks directly from GitHub or Amazon S3 buckets. 


### AWS S3 server side encryption 

Server-side encryption protects data at rest. Server-side encryption with Amazon S3-managed encryption keys (SSE-S3) uses strong multi-factor encryption. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.

If you need server-side encryption for all of the objects that are stored in a bucket, use a bucket policy. 


### AWS trusted advisor 

Performance , Fault Tolerance, Cost Optimization, Service Limits, and Security.

In security, it checks the security of your application by closing gaps, enabling various AWS security features, and examining your permissions.

* Security Groups - open on specific ports or rules that allow unrestricted access to a resource

* Amazon S3 Bucket Permissions - open access permissions

* MFA on Root Account - 

* IAM Password Policy - if there is an enabled policy

* AWS CloudTrail Logging- checks for your use of AWS CloudTrail

* Amazon Route 53 MX and SPF Resource Record Sets

* ELB Listener Security - Checks for load balancers with listeners that do not use recommended security configurations for encrypted communication

* ELB Security Groups

* CloudFront Custom SSL Certificates in the IAM Certificate Store

* CloudFront SSL Certificate on the Origin Server

* IAM Access Key Rotation - active IAM access keys that have not been rotated in the last 90 days

* Exposed Access Keys - checks popular code repositories for access keys that have been exposed to the public 

* Amazon EBS Public Snapshots - check if Amazon EBS) volume snapshots and alerts you if any snapshots are marked as public.


### AWS tagging

AWS allows customers to assign metadata to their AWS resources in the form of tags. Each tag is a simple label consisting of a customer-defined key and an optional value that can make it easier to manage, search for, and filter resources. Although there are no inherent types of tags, they enable customers to categorize resources by purpose, owner, environment, or other criteria. This webpage describes commonly used tagging categories and strategies to help AWS customers implement a consistent and effective tagging strategy. The following sections assume basic knowledge of AWS resources, tagging, detailed billing, and AWS Identity and Access Management (IAM).

**General Best Practices**

When creating a tagging strategy for AWS resources, make sure that it accurately represents organizationally relevant dimensions and adheres to the following tagging best practices:

Always use a standardized, case-sensitive format for tags, and implement it consistently across all resource types.

Consider tag dimensions that support the ability to manage resource access control, cost tracking, automation, and organization.

Implement automated tools to help manage resource tags. The Resource Groups Tagging API enables programmatic control of tags, making it easier to automatically manage, search, and filter tags and resources. It also simplifies backups of tag data across all supported services with a single API call per AWS Region.

Err on the side of using too many tags rather than too few tags.

Remember that it is easy to modify tags to accommodate changing business requirements, however consider the ramifications of future changes, especially in relation to tag-based access control, automation, or upstream billing reports. 

<https://aws.amazon.com/answers/account-management/aws-tagging-strategies/>

### Agile and Devops

<https://www.guru99.com/agile-vs-devops.html>

Agile involves continuous iteration of development and testing in the SDLC process. This software development method emphasizes on iterative, incremental, and evolutionary development.

Agile process breaks the product into smaller pieces and integrates them for final testing. It can be implemented in many ways such as scrum and sprint. It focuses on 

Small Team is at the core of Agile. As smaller is the team, the fewer people on it, the faster they can move.

Agile development is managed in units of "sprints." This time is much less than a month for each sprint. 

Agile is consider an effective enabler for Devops.

|View|Agile|DevOps|
|---|---|---|
|What is it?|an iterative approach which focuses on collaboration, feedback, and small, rapid releases.|a practice of bringing development and operations teams together.|
|Purpose|manage complex projects.|manage end-to-end engineering processes.|
|Task|Agile process focusses on constant changes.|DevOps focuses on constant testing and delivery.|
|Implementation|Agile method can be implemented within a range of tactical frameworks like a sprint, safe and scrum.|The primary goal of DevOps is to focus on collaboration, so it doesn't have any commonly accepted framework.|
|Team skill set|Agile development emphasizes training all team members to have a wide variety of similar and equal skills.|DevOps divides and spreads the skill set between the development and operation teams.|
|Team size|Small Team is at the core of Agile. As smaller is the team, the fewer people on it, the faster they can move.|Relatively larger team size as it involves all the stack holders.|
|Duration|Agile development is managed in units of "sprints." This time is much less than a month for each sprint.|DevOps strives for deadlines and benchmarks with major releases. The ideal goal is to deliver code to production DAILY or every few hours.|
|Feedback|Feedback is given by the customer.|Feedback comes from the internal team.|
|Target Areas|Software Development|End-to-end business solution and fast delivery.|
|Shift-Left Principles|Leverage shift-left|Leverage both shifts left and right.|
|Emphasis|Agile emphasizes on software development methodology for developing software. When the software is developed and released, the agile team will not care what happens to it.|DevOps is all about taking software which is ready for release and deploying it in a reliable and secure manner.|
|Cross-functional|Any team member should be able to do what's required for the progress of the project. Also, when each team member can perform every job, it increases understanding and bonding between them.|In DevOps, development teams and operational teams are separate. So, communication is quite complex.|
|Communication|Scrum is most common methods of implementing Agile software development. Daily scrum meeting is carried out.|DevOps communications involve specs and design documents. It's essential for the operational team to fully understand the software release and its hardware/network implications for adequately running the deployment process.|
|Documentation|Agile method is to give priority to the working system over complete documentation. It is ideal when you're flexible and responsive. However, it can hurt when you're trying to turn things over to another team for deployment.|In the DevOps, process documentation is foremost because it will send the software to the operational team for deployment. Automation minimizes the impact of insufficient documentation. However, in the development of complex software, it's difficult to transfer all the knowledge required.|
|Automation|Agile doesn't emphasize on automation. Though it helps.|Automation is the primary goal of DevOps. It works on the principle to maximize efficiency when deploying software.|
|Goal|It addresses the gap between customer need and development & testing teams.|It addresses the gap between development + testing and Ops.|
|Focus|It focuses on functional and non-function readiness.|It focuses more on operational and business readiness.|
|Importance|Developing software is inherent to Agile.|Developing, testing and implementation all are equally important.|
|Speed vs. Risk|Teams using Agile support rapid change, and a robust application structure.|In the DevOps method, the teams must make sure that the changes which are made to the architecture never develop a risk to the entire project.|
|Quality|Agile produces better applications suites with the desired requirements. It can easily adapt according to the changes made on time, during the project life.|DevOps, along with automation and early bug removal, contributes to creating better quality. Developers need to follow Coding and Architectural best practices to maintain quality standards.|
|Tools used|JIRA, Bugzilla, Kanboard are some popular Agile tools.|Puppet, Chef, TeamCity OpenStack, AWS are popular DevOps tools.|
|Challenges|The agile method needs teams to be more productive which is difficult to match every time.|DevOps process needs to development, testing and production environments to streamline work.|
|Advantage|Agile offers shorter development cycle and improved defect detection.|DevOps supports Agile's release cycle.|

### A collection of Devops books

[Google Site Reliability Engineering](https://landing.google.com/sre/books/) 

### Five principles for securing Devops

From (ISC)2 DevSecOps: Integrating Security into DevOps, 2018

- Automate security in leveraging API in cycle of initiate, control, return results, and productized support (CA Veracode, 2018)
- Integrate to fail quickly. Testing happens with every release by using CI/CD pipeline, avoid to leave security issues entirely with the developers, NOT a late step in the process - as early as possible
- No false positives. Not tolerable if the finding is a false positive
- Build security champions by training developers in secure coding. The training can be a force multiplier by reducing culture conflict and embeding app sec knowledge into the team
- Keep operational visibility. The app sec related processes need to support closed loop feedback from prod as well as security incidents and other inputs


Reference

[ISC2 Standard Development and Productivity Tools](/doc/ISC2 Standard Development and Productivity Tools.pdf)

Note: For app scanning tools, from a practical view, I would suggest adding Veracode to the tooling list provided it is widely used in industry.

[State of DevOps Report 2018](/doc/State of DevOps Report 2018.pdf)

[DevSecOps: How to Seamlessly Integrate Security Into DevOps. Gartner](/doc/Gartner devsecops_how_to_seamlessly.pdf)

### Security champions in devsecops

Some of the roles that security champion should assume as member of the dev team 

- Act as voice of security
- Decide when to engage security team
- Participate in code reviews
- Participate in threat modeling exercises
- Help with QA, testing
- Assist in triging security bugs
- App sec skills is not a key requirement as skills can be gained through training


### Traditional security controls that require adaptation

- static analysis 
- dynamic scanning
- security code reviews
- feedback

### Integrating SAST and DAST into CI/CD pipeline

From Veracode 

![alt text](/pic/sast dast cicd.png)

### Threat modeling

<https://www.dummies.com/programming/certification/security-threat-modeling/>

Threat modeling is a type of risk analysis used to identify security defects in the design phase of an information system. Threat modeling is most often applied to software applications.

Threat modeling is typically attack-centric; threat modeling most often is used to identify vulnerabilities that can be exploited by an attacker in software applications.

Threat modeling is most effective when performed at the design phase of an information system or application. When threats and their mitigation are identified at the design phase, much effort is saved through the avoidance of design changes and fixes in an existing system.

While there are different approaches to threat modeling, the typical steps are
- Identifying threats
- Determining and diagramming potential attacks
- Performing reduction analysis
- Remediation of threats

Identifying threats

One of the commonly referred model is called STRIDE developed by Microsoft, which is a list of basic threat categories:
- Spoofing of user identity
- Tampering
- Repudiation
- Information disclosure
- Denial of service
- Elevation of privilege

Determining and diagramming potential attacks
- A commonly used diagram at this stage is called [Attack tree][attack tree]

[attack tree]:https://en.wikipedia.org/wiki/Attack_tree


### Software Composition Analysis (SCA)

<https://resources.whitesourcesoftware.com/blog-whitesource/software-composition-security-analysis>

(extracted from above post)
- What is SCA

SCA is a relatively new term for a set of tools that provides visibility into open source inventory of application. SCA tools come in different forms, offering a range of capabilities from those focused on licensing compliance to others encompassing both security and license management. 

- What is SCA's typical functionality

First and foremost, SCA tools generate an inventory report of all open source components in your products, including all direct and transitive dependencies. Taking inventory of open source usage is critical as it is the basis for properly managing your open source usage. After all, how can you secure or ensure compliance of something you do not know you’re using?

Some fancy functionalities such as automatic policy enforcements can cross reference open source components found in your code with your organizational policies, triggering different responses from initiating an automated approval workflow to failing the build. The idea behind is to use only approved workflow. This step is sometimes automated and integrated into CI/CD. 

Leading tools are also able to automate the entire process of open source selection, approval and tracking, saving developers precious time and increasing their accuracy significantly. Some such tools are able to alert of vulnerabilities in a component while still on the web, before a pull is made and the component enters the system. Other tools are able to navigate developers to the precise location of a vulnerable component thereby reducing remediation efforts.

- Why is SCA becoming an integral part of tooling used in devops

Open source components have become the main building block in software applications in all verticals. Yet despite the heavy reliance on open source, most companies have been generally lax about ensuring that open source components meet basic security standards and that organizations are compliant with the required open source licenses.

The more popular an open source component is, the greater the value to hackers of exploiting a vulnerability found in it.
 
This is where SCA tools come into play. They provide essential security for software comprised in part of open source components. SCA tools identify which open source components a corporation is using in its source code, and match these components with community databases, advisories and issue trackers to bring to the surface any vulnerabilities that may exist in the source code.

SCA tools provide detailed view about a corporation’s open source inventory to executives, development, security and legal teams with the capability to generate reports for visibility. 

- The evolution of SCA

1st Generation: Open Source Code Scanning

2nd generation: Continuous Open Source Components Management 

[Forrester Wave SCA 2017](/doc/Forrester Wave SCA 2017.pdf){:target="_blank"}

[Gartner Application Security Testing 2018](/doc/Gartner Application Security Testing 2018.pdf){:target="_blank"}

### Service mesh

tbc

Everyone is talking about service meshes. What are they? Why are people so interested in them? And how does HashiCorp Consul fit into this conversation? HashiCorp co-founder and CTO Armon Dadgar explains the service mesh concept in plain English.

<https://www.hashicorp.com/resources/what-is-a-service-mesh>


### Cohesity 

tbc... placeholder 

### AWS Firewall Manager

tbc

AWS Firewall Manager is a security management service that makes it easier to centrally configure and manage AWS WAF rules across your accounts and applications. Using Firewall Manager, you can easily roll out AWS WAF rules for your Application Load Balancers and Amazon CloudFront distributions across accounts in AWS Organizations. 

### AWS Transit Gateway

tbc... New service offering in 2018.

[AWS transit gateway](https://aws.amazon.com/transit-gateway/) is a network transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. It has the ability to build a hub-and-spoke network topology. 

> The existing AWS connectivity options for VPCs include AWS Direct Connect, NAT Gateways, Internet Gateways, VPC Peering, AWS Managed VPN Connections, and PrivateLink. 
>
> They are strictly point-to-point, so the number of VPC-to-VPC connections grows quickly. 


<https://aws.amazon.com/blogs/aws/new-use-an-aws-transit-gateway-to-simplify-your-network-architecture/>


### AWS config and evident.io

[AWS config faq](https://aws.amazon.com/config/faq/)

By using AWS Config as an additional data source, the [Evident Security Platform](https://www.paloaltonetworks.com/products/secure-the-cloud/evident.html) is able to deliver faster, actionable security alerts to our customers before breaches occur. Also, by cross-checking new AWS Config data with our comprehensive AWS Security audits, ESP can further validate customers’ security postures.

Both tools require read-only access of the API, both tools monitor the cloud configuration against things like:

- commonly used frameworks (CIS, NIST, PCI, FedRAMP, GDPR, ISO) 

- corporate policy, such as restrictions on which region the company is supposed to be operated on, cost conscious concerns such as 

and alert violations.

evident.io supports AWS and Azure.