[{"content":"Test abc\n","permalink":"https://davidxiao.me/posts/cross-project-runtime-service-account-in-google-cloud-functions/","summary":"Test abc","title":"Cross-Project Runtime Service Account in Google Cloud Functions"},{"content":"SAML and OIDC What is the Real Difference Between SAML and OIDC\nBoth SAML and OIDC are authentication protocols and some times were referred to as identity protocols.\nThe basic login flow for both is the same.\nA user logs in to the Identity Provider. The user selects which app to access. The user’s information is passed from the IdP to the user’s browser or other endpoint. The endpoint passes the information on to the application. The application confirms they are authorized to access the resources. The user is allowed into the application. OIDC is built off of the OAuth 2.0 protocol. Whereas OAuth 2.0 is used to set up so that two applications such as two websites can trust each other and send data back and forth, OIDC works at the individual or user level.\nIn comparison to SAML, OIDC login flows work in the same way. But, there are three main differences:\nSAML transmits user data in XML format. OIDC transmits user data in JSON format. SAML calls the user data it sends a SAML Assertion. OIDC calls the data Claims. SAML calls the application or system the user is trying to get into the Service Provider. OIDC calls it the Relying Party. OIDC is gaining in popularity. It is considered simpler to implement than SAML and easily accessible through APIs because it works with RESTful API endpoints. This also means it works better with mobile applications.\nOAuth 2.0 Introduction to Identity - What is OAuth 2.0\nOAuth 2.0 is an authorization protocol and not an authentication protocol. As such, it is designed primarily as a means of granting access to a set of resources, for example, remote APIs or user’s data.\nOAuth 2.0 uses Access Tokens. An Access Token is a piece of data that represents the authorization to access resources on behalf of the end-user. OAuth 2.0 doesn’t define a specific format for Access Tokens. However, in some contexts, the JSON Web Token (JWT) format is often used. This enables token issuers to include data in the token itself. Also, for security reasons, Access Tokens may have an expiration date.\n","permalink":"https://davidxiao.me/posts/saml-oidc-oauth20-in-a-nutshell/","summary":"SAML and OIDC What is the Real Difference Between SAML and OIDC\nBoth SAML and OIDC are authentication protocols and some times were referred to as identity protocols.\nThe basic login flow for both is the same.\nA user logs in to the Identity Provider. The user selects which app to access. The user’s information is passed from the IdP to the user’s browser or other endpoint. The endpoint passes the information on to the application.","title":"SAML, OIDC and OAuth 2.0 in a Nutshell"},{"content":"Toda - Thank You \u0026ldquo;Toda\u0026rdquo; is pronounced basically \u0026ldquo;toh-DAH,\u0026rdquo; with the emphasis on the second syllable. This is important — if you put the stress on the first syllable (\u0026ldquo;TOH-dah\u0026rdquo;) it will make the word sound bizarre and may make it hard to understand you. It\u0026rsquo;s like pronouncing the English word \u0026ldquo;enough\u0026rdquo; as \u0026ldquo;EE-nuff\u0026rdquo; not \u0026ldquo;ee-NUFF\u0026rdquo;.\nIf you want to express sincere appreciation, say \u0026ldquo;Toda Raba\u0026rdquo; which means \u0026ldquo;Thank you very much\u0026rdquo;. \u0026ldquo;Raba\u0026rdquo; is pronounced \u0026ldquo;raa-BAH\u0026rdquo;.\nBevakasha - You\u0026rsquo;re Welcome In response to \u0026ldquo;Toda\u0026rdquo;, you can say (\u0026ldquo;Bi-varr-karr-SHA\u0026rdquo;) which means \u0026ldquo;You are welcome\u0026rdquo;.\nSleh(kh)a - Excuse Me \u0026ldquo;Sleh(kh)a\u0026rdquo; is pronounced \u0026ldquo;sli-HAH\u0026rdquo;.\n","permalink":"https://davidxiao.me/posts/three-hebrew-words-you-can-learn/","summary":"Toda - Thank You \u0026ldquo;Toda\u0026rdquo; is pronounced basically \u0026ldquo;toh-DAH,\u0026rdquo; with the emphasis on the second syllable. This is important — if you put the stress on the first syllable (\u0026ldquo;TOH-dah\u0026rdquo;) it will make the word sound bizarre and may make it hard to understand you. It\u0026rsquo;s like pronouncing the English word \u0026ldquo;enough\u0026rdquo; as \u0026ldquo;EE-nuff\u0026rdquo; not \u0026ldquo;ee-NUFF\u0026rdquo;.\nIf you want to express sincere appreciation, say \u0026ldquo;Toda Raba\u0026rdquo; which means \u0026ldquo;Thank you very much\u0026rdquo;.","title":"A few basic Hebrew words you can learn"},{"content":"Overall Reference https://wiki.ubuntu.com/UEFI/EDK2\nMy Environment Hypervisor OS: Ubuntu 20.04 LTS\nKernel: 5.4\nQEMU: 6.1\nGuest OS Type: pc-q35-6.1\nBefore You Start It does not seem to support zsh and oh-my-zsh, if you are using those as default shell like me, please first run bash and then rest of the commands in this guide. If your nasm version is less than 2.15.05 it may have some compatibility issue per this. To install nasm_2.15.05 as a separate package on Ubuntu, the .deb file can be downloaded from here. Install it via sudo dpkg -i nasm_2.15.05-1_amd64.deb https://github.com/tianocore/edk2/blob/master/BaseTools/Bin/nasm_ext_dep.yaml Prepare the Build Environment sudo apt install build-essential git uuid-dev iasl nasm ; # check nasm version and do necessary steps in the Caution section above git clone --depth 1 \u0026#34;https://github.com/tianocore/edk2.git\u0026#34; -b \u0026#34;edk2-stable202205\u0026#34; ; cd edk2 ; git submodule update --init --recursive ; # Important! This retrieves all the submodules needed make -C BaseTools ; . edksetup.sh #make sure to include the dot Make Changes for Config Presuming you are running X64 just like me, the target.txt file needs to be changed a bit. See below.\n$ vi Conf/target.txt\nReplace it with\nACTIVE_PLATFORM = OvmfPkg/OvmfPkgX64.dsc\nReplace it with\nTOOL_CHAIN_TAG = GCC5\nReplace it with \u0026lsquo;X64\u0026rsquo; for 64bit\nTARGET_ARCH = X64\nBuild the Ovmf Firmware Simply run build The firmware files are located in the Build/OvmfX64/RELEASE_GCC5/FV/ folder.\nBuild the firmware with Secure Boot support If you wish to build OVMF with Secure Boot - this could be helpful especially if you plan to install Windows as your guest OS, you need to follow the openssl installation instructions found in OpenSSL-HOWTO.txt, and build with the SECURE_BOOT_ENABLE option: $ build -DSECURE_BOOT_ENABLE=TRUE\nPost Build Manually copy the output files:\nsudo cp Build/OvmfX64/RELEASE_GCC5/FV/OVMF_CODE.fd /usr/share/OVMF/OVMF_CODE.secboot.202207.fd ; sudo cp Build/OvmfX64/RELEASE_GCC5/FV/OVMF_VARS.fd /usr/share/OVMF/OVMF_VARS.secboot.202207.fd ; Then, make necessary changes in the VM definition file. I\u0026rsquo;m using virsh for VM management, so I\u0026rsquo;m using the following command to edit my win10 VM. When it\u0026rsquo;s complete, the section of the VM xml would look like this:\n\u0026lt;os\u0026gt; \u0026lt;type arch=\u0026#39;x86_64\u0026#39; machine=\u0026#39;pc-q35-6.1\u0026#39;\u0026gt;hvm\u0026lt;/type\u0026gt; \u0026lt;loader readonly=\u0026#39;yes\u0026#39; type=\u0026#39;pflash\u0026#39;\u0026gt;/usr/share/OVMF/OVMF_CODE.secboot.202207.fd\u0026lt;/loader\u0026gt; \u0026lt;nvram\u0026gt;/usr/share/OVMF/OVMF_VARS.secboot.202207.fd\u0026lt;/nvram\u0026gt; \u0026lt;/os\u0026gt; ","permalink":"https://davidxiao.me/posts/build-your-own-ovmf-firmware-for-qemu/","summary":"Overall Reference https://wiki.ubuntu.com/UEFI/EDK2\nMy Environment Hypervisor OS: Ubuntu 20.04 LTS\nKernel: 5.4\nQEMU: 6.1\nGuest OS Type: pc-q35-6.1\nBefore You Start It does not seem to support zsh and oh-my-zsh, if you are using those as default shell like me, please first run bash and then rest of the commands in this guide. If your nasm version is less than 2.15.05 it may have some compatibility issue per this. To install nasm_2.","title":"Build Your Own OVMF Firmware for Qemu VM"},{"content":"TL;DR\nThis is a curated, opinionated guide to building a home server. The vision is to build a computing environment under the budget of USD $3,000 with commercially available hardware. The goal is to provide a linux environment that can manage VMs, containers and microvms with ease. (why not just use proxmox?) As a platform, it allows the enthusiasts to experiment and try out new stuff such as PCI passthrough, AWS Site-to-Site VPN and more.\nHardware The hardware list.\nComponent Configuration Retail Price in USD CPU 16-core AMD Ryzen 9 5950X (Vermeer) $799 Motherboard ASRock Rack xX570D4U-2L2T $519 Memory 64GB. 2x32GB DDR4 3200 ECC Unbuffered DIMM. Model: Kingston KSM32ED8/32ME 2x$199 Storage 1TB M.2 SSD. Model: SAMSUNG 980 PRO $199 GPU Nvidia RTX 3090 Founders Edition 24GB $1,499 PSU 850W Seasonic FOCUS PX-850 $159 Case Corsair 5000D Airflow ATX Mid-Tower $159 CPU cooler Corsair iCUE H150i RGB Pro XT, 360mm Radiator $145 The reasoning behind the hardware I pulled together for this build are:\nThe home server needs to have hardware level remote server management capabilities and 10G network. ASRock Rack X570D4U-2L2T checks all the boxes by having a built-in IPMI and 2x10G ethernet ports.\nAs a leisure gaming enthusiast, Nvidia 3090 provides excellent performance for PC games and HDR10 in 4k if not 8k. The card gets it done no matter what kind of GPU thirsty task I through at her. That said, Nvidia 3080Ti may be a good alternative choice here with comparable gaming performance and a more affordable price tag.\n64GB ECC memory ensures the server will have sufficient memory to run multiple VMs for most reasonable tasks and provides extra stability as ECC.\nThis build does not include any HDD other than the 1TB SSD as I already have a NAS at home. From a security and operational perspective, I believe having a dedicated storage, i.e. a NAS, as part of the home lab has certain advantages than having an all-in-one general purpose server that does both computing and storage.\nUEFI BIOS configuration Disable CSM to enforce UEFI boot. Enable onboard VGA.\nInstall and upgrade Windows 10 Create a bootable Windows 10 USB stick to install Windows. Install windows on the whole available space. When it\u0026rsquo;s installed, use the computer management to allocate sufficient disk space for ubuntu. In my case, I reserved about 500GB space for ubuntu.\nInstall and upgrade ubuntu When windows 10 is installed, create a bootable ubuntu USB stick to install ubuntu.\n# sudo apt-get update; # sudo apt-get install dist-upgrade; (reboot)\n(optional) Config NICs for a faster boot During the ubuntu boot-up, it will wait for some time for each network interface to be up. Configuring each unused network interface as “optional” will significantly speed up the boot time.\n# sudo vi /etc/netplan/00-installer-config.yaml Add “optional: true” under each network interface that is not in use. (reboot)\nInstall Oh-my-zsh Since we will be using the command line pretty extensively for the rest of the guide, having omz and extensions like history-substring-search and zsh-autosuggestions would make our life a little easier.\n# sudo apt install zsh # sh -c \u0026#34;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; # git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions Edit ~/.zshrc, replace source $ZSH/oh-my-zsh.sh with the following:\nplugins=(git history-substring-search zsh-autosuggestions) source $ZSH/oh-my-zsh.sh PROMPT=\u0026#34;%{$fg[cyan]%}ubuntu %{$reset_color%}%D{%f}|%D{%k:%M}%{$fg[cyan]%} [%c] %{$reset_color%}\u0026#34; bindkey \u0026#34;^[a\u0026#34; backward-word bindkey \u0026#34;^[e\u0026#34; forward-word export PROMPT_EOL_MARK=\u0026#39;\u0026#39; Restart the session. Everything oh-my-zsh should work now.\ninstall nvidia cuda toolkit The nvidia display driver is included as part of the CUDA toolkit\n(optional) disable gdm3 at boot up To disable gdm3 if needed, run:\n# sudo systemctl stop gdm3; sudo systemctl disable gdm3; Install ASPEED VGA driver IMPORTANT: Ensure the Onboard VGA is Enabled before installing the vga driver.\n# wget --no-check-certificate https://www.aspeedtech.com/file/support/Linux_DRM_LTS_112.zip; # unzip Linux_DRM_LTS_112.zip; # sudo apt-get install linux-headers-$(uname -r); # sudo dpkg -i ~/Linux_DRM/DKMS/ast-drm-linux5.04.deb; If next boot fails, revert to initrd.img-5.4.0-80-generic.old-dkms image. Run:\nupdate-initramfs....... If you see the following warning message W: Possible missing firmware /lib/firmware/ast_dp501_fw.bin for module ast, it\u0026rsquo;s related to a hard coded file location in the kernel module and can be safely disregarded. (reboot)\n(optional) to verify if the ast driver is loaded, run lspci|grep VGA;. If the driver is loaded, the output should look like: 29:00.0 VGA compatible controller: ASPEED Technology, Inc. ASPEED Graphics Family (rev 41)\nDisable cloud-init sudo touch /etc/cloud/cloud-init.disabled;\nInstall cockpit console In this section we will be installing the cockpit admin console and its virtual-machine component.\nsudo apt-get install cockpit cockpit-machines;\nWhen complete, visit https://:9090/ on your browser. If you are using Chrome on macOS you may see this screen without a button that allows you to proceed. There’s a trick to get through: Click anywhere on the page and type thisisunsafe to proceed.\n(optional) To fix the Software updates issue with ubuntu/cockpit combination, run:\nsudo systemctl enable network-manager.service;\nsudo systemctl disable systemd-networkd.service;\n(optional) Install a X Window Manager sudo apt install ubuntu-mate-desktop;\nWhen prompted, select “lightdm”. When installation is complete, run “sudo systemctl disable lightdm.service; sudo systemctl stop lightdm.service;”\nFor a quick test, run “mate-session” in a X11 forwarding enabled SSH session to start the Mate X window manager.\n(optional) Enable Chrome Remote Desktop references:\nhttps://cloud.google.com/architecture/chrome-desktop-remote-on-compute-engine#installing_chrome_remote_desktop_on_the_vm_instance https://support.google.com/chrome/answer/1649523 wget https://dl.google.com/linux/direct/chrome-remote-desktop_current_amd64.deb;\nsudo apt-get install \u0026ndash;assume-yes ./chrome-remote-desktop_current_amd64.deb;\nsudo bash -c \u0026rsquo;echo \u0026ldquo;exec /etc/X11/Xsession /usr/bin/mate-session\u0026rdquo; \u0026raquo; /etc/chrome-remote-desktop-session'\nOn macOS, visit https://remotedesktop.google.com/ on Chrome and follow instructions to complete the configuration. You should then have Chrome remote desktop access out of the box.\n(optional) Enable X11 Forwarding This step is only needed when you prefer to use X window server as opposed to Chrome Remote Desktop for remote access.\nIn order to remotely access the X window applications on the ubuntu host via X11,\nsudo vi /etc/ssh/sshd_config\nand ensure X11Forwarding is set to yes.\nFrom the guest OS (e.g. macOS), start a X window system, then open a terminal and run “ssh -X user@ubuntu” to connect to the remote host. When successfully logged in, run “xterm” on the remote host, the xterm window should show up on the macOS desktop.\nGPU passthrough References:\nhttps://mathiashueber.com/pci-passthrough-ubuntu-2004-virtual-machine/\nhttps://mathiashueber.com/storage-setup-virtual-machines/\nInstall required packages sudo apt install bridge-utils;\nubuntu 20.04.02 comes with virt-manager 2.2, we wanted to install version 3.2 from source sudo apt install gir1.2-gtk-vnc-2.0 gir1.2-gtksource-4 gir1.2-libvirt-glib-1.0 gir1.2-spiceclientglib-2.0 gir1.2-spiceclientgtk-3.0 libgtksourceview-4-0 libgtksourceview-4-common; #install dependant libs for virt-manager;\nwget https://virt-manager.org/download/sources/virt-manager/virt-manager-3.2.0.tar.gz;\ntar xzvf virt-manager-3.2.0.tar.gz;\ncd virt-manager-3.2.0;\nsudo ./setup.py install;\nsudo vi /etc/default/grub;\nEdit the line which starts with GRUB_CMDLINE_LINUX_DEFAULT to match:\nGRUB_CMDLINE_LINUX_DEFAULT=\u0026ldquo;amd_iommu=on iommu=pt\u0026rdquo;\nsudo update-grub;\n(reboot)\nConfirm the IOMMU kernel parameters by running cat /proc/cmdline;\nVerify if IOMMU is enabled by running dmesg |grep AMD-Vi;\nIf it’s enabled, the output would look like:\n[ 0.360711] pci 0000:00:00.2: AMD-Vi: IOMMU performance counters supported\n[ 0.362643] pci 0000:00:00.2: AMD-Vi: Found IOMMU cap 0x40\n[ 0.362645] pci 0000:00:00.2: AMD-Vi: Extended features (0x58f77ef22294ade):\n[ 0.362648] AMD-Vi: Interrupt remapping enabled\n[ 0.362648] AMD-Vi: Virtual APIC enabled\n[ 0.362649] AMD-Vi: X2APIC enabled\n[ 0.362725] AMD-Vi: Lazy IO/TLB flushing enabled\nDetermine IOMMU devices and groups Caveat: If you are planning to make hardware changes, e.g. installing a new M2.SSD or adding a new GPU, be aware that the PCI lane numbers and IOMMU group numbers may change after the installation. If you already have the GPU passthrough setup up and running, you need to plan it accordingly so that it won’t catch you off guard.\nEdit and run the following script as iommu.sh\nSource: https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF#Ensuring_that_the_groups_are_valid\n#!/bin/bash\nshopt -s nullglob\nfor g in find /sys/kernel/iommu_groups/* -maxdepth 0 -type d | sort -V; do\necho \u0026ldquo;IOMMU Group ${g##*/}:\u0026rdquo;\nfor d in $g/devices/*; do\necho -e \u0026ldquo;\\t$(lspci -nns ${d##*/})\u0026rdquo;\ndone;\ndone;\nLook for the GPU to be passed through. It should look like the following:\nIOMMU Group 28:\n2d:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [10de:2204] (rev a1)\n2d:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:1aef] (rev a1)\nsudo vi /etc/default/grub\nGRUB_CMDLINE_LINUX_DEFAULT=\u0026ldquo;video=efifb:off amd_iommu=on iommu=pt kvm.ignore_msrs=1 vfio-pci.ids=10de:2204,10de:1aef\u0026rdquo;\n**** \u0026ldquo;video=efifb:off\u0026rdquo; is needed. See the discussion here. ****\nsudo update-grub\n(reboot)\nRun lspci -nnv\nLook for the line \u0026ldquo;Kernel driver in use\u0026rdquo; for the GPU and its audio part. It should indicate Kernel driver in use: vfio-pci\n2d:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [10de:2204] (rev a1) (prog-if 00 [VGA controller])\nSubsystem: NVIDIA Corporation Device [10de:147d]\nFlags: fast devsel, IRQ 255\nMemory at fb000000 (32-bit, non-prefetchable) [size=16M]\nMemory at 7fe0000000 (64-bit, prefetchable) [size=256M]\nMemory at 7ff0000000 (64-bit, prefetchable) [size=32M]\nI/O ports at f000 [disabled] [size=128]\nExpansion ROM at fc000000 [disabled] [size=512K]\nCapabilities: Kernel driver in use: vfio-pci\nKernel modules: nvidiafb, nouveau\n2d:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:1aef] (rev a1)\nSubsystem: NVIDIA Corporation Device [10de:147d]\nFlags: fast devsel, IRQ 255\nMemory at fc080000 (32-bit, non-prefetchable) [disabled] [size=16K]\nCapabilities: Kernel driver in use: vfio-pci\nKernel modules: snd_hda_intel\nMake sure you have Windows 10 ISO and virtio windows drivers ready for installation. virtio files can be downloaded here https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/latest-virtio/\n(optional) USB passthrough\nUse the following script (usb.sh) to determine what bus and iommu group each USB device has attached to.\nSource: https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF#USB_controller\n#!/bin/bash for usb_ctrl in /sys/bus/pci/devices/*/usb*; do pci_path=${usb_ctrl%/*}; iommu_group=$(readlink $pci_path/iommu_group); echo \u0026#34;Bus $(cat $usb_ctrl/busnum) --\u0026gt; ${pci_path##*/} (IOMMU group ${iommu_group##*/})\u0026#34;; lsusb -s ${usb_ctrl#*/usb}:; echo; done *** Troubleshooting Notes and Solution on isolating USB keyboard and mouse *** Device passthrough is done on an IOMMU group level, in other words, when passing through a device, not just the device itself but also each and every other device in the same IOMMU group will be passed though.\nIn a perfect world, there should be an IOMMU group that includes only USB devices and nothing else. In the real world, there may be other types of devices in the group which makes the passthrough messy. Unfortunately in my case that’s the case.\nGiven the IOMMU group 20 includes not just USB keyboard and mouse but a few other devices such as “American Megatrends Virtual Keyboard and Mouse”, we need to find a way to move the USB devices into a separate and isolated IOMMU group, otherwise the USB device passthrough won’t work(tested).\n(optional) manually compile and install QEMU The default QEMU version that ubuntu 20.04 repo includes is QEMU 4.2. To get the current QEMU version 6.1 rc3, I decided to manually install it.\nThe binaries are installed under /usr/local/bin.\ncd\nsudo apt-get install build-essential gcc pkg-config glib-2.0 libglib2.0-dev libsdl1.2-dev libaio-dev libcap-dev libattr1-dev libpixman-1-dev libusb-1.0-0-dev\nwget https://download.qemu.org/qemu-6.1.0-rc3.tar.xz\ntar xvJf qemu-6.1.0-rc3.tar.xz\ncd qemu-6.1.0-rc3\npython3 -m venv py3-env\nsource ./py3-env/bin/activate\npip3 install ninja\n./configure \u0026ndash;target-list=x86_64-linux-user,x86_64-softmmu \u0026ndash;disable-debug-info \u0026ndash;enable-libusb\nmake -j24 #utilize multiple cores on my 5950X cpu\nsudo make install\nWhen QEMU manual installation is complete, edit /etc/libvirt/qemu/win10.xml and change the \u0026lt;emulator\u0026gt; line as follows:\n\u0026lt;emulator\u0026gt;/usr/local/bin/qemu-system-x86_64\u0026lt;/emulator\u0026gt; (reboot)\nStart the vm using virt-manager. If you see the following error:\nError starting domain: internal error: Failed to start QEMU binary /usr/local/bin/qemu-system-x86_64 for probing: libvirt: error : cannot execute binary /usr/local/bin/qemu-system-x86_64: Permission denied\nTraceback (most recent call last):\nFile \u0026ldquo;/usr/share/virt-manager/virtManager/asyncjob.py\u0026rdquo;, line 65, in cb_wrapper\ncallback(asyncjob, *args, **kwargs)\nFile \u0026ldquo;/usr/share/virt-manager/virtManager/asyncjob.py\u0026rdquo;, line 101, in tmpcb\ncallback(*args, **kwargs)\nFile \u0026ldquo;/usr/share/virt-manager/virtManager/object/libvirtobject.py\u0026rdquo;, line 57, in newfn\nret = fn(self, *args, **kwargs)\nFile \u0026ldquo;/usr/share/virt-manager/virtManager/object/domain.py\u0026rdquo;, line 1329, in startup\nself._backend.create()\nFile \u0026ldquo;/usr/lib/python3/dist-packages/libvirt.py\u0026rdquo;, line 1234, in create\nif ret == -1: raise libvirtError (\u0026lsquo;virDomainCreate() failed\u0026rsquo;, dom=self)\nlibvirt.libvirtError: internal error: Failed to start QEMU binary /usr/local/bin/qemu-system-x86_64 for probing: libvirt: error : cannot execute binary /usr/local/bin/qemu-system-x86_64: Permission denied\nIt may need to change some apparmor config.\nIn /etc/apparmor.d/abstractions/libvirt-qemu\nadd\n/usr/local/share/qemu/** r,\nand add\n/usr/local/bin/qemu-system-x86_64 rmix,\n/usr/local/bin/qemu-x86_64 rmix,\nIn /etc/apparmor.d/usr.sbin.libvirtd\nadd\n/usr/local/bin/* PUx,\nWhen complete, run\nsudo systemctl reload apparmor\n","permalink":"https://davidxiao.me/posts/a-not-so-complete-guide-on-building-a-home-server/","summary":"This is a curated, opinionated guide to building a full-stack home server from hardware to application. The goal is to provide a linux environment that can manage VMs, containers and microvms with ease. As a platform, it allows the enthusiasts to experiment and try out new stuff such as PCI passthrough, AWS Site-to-Site VPN and more.","title":"A not so complete guide on building a home server"},{"content":"TL;DR\nDeploy a sample Python 3 web application on Google Cloud AppEngine in 5 minutes. See App Engine Quickstart for reference.\nSteps Open the cloud shell on Google Cloud Console and run the following commands:\n👉 cd 👉 gcloud app create ... 👉 git clone https://github.com/GoogleCloudPlatform/python-docs-samples 👉 cd ~/python-docs-samples/appengine/standard_python3/hello_world 👉 (optional) cloudshell workspace ~/python-docs-samples/appengine/standard_python3/hello_world 👉 virtualenv --python python3 ~/envs/hello_world 👉 pip3 install -r requirements.txt 👉 python3 main.py * Serving Flask app \u0026#39;main\u0026#39; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: on * Running on http://127.0.0.1:8080/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 111-142-068 👉 (optional) click on the Web Preview button on cloud shell to open a new browser window to see the hello world app 👉 gcloud app deploy Services to deploy: descriptor: [/home/david/python-docs-samples/appengine/standard_python3/hello_world/app.yaml] source: [/home/david/python-docs-samples/appengine/standard_python3/hello_world] target project: [********] target service: [default] target version: [202107********] target url: [https://********.appspot.com] target service account: [App Engine default service account] Do you want to continue (Y/n)? y Beginning deployment of service [default]... Created .gcloudignore file. See `gcloud topic gcloudignore` for details. ╔════════════════════════════════════════════════════════════╗ ╠═ Uploading 6 files to Google Cloud Storage ═╣ ╚════════════════════════════════════════════════════════════╝ File upload done. Updating service [default]...done. Setting traffic split for service [default]...done. Deployed service [default] to [https://********.appspot.com] You can stream logs from the command line by running: $ gcloud app logs tail -s default To view your application in the web browser run: $ gcloud app browse By default, the new application is publicly accessible.\nThere are many ways to protect web applications from unauthorized access. On Google Cloud, BeyondCorp Enterprise is an enterprise scale zero trust platform that enables customers to define fine-grained access level policies leveraging security signals from a variety of data sources including device, user identity profile, network and third-party security solution.\n","permalink":"https://davidxiao.me/posts/deploy-a-demo-app-to-appengine-on-google-cloud/","summary":"Learn how to deploy a demo Python 3 web application on Google Cloud AppEngine. AppEngine is a managed platform on Google Cloud that allows customers to quickly deploy a scalable application without provisoning infrastructure.","title":"Deploy a web app within 5 minutes"},{"content":"References Simulating Site-to-Site VPN Customer Gateways Using strongSwan\nAWS Site-to-Site VPN with IPSec VPN (Strongwan) and BGP (FRRouting)\nBuilding a Scalable and Secure Multi-VPC AWS Network Infrastructure\n","permalink":"https://davidxiao.me/posts/setting-up-site-2-site-vpn-between-my-homelab-and-aws/","summary":"References Simulating Site-to-Site VPN Customer Gateways Using strongSwan\nAWS Site-to-Site VPN with IPSec VPN (Strongwan) and BGP (FRRouting)\nBuilding a Scalable and Secure Multi-VPC AWS Network Infrastructure","title":"Setting Up Site-to-Site VPN between My Homelab and AWS"},{"content":"If you like to add some vintage like retro game feeling to your website, try use pixel art images might be a good idea. Here\u0026rsquo;s a quick howto.\nInstall GIMP GIMP is an open source image processing program available on macOS, Linux and Windows. Visit https://www.gimp.org/ to download and install.\nIf you are like me who uses macOS and have the mighty homebrew package manager installed, simply run:\nbrew install gimp Start Converting Start the gimp, open your image-to-be-converted, then from the menu select Filters/Blur/Pixelize...\nOn the dialog box, you want to adjust Block Width and preview the results.\nThat\u0026rsquo;s it!\nAdjust the Block Width ","permalink":"https://davidxiao.me/posts/convert-any-image-into-pixel-art-using-gimp/","summary":"If you like to add some vintage like retro game feeling to your website, try use pixel art images might be a good idea. Here\u0026rsquo;s a quick howto.","title":"Convert Any Image Into Pixel Art Using Gimp"},{"content":"Single Region vs. Global ELB provides single-region failover and load balancing while Route 53(R53) operates at DNS level so that can failover across regions.\nInstant Failover ELB routes traffic off an unhealthy target (like an EC2) immediately when it is detected while R53 operates at DNS level, so when a client endpoint have cached DNS results in their DNS resolvers, it will not be effectively redirected until it passes TTL.\nAWS vs Non-AWS ELB performs failover within the AWS target groups where only AWS resources can be registered. R53 operates at DNS level and is resource type agnostic.\n","permalink":"https://davidxiao.me/posts/differences-between-route53-failover-and-elb-failover/","summary":"This post provides clarity over the differences between AWS R3 failover and ELB LB.","title":"Differences Between AWS Route 53 Failover and AWS ELB LB"},{"content":"Multiple Availablity Zone and Cross Region When it comes to high availability and scalability, cloud networking has an edge over data center networking.\nMultiple Region and Availablity Zone and peered VPCs can be provisioned on-demand almost instantly on cloud while setting up a conventional data center could take weeks if not months.\nFull Control over the Network Infrastructure Cloud networking operates on the \u0026ldquo;Shared Responsibility Model\u0026rdquo; which means CSP manages the network infrastructure, things like VLAN tags are out of customer\u0026rsquo;s control.\nCertain networking capabilities such as multicast routing is also not supported by every cloud provider. It is changing though, at the time of writing, AWS just announced support for multicast on transit gateways.\nSingle Tenant vs Multi-Tenant Cloud network such as VPC typically runs on infrastructure that is shared with other customers while on-premises data center usually is owned by the organization.\nAPI and Compatibility Cloud networking provides APIs that is an integral part of the Cloud service. For example, AWS VPC provides API that are integrated with EC2.\nOn-premises data centers use technologies of their choice. Some such as Cisco ACI and Nutanix provide their own set of APIs.\nNetwork Latency On-premises networks can provide very low network latency. It\u0026rsquo;s not hard to find switches that provide nano second latency. For instance, Cisco has switches that have 39ns port-to-port latency.\nCSPs like AWS are catching up but is nowhere near as good yet.\nLegacy Systems and BYOD Legacy systems that are installed on the data center rely on specific HSM modules could be part of a mission critical system.\nWhen planning on migrating such systems to cloud, \u0026ldquo;lift and shift\u0026rdquo; strategy don\u0026rsquo;t work because most cloud service providers do not allow BYOD.\nIP Address Allocation Cloud networking usually reserve a few IPs on each subnet for the cloud infrastructure.\nFor example, AWS reserves first four IP addresses and the last IP address in each subnet CIDR block. See detail\n","permalink":"https://davidxiao.me/posts/differences-between-on-premises-networking-and-public-cloud-networking/","summary":"This post discusses some of the major differences between on-prem data center networking and cloud networking.","title":"Differences between On-Prem Networking and Cloud Networking"},{"content":"Sample Code See my github repo https://github.com/davxiao/tf-proxmox\n","permalink":"https://davidxiao.me/posts/create-vms-on-proxmox-with-terraform/","summary":"Create new VMs on Proxmox by using API and cloning from existing VM template. All code is in Terraform. Sample code is provided on github repo.","title":"Create VMs on Proxmox With Terraform"},{"content":"Over the past few months I\u0026rsquo;ve collected a few good resources regarding Kubernetes security.\nI will add more as I learn.\nReference Securing a Cluster This document covers topics related to protecting a cluster from accidental or malicious access and provides recommendations on overall security.\nCIS Kubernetes Benchmark version 1.6.1 Released in October 2020, it provides prescriptive guidance for establishing a secure configuration posture for Kubernetes.\nKubernetes security best practices It covers a few suggestions on what can you do to make your Kubernetes workloads more secure.\nDisable public access\nImplement role-based access control\nEncrypt secrets at rest\nConfigure admission controllers\nImplement networking policies\nConfigure secure context for containers\nSegregate sensitive workloads\nScan container images\nEnable audit logging\nKeep your Kubernetes version up to date\nKubernetes Security Best Practices It discusses the special security concerns arising in Kubernetes environments, and best practices in properly setting up the k8s environment to mitigate vulnerabilities:\nwork with namespaces for authentication, authorization and access control\nworking with reliable docker images and updating relevant software\ndefining resource quotas to avoid resource cannibalization -setting up network policies for proper segmentation and traffic control\n","permalink":"https://davidxiao.me/posts/k8s-security/","summary":"Over the past few months I\u0026rsquo;ve collected a few good resources regarding Kubernetes security.\nI will add more as I learn.\nReference Securing a Cluster This document covers topics related to protecting a cluster from accidental or malicious access and provides recommendations on overall security.\nCIS Kubernetes Benchmark version 1.6.1 Released in October 2020, it provides prescriptive guidance for establishing a secure configuration posture for Kubernetes.\nKubernetes security best practices It covers a few suggestions on what can you do to make your Kubernetes workloads more secure.","title":"k8s Security"},{"content":"TL;DR\nCloud Access Security Broker (CASB) is considered a common solution to mitigate shadow IT and data exfiltration risks on many organization\u0026rsquo;s migrating to cloud journey.\nThis post discussed some common deployment architecture and use cases respectively.\nCASB and Context Aware Access CASB can mitigate data exfiltration risks from insider - it can correlate the download of sensitive data from a managed cloud service with the upload of that data to a personal cloud service.\nContext Aware Access is focusing on mitigating authentication and authorization risks and promoting a \u0026ldquo;Zero Trust\u0026rdquo; access model in VPN-less settings.\nReverse proxy A reverse proxy deployment steers browser-based cloud traffic from managed cloud apps to the CASB Cloud.\nReverse Proxy Deployment A CASB in reverse proxy mode proxies all traffic to and from cloud apps. Unlike a forward proxy, the endpoint or network does not need to be managed.\nA common approach is to leverage the organization\u0026rsquo;s identity solution (IdM), for example Okta, to route traffic through the CASB reverse proxy following authentication. In this way, all traffic bound for a cloud service is pervasively steered over to the CASB proxy.\nReverse proxy is essentially the only deployment architecture that supports both managed and unmanaged devices accessing cloud apps.\nChallenges\nIt supports web browser traffic only – native apps or sync clients may have non-web authentication methods. (e.g. JWT or other forms of API token)\nForward proxy CASB forwarder is deployed on-premises as a VM that steers local cloud and web traffic to the CASB Cloud.\nForward Proxy Deployment There are two approaches:\nIf the organization has an existing secure web gateway, a common practice is to configure proxy chaining to the upstream CASB forward proxy.\nIf no secure web gateway exists, an endpoint agent can be deployed to route cloud traffic through the forward proxy.\nChallenges\nIt does not work well for either unmanaged devices or users that are accessing cloud over non-corporate network, e.g. at home.\nAPI Use API connectors to connect CASB to cloud apps like Office 365, Box, Salesforce that offer APIs to support visibility and policy enforcement.\nAPI Deployment Architecture Typical APIs such as audit trails of user activity, content inspection, user permissions on storage and security settings can be leveraged by CASB to enable visibility and policy enforcement.\nChallenges\nDoes not discover \u0026ldquo;Shadow IT\u0026rdquo;.\nCASB vendor may not support Cloud apps that end user currently uses or choose to use in the future.\nAPI based capabilities vary for each Cloud app.\nLog collection CASB can be configured to parse log traffic from a perimeter device. This provides cloud service discovery capabilities. Logs can be uploaded directly to the CASB Cloud or an on-premises log parser can be deployed to continuously send log data to the CASB.\nLog Collection Deployment Challenges\nLogs may not be support off-the-shelf, customization may be required.\nReference Wikipedia: CASB\nCASB Deployment Options (Netskope)\n","permalink":"https://davidxiao.me/posts/casb-deployment/","summary":"Cloud Access Security Broker (CASB) is considered a common solution to mitigate \u0026ldquo;shadow IT\u0026rdquo; and data exfiltration risks on many organization\u0026rsquo;s journey to cloud.","title":"CASB Deployment"},{"content":"TL;DR\nThis is the first post of a series that attempts to discuss Zero Trust in security from a conceptual and implementation perspective.\nWhat is Zero Trust Zero Trust is a security model where app components and microservices are considered discrete from one another - no component or microservice trusts any other by default.\nZero Trust by design does not trust the underlying internal network fabric, and extends it to things such as input and output validation at app and microservice level. For example, it mandates that input from any source is not trusted unless it can be verified otherwise.\nAdditional efforts can include designing a defense-in-depth approach to protect against individual components, microservices, or identities compromise.\nWhile conventional network security seek to build a secure perimeter – everything within the perimeter is trusted and anything outside the perimeter is not, a Zero Trust system evaluates actions to reduce the risk of unauthorized access to data and resources.\nZero Trust Principles Never trust, always verify\nConnecting from a particular network must not determine which services user can access\nAccess to services is granted based on what we know about the user and the device\nAll access to services must be authenticated, authorized, and encrypted\nSecurity Controls in Enforcing a Zero Trust Model Verifying identity\nOne components of a Zero Trust system is the ability to verify a user’s identity before access is granted to the corporate network.\nVerifying devices\nUnmanaged devices are an easy entry point for bad actors, ensuring that only healthy devices can access critical applications and data is vital for enterprise security.\nVerifying access\nSome scenarios require users to work from unmanaged devices. With those situations in mind, it transitioned from a corporate network approach to internet-first access methods, with a final goal of internet-only access methods in sight.\nThis strategy reduces users accessing the corporate network for most scenarios, and will establish a set of managed virtualized services that make applications and full functional desktop environments available to users with unmanaged devices.\nEmerging Technologies Context-Aware Access\n[To be continued]\nReference How to think about Zero Trust architectures on AWS Zero Trust goes beyond building a network boundary between each microservice in your architecture. Beyond strengthening your component perimeters; rethink threat sources and investment to protect against them.\nZero Trust models are not always appropriate. It offers some security benefits, but it also introduces cost, complexity, and operational overhead for maintaining the overall system.\nWhen considering Zero Trust architecture, evaluate all five pillars of the AWS Well-Architected framework to properly balance your needs.\nBeyondCorp at Google BeyondCorp is Google\u0026rsquo;s implementation of the zero trust security model.\nTransitioning to modern access architecture with Zero Trust Microsoft\u0026rsquo;s view on \u0026ldquo;Zero Trust\u0026rdquo; is based on the principle:\nnever trust, always verify.\nIt protects organizations by managing and granting access based on the continual verification of identities, devices and services.\n","permalink":"https://davidxiao.me/posts/what-is-zero-trust/","summary":"This is the first post of a series that attempts to discuss Zero Trust in security from a conceptual and implementation perspective.","title":"What Is Zero Trust"},{"content":"TL;DR\nThis post attempts to explain the BGP network protocol in plain English and answer those questions:\nwhat is it?\nhow it works at a high level?\nwhat are the security concerns?\nIn a Nutshell BGP is a routing method that enables the internet to function.\nToday, BGP is widely used to route IP traffic between large scale inter-connected networks. For example, when routing between corporate networks or connecting on-prem data center to a cloud provider\u0026rsquo;s edge location, BGP is typically considered de facto protocol.\nBefore diving into BGP, let\u0026rsquo;s start with a simple definition of network routing:\nThe name of a resource indicates what we seek;\nan address indicates where it is;\nand a route tells us how to get there.\n- John F. Shoch\nPre-BGP Era In the pre-BGP era when internet was much smaller, separate networks like universities usually each maintain its own static routing table - meaning the routing table on each network is manually updated by the admin and contains IP prefixes the admin is made aware of.\nWhen the scale of internet grows, the number of inter-connected networks increases and the inter-network topology started evolving from a tree-like topology to a mesh-like toplogy to allow for redundancy and scalability.\ntree-like network topology vs full-mesh network topology \u0026lt;br\u0026gt;*credits:* [Ziv Leyes](https://www.imperva.com/blog/author/ziv/) Over time, those changes altogether made the static routing no longer a sustainable approach.\nAnother new type of construct that emerged is what\u0026rsquo;s called Autonomous System (AS) architecture.\nAn AS can be an Internet Service Provider, a university or an entire corporate network, including multiple locations (IP addresses). Each AS is represented by a unique number called an ASN.\nAt first, AS became the new level that inter-network operates on and routes against. As the internet continues to grow, it calls for a new protocol that can dynamically exchange routes information and decides on the routes depends on many other factors.\nBGP Routing In June 1989, the first version of a new routing protocol, known as the Border Gateway Protocol, was formalized.\nBGP operates on OSI Layer 4, in other words, it understands IP address.\nBGP is primarily designed to exchange routing and reachability information between AS on the Internet but can be used on private networks as well.\nFor example, a dedicated line between on-prem data center and a public cloud service provider (e.g. it\u0026rsquo;s called DirectConnect in AWS) can take advantage of BGP routing as well.\nThere are a range of ASNs that is reserved for private ASN. See here.\nBGP Route Propagation between Neighboring Domains\u0026lt;br\u0026gt;*credits:* [Ziv Leyes](https://www.imperva.com/blog/author/ziv/) Every AS in BGP has its border router that speaks the \u0026ldquo;BGP language\u0026rdquo; and connects to other AS. Sometimes the border router is also called BGP speaker.\nAs of August 2019 there are over 92,000 assigned ASN over the internet.\nBGP operations BGP neighbors, called peers, are established by manual configuration among routers to create a TCP session on port 179.\nBy design, routers running BGP accept advertised routes from other BGP routers by default. This allows for automatic and decentralized routing of traffic across the Internet.\nBGP security considerations Unfortunately BGP is not secure by design.\nDue to the extent to which BGP is embedded in the core systems of the Internet, and the number of different networks operated by many different organizations which collectively make up the Internet, correcting this vulnerability is a technically and economically challenging problem.\n- Wikipedia\nCisco has published an article regarding protecting BGP:Protecting Border Gateway Protocol for the Enterprise\nThe main threats identified in the article above:\nBGP Route Manipulation\nA malicious device alters the contents of the BGP routing table thus prevent traffic from reaching its intended destination without acknowledgement or notification.\nBGP Route Hijacking\nA rogue BGP peer maliciously announces a victim\u0026rsquo;s prefixes in an effort to reroute some or all traffic to itself for untoward purposes. For example, to view traffic that the router would otherwise not be able to read.\nBGP Denial of Service (DoS)\nA malicious host sends unexpected or undesirable BGP traffic to a victim in an attempt to expend all available BGP or compute resources results in a lack of resources for legit BGP traffic processing.\nMisconfiguration\nInadvertent mistakes among BGP peers can also have a disruptive impact on a router\u0026rsquo;s BGP process. Security controls should be applied to mitigate impacts from such kinds of incidents.\nReference BGP for Humans: Making sense of Border Gateway Protocol, Ziv Leyes\nInternet Routing and Traffic Engineering, AWS Architecture Blog\nA Practical Guide to Troubleshooting with Traceroute\nBorder Gateway Protocol, Wikipedia\n","permalink":"https://davidxiao.me/posts/bgp-protocol-in-5-minutes/","summary":"This post attempts to explain the Border Gateway Protocol in plain English: what it is; how it works at a high level and some of the threats from a security perspective.","title":"Explain Border Gateway Protocol in 5 Minutes"},{"content":"Reference Securing cloud-connected devices with Cloud IoT and Microchip\nUnderstanding the AWS IoT Security Model\n","permalink":"https://davidxiao.me/posts/iot-security/","summary":"IoT security is a relatively new area that presents its own unique challenges. This post discusses some of the security risks and possible solutions.","title":"IoT Security"},{"content":"TL;DR\nOWASP Top 10 Web Application Security Risks is a security project commonly referenced in web security and application security space.\nThe following table provides a quick summary of what are the Top 10 and how easy and effective to mitigate them using WAF (web application firewall).\n😈 effective\n🤔 possible with some considerations\n😨 not considered broadly effective or as a primary mitigation\nID OWASP Item Mitigate with WAF 1 Injection 😈 2 Broken Authentication 😨 3 Sensitive Data Exposure 😨 4 XML External Entities XXE 😈 5 Broken Access Control 🤔 6 Security Misconfiguration 🤔 7 Cross-Site Scripting XSS 😈 8 Insecure Deserialization 🤔 9 Using Components with Known Vulnerabilities 😨 10 Insufficient Logging \u0026amp; Monitoring 😨 1. Injection Most common form is SQL injection. There are other forms of injection such as OS and LDAP injection. The attacker’s hostile data can trick the interpreter into executing unintended commands or accessing data without proper authorization.\nWAF: It is usually effective in matching and mitigating such attacks.\n2. Broken Authentication To obtain these credentials, attackers either relying on vulnerabilities in the way client-server communication is implemented or targeting how tokens are generated, stored, transferred or invalidated by the application.\nAttackers then use the credentials to impersonate legitimate users and make requests to your web applications using those tokens.\nWAF is hard to mitigate this type of attacks in general. You might be able to add compromised or stolen tokens to a blacklist WAF rule.\n3. Sensitive Data Exposure Many web applications and APIs do not properly protect sensitive data, such as financial, healthcare, and PII. Attackers may steal or modify such weakly protected data to conduct credit card fraud, identity theft, or other crimes. Sensitive data may be compromised without extra protection, such as encryption at rest or in transit, and requires special precautions when exchanged with the browser.\nWAF is typically hard to mitigate such kind of risks. For example, data has been decrypted from the connection level for WAF inspection so WAF has no impact on enforcing encryption hygiene.\n4. XML External Entities (XXE) Many older or poorly configured XML processors evaluate external entity references within XML documents. External entities can be used to disclose internal files using the file URI handler, internal file shares, internal port scanning, remote code execution, and denial of service attacks.\nWAF can be helpful in mitigating this type of risk as long as the entity references can be matched as pattern.\n5. Broken Access Control It allows internal objects to be manipulated without the requestor’s access permissions being properly validated.\nDepending on the specific workload, this can lead to exposure of unauthorized data, manipulation of internal web application state, path traversal, and local file inclusion.\nWAF can be effective against certain types of such attack by matching dangerous HTTP request patterns that can indicate path traversal attempts, or remote and local file inclusion.\nFor example:\nhttps://example.com/download.php?file=..%2F..%2Fetc%2Fpasswd 6. Security Misconfiguration This is commonly a result of insecure default configurations or enabling verbose error messages containing sensitive information.\nTo mitigate the risk, operating systems, frameworks, libraries and applications be securely configured and stay up-to-date in a timely fashion.\nFor example:\nLeaving default directory listings enabled on production web servers. This allows malicious users to browse for files that are hosted by the web server. WAF can be leveraged to mitigate attempts as long as the HTTP request patterns that attempt to exploit them are recognizable.\nThese patterns, however, are also application-stack specific. They depend on the operating system, web server, frameworks, or the programming languages your code uses.\n7. Cross-Site Scripting XSS XSS flaws occur whenever an application includes untrusted data in a new web page without proper validation or escaping, or updates an existing web page with user-supplied data using a browser API that can create HTML or JavaScript.\nAttackers leverage this flow to execute scripts in the victim’s browser which can retrieve user cookies or redirect the user to malicious sites.\nStored XSS Attacks\naka Persistent XSS. The injected script is permanently stored on the target server such as database or a comment field on a web page etc. The victim retrieves the malicious script when it visits the web page.\nReflected XSS Attacks\naka Non-Persistent XSS. The injected script is reflected back to the same visitor via various forms such as in an error message, search result, or any other response when server response includes some or all of the input sent to the server as part of the request.\nFor example, when username bob log on failed, a vulnerable server produces error message that includes bob without proper escaping and safety checks.\nAn attacker can validate if a target website is vulnerable by constructing a deliberating input as username that includes executable JS code in \u0026lt;script\u0026gt;...\u0026lt;/script\u0026gt; code block and attempting to submit it on the web site. If the JS code can be reflected back to the same visitor\u0026rsquo;s browser and got executed, the validation is successful.\nCombined with the delivery technique discussed in the next paragraphs, attacker would be able to exploit this vulnerability and pull off an attack.\nReflected attacks are delivered to victims via another means such as in a phishing e-mail. When a user is tricked into clicking on a malicious link from the email, it submits a specially crafted form with some JS code to obtain sensitive such as document.cookie to a website such as a online bank website that has XSS vulnerability.\nThe injected JS code travels to the vulnerable bank website, which in turn reflects the JS code back to the user’s browser as part of the HTML. The browser will execute the JS code and allow it to obtain the cookie because the JS appears to come from the same origin.\nFrom OWASP:\nXSS flaws can be difficult to identify and remove from a web application. The best way to find flaws is to perform a security review of the code and search for all places where input from an HTTP request could possibly make its way into the HTML output.\nWAF is relatively easy to mitigate this type of attack in common scenarios because they require specific key HTML tag names in the HTTP request.\n8. Insecure Deserialization Insecure deserialization often leads to remote code execution. Even if deserialization flaws do not result in remote code execution, they can be used to perform attacks, including replay attacks, injection attacks, and privilege escalation attacks.\nWAF is relatively effective in mitigating this type of attacks but there are some considerations:\nIt would require creating custom rules to match known patterns. These patterns are application specific and require more in-depth knowledge of those applications.\nTake into account the limits the service imposes on such rules.\n9. Using Components with Known Vulnerabilities Components, such as libraries, frameworks, and other software modules, run with the same privileges as the application. If a vulnerable component is exploited, such an attack can facilitate serious data loss or server takeover. Applications and APIs using components with known vulnerabilities may undermine application defenses and enable various attacks and impacts.\nWAF is not considered the primary mitigating control for such risks. As secondary control, one can use WAF to filter and block HTTP requests to functionality of such components that you aren’t using in your applications. This helps reduce the attack surface of those components.\n10. Insufficient Logging \u0026amp; Monitoring Insufficient logging and monitoring, coupled with missing or ineffective integration with incident response, allows attackers to further attack systems, maintain persistence, pivot to more systems, and tamper, extract, or destroy data. Most breach studies show time to detect a breach is over 200 days, typically detected by external parties rather than internal processes or monitoring.\nWAF is not considered the primary mitigating control for such risks. WAF can be produce its own logs for further consumption and analysis.\n","permalink":"https://davidxiao.me/posts/owasp-top-ten/","summary":"The OWASP Top 10 represents a broad consensus about the most common and critical security risks to web applications. It can be used as reference for web application security.","title":"OWASP Top Ten"},{"content":"Last Updated: Oct 13, 2020\nThis Privacy Policy (\u0026ldquo;Policy\u0026rdquo;) outlines the personal information handling practices on the main site davidxiao.me and the mirror site davxiao.github.io (\u0026ldquo;this site\u0026rdquo;, \u0026ldquo;I,\u0026rdquo;, \u0026ldquo;me,\u0026rdquo; and \u0026ldquo;my\u0026rdquo;).\nBefore you post any comments on this site, please carefully review this Privacy Policy. By posting comments on this site, you understand that your information will be collected, used, and disclosed as outlined in this Privacy Policy.\nIF YOU DO NOT AGREE TO THIS PRIVACY POLICY, PLEASE DO NOT POST ANY COMMENTS ON THIS SITE.\nTypes and Uses of Personal Information If you are browsing this site but not posting any comments on this site, no personal information is collected.\nHowever, if you wish to post comments on this site, you would first need to complete Social Login on this site with one of the following Identity Providers: Google, Twitter, Facebook, Github.\nYour name, language preference, and profile pictures will be shared with this site\u0026rsquo;s comment engine by the identity provider upon completion of the social login.\nComment Engine I use Remark42 as this site\u0026rsquo;s comment engine. It is open source and privacy focused. It saves the minimum amount of information required to be able to show comments. Such information includes name, avatar (to be shown with comment message) and your id in service you used to login (to understand it\u0026rsquo;s still you when you log in next time). Id is hashed, there\u0026rsquo;s no easy way for me to identify the profile you used for login.\nFind more details about Remark42\u0026rsquo;s privacy in here and be sure to check its source code in case you have further questions.\nAggregate Information (non-personally identifiable) This site uses Google Analytics to collect aggregated information including browser information and site traffic statistics. Find more detail about Google Analytics privacy information here.\nThis site does not use any Web Banner or Online advertising.\nCookies A cookie is a small text file that is stored on a user\u0026rsquo;s computer for record-keeping purposes.\nThis website uses cookies to work properly. Cookies are small pieces of information your browser sends to server with every request. Here\u0026rsquo;s the list of cookies used on this website:\nGoogle Analytics uses cookies for its analytics purposes. When logging in to post a comment, comment engine requests two cookies: one to determine it\u0026rsquo;s still you; the other one is added as an information security measure against Cross-site request forgery. If you reject cookies, you may still visit this site, but your ability to use some areas of this site, such as comments, will require logging in.\nLinks to Other Sites This site contains links to other sites that are not owned or controlled by davidxiao.me. Please be aware that I am not responsible for the privacy practices of such other sites. This privacy statement applies only to information collected by this site.\nChanges in This Privacy Statement If this privacy policy needs to be updated, the updated version will be posted to this privacy statement, the homepage, and other places I deem appropriate.\nQuestions If you have any questions about the Privacy Policy, you can reach out to me at root@davidxiao.me\n","permalink":"https://davidxiao.me/privacy/","summary":"Last Updated: Oct 13, 2020\nThis Privacy Policy (\u0026ldquo;Policy\u0026rdquo;) outlines the personal information handling practices on the main site davidxiao.me and the mirror site davxiao.github.io (\u0026ldquo;this site\u0026rdquo;, \u0026ldquo;I,\u0026rdquo;, \u0026ldquo;me,\u0026rdquo; and \u0026ldquo;my\u0026rdquo;).\nBefore you post any comments on this site, please carefully review this Privacy Policy. By posting comments on this site, you understand that your information will be collected, used, and disclosed as outlined in this Privacy Policy.\nIF YOU DO NOT AGREE TO THIS PRIVACY POLICY, PLEASE DO NOT POST ANY COMMENTS ON THIS SITE.","title":"Privacy Policy"},{"content":" Step by Step Alice and Bob agree publicly on a prime modulus 17, and a generator 3\nAlice selects a private random number 15\nAlice calculates 3^15 mod 17 (three to the power fifteen mod seventeen), sends the result 6 publicly to Bob\nBob selects a private random number 13\nBob calculates 3^13 mod 17, sends the result 12 publicly to Alice\nAlice takes Bob\u0026rsquo;s public result and raise it to the power of her own private number and mod it, i.e. calculates 12^15 mod 17, the result is 10\nBob does the same procedure as Alice, i.e. calculates 6^13 mod 17, the result is 10 (the same)\nAn analysis of how calculation in step 6 and step 7 are done shows that either side actually did the same calculation with the exponents in a different order. In mathematics, when you flip the exponent, the result doesn\u0026rsquo;t change.\nExponents in a different order ","permalink":"https://davidxiao.me/posts/diffie-hellman-key-exchange-in-a-nutshell/","summary":"DH key exchange is a critical component in virtually every PKI implementation. Having a working knowledge of what it is and how it works would help in understanding PKI as a whole.","title":"Diffie-Hellman Key Exchange in a Nutshell"},{"content":"TL;DR\nWhen TLS mutual authentication is put in place between controller node and work nodes in a cluster setting, it\u0026rsquo;s not uncommon to see client certificates signed by either self-signed CA or private CA.\nThis post covers how to generate a self-signed root CA and to sign a client certificate using openssl. In cases when private CA is employed instead, the client certificate signing portion is still relevant.\nThe configuration described in this post is for testing only. It is NOT secure for production use. Self-signed CA Certificate Before we start, let\u0026rsquo;s create a root directory called tls-cert.\nUnder the root directory, we create a sub directory and some files:\n[tls-cert] mkdir ca [tls-cert] mkdir ca/ca.db.certs [tls-cert] touch ca/ca.db.index [tls-cert] echo \u0026#34;123456\u0026#34; \u0026gt; ca/ca.db.serial Next step is to generate a CA key: in this example it\u0026rsquo;s 2048-bit RSA:\n[tls-cert] openssl genrsa -out ca/ca.key 2048 Next is to create a self-signed X509 certificate out of the CA key. The client keys will be signed with it later.\n[tls-cert] openssl req -new -x509 -days 365 -key ca/ca.key -out ca/ca.crt You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter \u0026#39;.\u0026#39;, the field will be left blank. ----- Country Name (2 letter code) []:. State or Province Name (full name) []:. Locality Name (eg, city) []:. Organization Name (eg, company) []:ca-org-name Organizational Unit Name (eg, section) []:ca-orgunit-name Common Name (eg, fully qualified host name) []:ca-common-name Email Address []:admin@ca.com [tls-cert] Client certificate Now that we\u0026rsquo;ve generated CA certificate, it\u0026rsquo;s time to generate a private key for the client and create a CSR (Certificate Signing Request)for the key:\n[tls-cert] openssl req -new -newkey rsa:2048 -nodes -keyout client.key -out client-key-sign-req.pem Generating a 2048 bit RSA private key ...................................................................................................+++ ........+++ writing new private key to \u0026#39;client.key\u0026#39; ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter \u0026#39;.\u0026#39;, the field will be left blank. ----- Country Name (2 letter code) []:. State or Province Name (full name) []:. Locality Name (eg, city) []:. Organization Name (eg, company) []:client-org-name Organizational Unit Name (eg, section) []:client-orgunit-name Common Name (eg, fully qualified host name) []:client-common-name Email Address []:admin@client.com Please enter the following \u0026#39;extra\u0026#39; attributes to be sent with your certificate request A challenge password []: [tls-cert] openssl needs to know a few parameters in order to sign the CSR. Let\u0026rsquo;s put all the parameters into a file and save it as ca.conf:\nWith that sorted out, the last step is to sign the CSR file client-key-sign-req.pem:\n[tls-cert] openssl ca -config ca.conf -out client-certificate.pem.crt -infiles client-key-sign-req.pem Using configuration from ca.conf Check that the request matches the signature Signature ok The Subject\u0026#39;s Distinguished Name is as follows organizationName :ASN.1 12:\u0026#39;client-org-name\u0026#39; organizationalUnitName:ASN.1 12:\u0026#39;client-orgunit-name\u0026#39; commonName :ASN.1 12:\u0026#39;client-common-name\u0026#39; emailAddress :IA5STRING:\u0026#39;admin@client.com\u0026#39; Certificate is to be certified until Oct 11 02:08:05 2021 GMT (365 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated [tls-cert] Conclusion We\u0026rsquo;ve produced the following files in this example:\nca/ca.key: private key of the self-signed root CA. It must be kept secret at all times.\nca/ca.crt: CA certificate of the root CA. It should be made available to whichever party that needs to verify the certificates that were signed by the root CA.\nclient.key: private key of the client certificate. It must be kept secret except during the client authentication process.\nclient-certificate.pem.crt: CA certificate of the client. It should be made available to whichever party that needs to verify the identity of the client.\n","permalink":"https://davidxiao.me/posts/sign-client-certificate-using-self-signed-ca-certificate/","summary":"In a cluster setting where TLS mutual authentication is required, it\u0026rsquo;s not uncommon to see client certificates signed by either self-signed root CA or private CA.","title":"Sign Client Certificate Using Self Signed CA Certificate"},{"content":"TLDR;\nThis post is my collection of python3 code snippets. It comes in handy when manipulating string and list.\nsingle or double quoted string In Python, such sequence of characters is included inside single or double quotes. There is no difference in single or double quoted string. Both can be used interchangeably.\nRemove a few elements from the beginning Slice out the first 2 elements and return the rest.\ns = \u0026#39;abcdef\u0026#39; res = s[2:] # res = \u0026#39;cdef\u0026#39; Remove a few elements from the end Slice out the last 3 elements and return the rest.\ns = \u0026#39;abcdef\u0026#39; res = s[:-3] # res = \u0026#39;abc\u0026#39; Retrieve the first few elements only s = \u0026#39;abcdef\u0026#39; res = s[:3] # res = \u0026#39;abc\u0026#39; Retrieve the last few elements only s = \u0026#39;abcdef\u0026#39; res = s[-3:] # res = \u0026#39;def\u0026#39; Retrieve elements from the middle s = \u0026#39;0123456789\u0026#39; res = s[3:7] # res = \u0026#39;3456\u0026#39; Remove single element by zero-index Remove \u0026rsquo;d\u0026rsquo; by its index: 3\nIf one-based position is given instead of zero-based index, convert from position to index. e.g. index = position - 1\ns = \u0026#39;abcdef\u0026#39; res = s[:3]+s[4:] # res = \u0026#39;abcef\u0026#39; Remove multiple elements by zero-index Remove 4 element by its index starting from index 3\ns = [\u0026#39;0123456789\u0026#39;] start_index = 3 to_cut = 4 res = s[:start_index]+s[start_index+to_cut:] # res = \u0026#39;012789\u0026#39; # zero-index 4 to 7 are removed Remove/Replace element by value string version\ns = \u0026#39;abcdef\u0026#39; res = s.replace(\u0026#39;c\u0026#39;, \u0026#39;w\u0026#39;) # replace \u0026#39;c\u0026#39; with \u0026#39;w\u0026#39; # res = \u0026#39;abwdef\u0026#39; list version\ns = [\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;d\u0026#39;,\u0026#39;e\u0026#39;,\u0026#39;f\u0026#39;] res = s try: pos = res.index(\u0026#39;c\u0026#39;) except: pos = -1 if (pos != -1): res.pop(pos) #remove \u0026#39;c\u0026#39; res.insert(pos, \u0026#39;w\u0026#39;) #replace with \u0026#39;w\u0026#39; # res = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;w\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;f\u0026#39;] convert string to list character-wise s = \u0026#39;abcdef\u0026#39; res=[] res[:0]=s # res = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;f\u0026#39;] convert list to string s = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;f\u0026#39;] res = \u0026#39;\u0026#39;.join(s) # res = \u0026#39;abcdef\u0026#39; Split string into word list s = \u0026#39;welcome to the jungle\u0026#39; res = s.split() # res = [\u0026#39;welcome\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;jungle\u0026#39;] Join words into a string with whitespace s = [\u0026#39;welcome\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;jungle\u0026#39;] res = \u0026#39; \u0026#39;.join(s) # res = \u0026#39;welcome to the jungle\u0026#39; iterate list items to generate a new list s = [\u0026#39;welcome\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;jungle\u0026#39;] res = [x*2 for x in s] #res = [\u0026#39;welcomewelcome\u0026#39;, \u0026#39;toto\u0026#39;, \u0026#39;thethe\u0026#39;, \u0026#39;junglejungle\u0026#39;] swap case of a string (uppercase to lowercase and vice versa) python provides a string function that does exactly that:\nres = s.swapcase()\ndef swapcase(c): if c.isupper(): return c.lower() if c.islower(): return c.upper() return c s = \u0026#39;Welcome To The Jungle\u0026#39; res = [swapcase(x) for x in s] res = \u0026#39;\u0026#39;.join(res) # res = \u0026#39;wELCOME tO tHE jUNGLE\u0026#39; produce list or string in reverse order both string and list\ns = [\u0026#39;welcome\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;jungle\u0026#39;] res = s[::-1] # res = [\u0026#39;jungle\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;welcome\u0026#39;] find all occurances in a string s = \u0026#39;welcome to to the jungle\u0026#39; find_s = \u0026#39;to\u0026#39; find_len = len(find_s) pointer = 0 while True: index = s.find(find_s,pointer) if index == -1: break print(\u0026#39;found at index \u0026#39;, index) pointer += index pointer += find_len # found at index 8 # found at index 11 replace all occurances in a string s = \u0026#39;welcome to to the jungle\u0026#39; find_s = \u0026#39;to\u0026#39; replace_s = \u0026#39;what\u0026#39; res = s.replace(find_s, replace_s) # res = \u0026#39;welcome what what the jungle\u0026#39; detect duplicate items in a list method 1: loop\ndef has_dup(lst): flag = 0 for i in range (len(lst)): for j in range (i+1,len(lst)): if (lst[i] == lst[j]): flag = 1 if (flag == 1): return True else: return False l=[1, 2, 3, 4, 5, 6] print(has_dup(l)) # False method 2: create a temp set\nset is a series of hashable objects, in a way it\u0026rsquo;s like dict but the main difference between the two is that a dict item contains both key and value while a set item contains only key.\ns = [1,2,3,1,5,1] res = set(s) if (len(s) == len(res)): print(\u0026#39;no duplicates found\u0026#39;) else: print(\u0026#39;duplicates found\u0026#39;) # duplicates found remove duplicate words in a string def unique_list(l): ulist = [] [ulist.append(x) for x in l if x not in ulist] return ulist s = \u0026#39;calvin klein design dress calvin klein\u0026#39; res = \u0026#39; \u0026#39;.join(unique_list(s.split())) # res = \u0026#39;calvin klein design dress\u0026#39; ROT13-like conversion over a string Be aware ASCII code can be outside of printable range.\ns = \u0026#39;This is a plaintext message\u0026#39; offset = 1 res = \u0026#39;\u0026#39;.join([chr(ord(c)+offset) for c in s]) # res = \u0026#39;Uijt!jt!b!qmbjoufyu!nfttbhf\u0026#39; Use regex to split complicated string into words Compared to string.split() method, regex approach preserves separators in the result so that it\u0026rsquo;s possible to re-construct the original string from the result.\nimport re s = \u0026#39;Words, words, words. \u0026#39; res=re.split(\u0026#39;([,. ]+)\u0026#39;, s) # \u0026#39;(...)\u0026#39; enables the matched separators preserved in the result list. # res = [\u0026#39;Words\u0026#39;, \u0026#39;, \u0026#39;, \u0026#39;words\u0026#39;, \u0026#39;, \u0026#39;, \u0026#39;words\u0026#39;, \u0026#39;. \u0026#39;, \u0026#39;\u0026#39;] Use regex to search a pattern import re s = \u0026#39;Welcome to the jungle...\u0026#39; match = re.search(\u0026#39;to +the +(\\w+)\u0026#39;, s) if match: res = match.group(0) # match \u0026#39;to +the +(\\w+)\u0026#39; as a whole # res = \u0026#39;to the jungle\u0026#39; res = match.group(1) # match \u0026#39;(\\w+)\u0026#39; portion # res = \u0026#39;jungle\u0026#39; else: print(\u0026#39;no match found\u0026#39;) count number of occurances of substring string.count() approach\ns = \u0026#39;jungle and jungle and another jungle...\u0026#39; res = s.count(\u0026#39;jungle\u0026#39;)) # res = 3 regular expression approach\nimport re s = \u0026#39;Welcome to the jungle. It is a big jungle with many animals. Lion is the king of the jungle.\u0026#39; match = re.findall(\u0026#39;jungle\u0026#39;, s) if match: res = len(match) # return a list of string else: print(\u0026#39;no match found\u0026#39;) escape string into html text For HTML, it needs to escape the following:\n\u0026lt; to \u0026amp;lt;\n\u0026gt; to \u0026amp;gt;\n\u0026amp; to \u0026amp;amp;\ns = \u0026#39;escape html string \u0026lt;body\u0026gt;\u0026amp;\u0026lt;/body\u0026gt;\u0026#39; res = s.replace(\u0026#39;\u0026amp;\u0026#39;, \u0026#39;\u0026amp;amp;\u0026#39;).replace(\u0026#39;\u0026gt;\u0026#39;, \u0026#39;\u0026amp;gt;\u0026#39;).replace(\u0026#39;\u0026lt;\u0026#39;, \u0026#39;\u0026amp;lt;\u0026#39;) # res = \u0026#39;escape html string \u0026amp;lt;body\u0026amp;gt;\u0026amp;amp;\u0026amp;lt;/body\u0026amp;gt;\u0026#39; Use global variable maxlen = 0 def wordlength(x): global maxlen # to access global variable within a function if len(x) \u0026gt; maxlen: maxlen = len(x) s = \u0026#39;unsafe html string \u0026lt;body\u0026gt;\u0026amp;\u0026lt;/body\u0026gt;\u0026#39; words = s.split() [wordlength(x) for x in words] # maxlen = 14 get both index and value when looping a list for both string and list\ns = \u0026#39;abcdef\u0026#39; for i, value in enumerate(s): print (\u0026#34;index \u0026#34;, i, \u0026#34;value \u0026#34;, value) create a list of empty items res = [None]*4 # res = [None, None, None, None] # Note this is different res = [\u0026#39;\u0026#39;]*4 # res = [\u0026#39;\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;\u0026#39;] determine if a list is sorted s = [1,2,3,4,5,6] if (s == sorted(s)): print(\u0026#34;sorted\u0026#34;) else: print(\u0026#34;not sorted\u0026#34;) looping a dict ages = { \u0026#34;Peter\u0026#34;: 10, \u0026#34;Isabel\u0026#34;: 11, \u0026#34;Anna\u0026#34;: 9, \u0026#34;Thomas\u0026#34;: 10, \u0026#34;Bob\u0026#34;: 10, \u0026#34;Joseph\u0026#34;: 11, \u0026#34;Maria\u0026#34;: 12, \u0026#34;Gabriel\u0026#34;: 10, } # loop to get all keys for x in ages: print(x) # loop to get all value for x in ages: print(ages[x]) # loop to get both keys and values for name, age in ages.items(): print(name, age) nested dict students = { \u0026#34;Peter\u0026#34;: {\u0026#34;age\u0026#34;: 10, \u0026#34;address\u0026#34;: \u0026#34;Lisbon\u0026#34;}, \u0026#34;Isabel\u0026#34;: {\u0026#34;age\u0026#34;: 11, \u0026#34;address\u0026#34;: \u0026#34;Sesimbra\u0026#34;}, \u0026#34;Anna\u0026#34;: {\u0026#34;age\u0026#34;: 9, \u0026#34;address\u0026#34;: \u0026#34;Lisbon\u0026#34;}, } for p_id, p_info in students.items(): print(\u0026#34;\\nPerson Name:\u0026#34;, p_id) for key in p_info: print(key + \u0026#39;:\u0026#39;, p_info[key]) find the max value in a dict and return the key ages = { \u0026#34;Peter\u0026#34;: 10, \u0026#34;Isabel\u0026#34;: 11, \u0026#34;Anna\u0026#34;: 9, \u0026#34;Thomas\u0026#34;: 10, \u0026#34;Bob\u0026#34;: 10, \u0026#34;Joseph\u0026#34;: 11, \u0026#34;Maria\u0026#34;: 12, \u0026#34;Gabriel\u0026#34;: 10, } value = list(ages.values()) key = list(ages.keys()) print (key[value.index(max(value))]) # Maria use nested list to cache multiplication results matrix = [] for i in range(10): #0-9 row = [] for j in range(10): #0-9 row.append(i*j) matrix.append(row) def multiply(x,y): try: return matrix[x][y] except: return x*y print(multiply(6,9)) # 54 print(multiply(66,99)) # 6534 ","permalink":"https://davidxiao.me/posts/python3-cheatsheet/","summary":"This post is my collection of python3 code snippets including string and list manipulation.","title":"Python3 CheatSheet"},{"content":"Overview When auditing system events or performing an investigation to understand what happened, it is imperative to identify the IAM principal, to establish traceability and timelines.\nIn context of AWS CloudTrail, it means looking up events pertaining to the IAM principal and actions in question as well as looking for useful information inside such events.\nWhen a user assumes role cross-account in a multi-account environment, it can be done two ways: either programatically or via AWS management console.\nSince either way generates different CloudTrail events, I will disuss two examples in this post respectively.\nAssume Role Programmatically Cross-Account Assume Role via AWS Console Cross-Account A typical investigation flow that involves cross-account assumerole goes like this:\nStep 1: Identify an event on CloudTrail that needs investigation\nStep 2: Identify the closest AssumeRole event that happens before the event in question\nStep 3: Locate the closest SwitchRole event that happens at the same time of AssumeRole. If found, it indicates the user session was established via AWS Console\nStep 4: If you have access to the Identity Account, locate the AssumeRole\nIn the following example, we investigate a \u0026ldquo;suspicious\u0026rdquo; CreateUser event.\nIdentity Account: 203016562928\nIAM username: bob@example.com\nRole Account: 776613361644\nRole Name: assume-admin-role-example\nCreateUser\n{ \u0026#34;eventVersion\u0026#34;: \u0026#34;1.05\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;AssumedRole\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;AROAJBP4A5WSXVNY72RLE:bob@example.com\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:sts::776613361644:assumed-role/assume-admin-role-example/bob@example.com\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;776613361644\u0026#34;, \u0026#34;accessKeyId\u0026#34;: \u0026#34;ASIA3JUODO7W6YEVI655\u0026#34;, \u0026#34;sessionContext\u0026#34;: { \u0026#34;sessionIssuer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Role\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;AROAJBP4A5WSXVNY72RLE\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:iam::776613361644:role/assume-admin-role-example\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;776613361644\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;assume-admin-role-example\u0026#34; }, \u0026#34;webIdFederationData\u0026#34;: {}, \u0026#34;attributes\u0026#34;: { \u0026#34;mfaAuthenticated\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;creationDate\u0026#34;: \u0026#34;2020-09-17T19:04:10Z\u0026#34; } } }, \u0026#34;eventTime\u0026#34;: \u0026#34;2020-09-17T19:05:08Z\u0026#34;, \u0026#34;eventSource\u0026#34;: \u0026#34;iam.amazonaws.com\u0026#34;, \u0026#34;eventName\u0026#34;: \u0026#34;CreateUser\u0026#34;, \u0026#34;awsRegion\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;sourceIPAddress\u0026#34;: \u0026#34;75.15.154.15\u0026#34;, \u0026#34;userAgent\u0026#34;: \u0026#34;console.amazonaws.com\u0026#34;, \u0026#34;requestParameters\u0026#34;: { \u0026#34;userName\u0026#34;: \u0026#34;test-no-permission\u0026#34;, \u0026#34;tags\u0026#34;: [] }, \u0026#34;responseElements\u0026#34;: { \u0026#34;user\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;test-no-permission\u0026#34;, \u0026#34;userId\u0026#34;: \u0026#34;AIDA3JUODO7W7VCWFDJMM\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:iam::776613361644:user/test-no-permission\u0026#34;, \u0026#34;createDate\u0026#34;: \u0026#34;Sep 17, 2020 7:05:08 PM\u0026#34; } }, \u0026#34;requestID\u0026#34;: \u0026#34;cc58c060-fe96-4678-b0bf-b888f12bf008\u0026#34;, \u0026#34;eventID\u0026#34;: \u0026#34;38d0221b-61e0-47d6-9c45-7eb2dc55125b\u0026#34;, \u0026#34;eventType\u0026#34;: \u0026#34;AwsApiCall\u0026#34;, \u0026#34;recipientAccountId\u0026#34;: \u0026#34;776613361644\u0026#34; } AssumeRole\n{ \u0026#34;eventVersion\u0026#34;: \u0026#34;1.05\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;AWSAccount\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;AIDAS6RF25DXSM2CA5KAD\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;203016562928\u0026#34; }, \u0026#34;eventTime\u0026#34;: \u0026#34;2020-09-17T19:04:10Z\u0026#34;, \u0026#34;eventSource\u0026#34;: \u0026#34;sts.amazonaws.com\u0026#34;, \u0026#34;eventName\u0026#34;: \u0026#34;AssumeRole\u0026#34;, \u0026#34;awsRegion\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;sourceIPAddress\u0026#34;: \u0026#34;AWS Internal\u0026#34;, \u0026#34;userAgent\u0026#34;: \u0026#34;AWS Internal\u0026#34;, \u0026#34;requestParameters\u0026#34;: { \u0026#34;roleArn\u0026#34;: \u0026#34;arn:aws:iam::776613361644:role/assume-admin-role-example\u0026#34;, \u0026#34;roleSessionName\u0026#34;: \u0026#34;bob@example.com\u0026#34; }, \u0026#34;responseElements\u0026#34;: { \u0026#34;credentials\u0026#34;: { \u0026#34;accessKeyId\u0026#34;: \u0026#34;ASIA3JUODO7WTIO2PI64\u0026#34;, \u0026#34;expiration\u0026#34;: \u0026#34;Sep 17, 2020 8:04:10 PM\u0026#34;, \u0026#34;sessionToken\u0026#34;: \u0026#34;IQoJb3JpZ2luX2VjEAMaCXVzLWVhc3QtMSJIMEYCIQDsEwOWhE/9cNh+Xpg+V6r8ug3ULRnoOCPNCQhorh13xgIhAIfc+u8ttYNsjLQJRMvo7EnDXMkOAViMFuU7Mma8zjGEKqMCCBwQARoMNzc2NjEzMzYxNjQ1Igyt06eMsyzHkQJRK84qgAIF/jUW99caWE07piwWI2EGTHU2KMx6ioRz3uDbDS24GKy3XvQRalC+5YTZoOQDQvpziRmO33BEzM6Ws5TBTggo/yXGAJRQthB8IqiGkbsClbOG8cYsuhRXK3+yK8OHhCSfr0ehO2SYNiaqEClyT9n8QtEmkQawN56IiOoE9HBzTA7xxYbj7XULL/okog7D3l18NG32rxhHS1ACDN3ro9RGjrPicn9PHFfBqvK+uP3JJQVlcQZu6yGVvtH3rIiYdwAo5bSEc3G9G/LSEiwh47o7NrTOzrRsznARBgefSPs9K3qIgMpZHs3DgqJ0TID1k5y1w4KlPvL36C/+LEmxckNYMKrmjvsFOpwBGDQAVghEOsvntE328Yt2M9Yv0x55cZ5RPJ2pGtQW4geb8+aT2ThZ1zSGwlnvMM8TE+HAwUs0+GQwZbp5UpIiDYLzUeZ0pYWVBHmv/YzN3w0bSKTrC8Jc/0aAaUnxmKMkH5AWO6pBelw8KtVIvd9BwKgBQKVo+tsAVGEdKbVTlwOvLcuhWonCvjoxPCiPgNR05HF8QANbpWet0p+k\u0026#34; }, \u0026#34;assumedRoleUser\u0026#34;: { \u0026#34;assumedRoleId\u0026#34;: \u0026#34;AROAJBP4A5WSXVNY72RLE:bob@example.com\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:sts::776613361644:assumed-role/assume-admin-role-example/bob@example.com\u0026#34; } }, \u0026#34;requestID\u0026#34;: \u0026#34;fdbb008c-63ce-4207-8171-b041d6f38672\u0026#34;, \u0026#34;eventID\u0026#34;: \u0026#34;40b4d219-0448-436f-9420-cdd3dc654b44\u0026#34;, \u0026#34;resources\u0026#34;: [ { \u0026#34;accountId\u0026#34;: \u0026#34;776613361644\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;AWS::IAM::Role\u0026#34;, \u0026#34;ARN\u0026#34;: \u0026#34;arn:aws:iam::776613361644:role/assume-admin-role-example\u0026#34; } ], \u0026#34;eventType\u0026#34;: \u0026#34;AwsApiCall\u0026#34;, \u0026#34;recipientAccountId\u0026#34;: \u0026#34;776613361644\u0026#34;, \u0026#34;sharedEventID\u0026#34;: \u0026#34;19ee34b2-52bd-4dfa-8c8e-cf68344062a6\u0026#34; } SwitchRole\n{ \u0026#34;eventVersion\u0026#34;: \u0026#34;1.05\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;AssumedRole\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;AROAJBP4A5WSXVNY72RLE:bob@example.com\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:sts::776613361644:assumed-role/assume-admin-role-example/bob@example.com\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;776613361644\u0026#34; }, \u0026#34;eventTime\u0026#34;: \u0026#34;2020-09-17T19:04:10Z\u0026#34;, \u0026#34;eventSource\u0026#34;: \u0026#34;signin.amazonaws.com\u0026#34;, \u0026#34;eventName\u0026#34;: \u0026#34;SwitchRole\u0026#34;, \u0026#34;awsRegion\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;sourceIPAddress\u0026#34;: \u0026#34;75.15.154.15\u0026#34;, \u0026#34;userAgent\u0026#34;: \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36\u0026#34;, \u0026#34;requestParameters\u0026#34;: null, \u0026#34;responseElements\u0026#34;: { \u0026#34;SwitchRole\u0026#34;: \u0026#34;Success\u0026#34; }, \u0026#34;additionalEventData\u0026#34;: { \u0026#34;SwitchFrom\u0026#34;: \u0026#34;arn:aws:iam::203016562928:user/bob@example.com\u0026#34;, \u0026#34;RedirectTo\u0026#34;: \u0026#34;https://console.aws.amazon.com/console/home\u0026#34; }, \u0026#34;eventID\u0026#34;: \u0026#34;70627092-0c9c-4163-9975-42ffcc50a37a\u0026#34;, \u0026#34;eventType\u0026#34;: \u0026#34;AwsConsoleSignIn\u0026#34;, \u0026#34;recipientAccountId\u0026#34;: \u0026#34;776613361644\u0026#34; } in the source account look up for AssumeRole around the same time with the same \u0026ldquo;sharedEventID\u0026rdquo; AssumeRole\n{ \u0026#34;eventVersion\u0026#34;: \u0026#34;1.05\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;IAMUser\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;AIDAS6RF25DXSM2CA5KAD\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:iam::203016562928:user/bob@example.com\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;203016562928\u0026#34;, \u0026#34;accessKeyId\u0026#34;: \u0026#34;ASIAS6RF25DXQR3PH2AX\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;bob@example.com\u0026#34;, \u0026#34;sessionContext\u0026#34;: { \u0026#34;sessionIssuer\u0026#34;: {}, \u0026#34;webIdFederationData\u0026#34;: {}, \u0026#34;attributes\u0026#34;: { \u0026#34;mfaAuthenticated\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;creationDate\u0026#34;: \u0026#34;2020-09-17T13:58:45Z\u0026#34; } }, \u0026#34;invokedBy\u0026#34;: \u0026#34;AWS Internal\u0026#34; }, \u0026#34;eventTime\u0026#34;: \u0026#34;2020-09-17T19:04:10Z\u0026#34;, \u0026#34;eventSource\u0026#34;: \u0026#34;sts.amazonaws.com\u0026#34;, \u0026#34;eventName\u0026#34;: \u0026#34;AssumeRole\u0026#34;, \u0026#34;awsRegion\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;sourceIPAddress\u0026#34;: \u0026#34;AWS Internal\u0026#34;, \u0026#34;userAgent\u0026#34;: \u0026#34;AWS Internal\u0026#34;, \u0026#34;requestParameters\u0026#34;: { \u0026#34;roleArn\u0026#34;: \u0026#34;arn:aws:iam::776613361644:role/assume-admin-role-example\u0026#34;, \u0026#34;roleSessionName\u0026#34;: \u0026#34;bob@example.com\u0026#34; }, \u0026#34;responseElements\u0026#34;: { \u0026#34;credentials\u0026#34;: { \u0026#34;accessKeyId\u0026#34;: \u0026#34;ASIA3JUODO7WTIO2PI64\u0026#34;, \u0026#34;expiration\u0026#34;: \u0026#34;Sep 17, 2020 8:04:10 PM\u0026#34;, \u0026#34;sessionToken\u0026#34;: \u0026#34;IQoJb3JpZ2luX2VjEAMaCXVzLWVhc3QtMSJIMEYCIQDsEwOWhE/9cNh+Xpg+V6r8ug3ULRnoOCPNCQhorh13xgIhAIfc+u8ttYNsjLQJRMvo7EnDXMkOAViMFuU7Mma8zjGEKqMCCBwQARoMNzc2NjEzMzYxNjQ1Igyt06eMsyzHkQJRK84qgAIF/jUW99caWE07piwWI2EGTHU2KMx6ioRz3uDbDS24GKy3XvQRalC+5YTZoOQDQvpziRmO33BEzM6Ws5TBTggo/yXGAJRQthB8IqiGkbsClbOG8cYsuhRXK3+yK8OHhCSfr0ehO2SYNiaqEClyT9n8QtEmkQawN56IiOoE9HBzTA7xxYbj7XULL/okog7D3l18NG32rxhHS1ACDN3ro9RGjrPicn9PHFfBqvK+uP3JJQVlcQZu6yGVvtH3rIiYdwAo5bSEc3G9G/LSEiwh47o7NrTOzrRsznARBgefSPs9K3qIgMpZHs3DgqJ0TID1k5y1w4KlPvL36C/+LEmxckNYMKrmjvsFOpwBGDQAVghEOsvntE328Yt2M9Yv0x55cZ5RPJ2pGtQW4geb8+aT2ThZ1zSGwlnvMM8TE+HAwUs0+GQwZbp5UpIiDYLzUeZ0pYWVBHmv/YzN3w0bSKTrC8Jc/0aAaUnxmKMkH5AWO6pBelw8KtVIvd9BwKgBQKVo+tsAVGEdKbVTlwOvLcuhWonCvjoxPCiPgNR05HF8QANbpWet0p+k\u0026#34; }, \u0026#34;assumedRoleUser\u0026#34;: { \u0026#34;assumedRoleId\u0026#34;: \u0026#34;AROAJBP4A5WSXVNY72RLE:bob@example.com\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:sts::776613361644:assumed-role/assume-admin-role-example/bob@example.com\u0026#34; } }, \u0026#34;requestID\u0026#34;: \u0026#34;fdbb008c-63ce-4207-8171-b041d6f38672\u0026#34;, \u0026#34;eventID\u0026#34;: \u0026#34;14fb06e3-5649-4fc3-a274-226ba85c8be6\u0026#34;, \u0026#34;resources\u0026#34;: [ { \u0026#34;accountId\u0026#34;: \u0026#34;776613361644\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;AWS::IAM::Role\u0026#34;, \u0026#34;ARN\u0026#34;: \u0026#34;arn:aws:iam::776613361644:role/assume-admin-role-example\u0026#34; } ], \u0026#34;eventType\u0026#34;: \u0026#34;AwsApiCall\u0026#34;, \u0026#34;recipientAccountId\u0026#34;: \u0026#34;203016562928\u0026#34;, \u0026#34;sharedEventID\u0026#34;: \u0026#34;19ee34b2-52bd-4dfa-8c8e-cf68344062a6\u0026#34; } Reference How to Audit Cross-Account Roles Using AWS CloudTrail and Amazon CloudWatch Events\n","permalink":"https://davidxiao.me/posts/how-to-audit-user-activities-using-aws-cloudtrail-part-2/","summary":"This is the second post of a series that demonstrates how to leverage AWS CloudTrail in auditing user actions. This post is focused on cross account access.","title":"Audit User Actions Using CloudTrail - Part 2"},{"content":"","permalink":"https://davidxiao.me/posts/how-to-audit-user-activities-using-aws-cloudtrail-part-1/","summary":"AWS CloudTrail is an essential service that records user actions and systems events. This is the first post of a series that demonstrates how to leverage CloudTrail to identify the IAM principal and establish timelines. This post is focused on same account access.","title":"Audit User Actions Using CloudTrail - Part 1"},{"content":"Why Do I Care Cloudtrail is an essential service in AWS that provides the source of truth on what has happened at API and event level.\nWhether you are troubleshooting or investigating something on AWS, being able to look up user identity across the Cloudtrail event logs can be very helpful.\nBy default AWS provides 90 days of event history and you can look up on key fields such as User name, event time or event id.\nIn some cases that\u0026rsquo;s all you need.\nBut there are cases where you need to go beyond the 90 days and want to be able to extract user identity information from Cloudtrail logs directly.\nFor example, you may wish to write a Lambda function to auto-tag any new EC2 instances with username of the creator, eventid, eventtime when a user is creating new EC2 instances.\nFor another example, you may need to search history go past 90 days to look for information like WHO did WHAT and WHEN.\nIn those cases, understand the JSON structure of Cloudtrail log and specifically the identity related portion comes handy.\neventType Cloudtrail records various types of events. In each JSON record, eventType indicates the type of the event. Each event type has a different JSON structure.\nThe following types cover the most cases I\u0026rsquo;m aware of but the list is not intended to be exhaustivee - I will add to it as I learn.\nAwsApiCall API call is the most common event type. It represents an API call on an AWS service.\nThe great thing about this event type is it can be triggered on CloudWatch event. update: recently CloudWatch Event is renamed to AWS EventBridge.\nAwsConsoleSignIn This type of event is generated when a user signed in on AWS management console.\nAwsServiceEvent Services such as AWS SSO generates such type of event when authenticating or federating a user.\nuserIdentity.type On each record, the userIdentity block represents the identity information. Various types of userIdentity exists. The most common ones are: IAMUser, AssumedRole, AWSService, SAMLUser and Unknown.\nIAMUser The below json is extracted from a Cloudtrail event that represents an API call made by an IAM user. User name can be extracted from the userIdentity.userName field.\n{ \u0026#34;eventVersion\u0026#34;: \u0026#34;1.05\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;IAMUser\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;AIDAUWQOET4WMTL6OV3SZ\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:iam::323225952045:user/tool-poc\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;323225952045\u0026#34;, \u0026#34;accessKeyId\u0026#34;: \u0026#34;AKIAUWQOET4WFCRTJDF5\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;tool-poc\u0026#34; }, \u0026#34;eventID\u0026#34;: \u0026#34;1e85a381-9e58-4612-a8d5-abc30ff95f65\u0026#34;, \u0026#34;eventType\u0026#34;: \u0026#34;AwsApiCall\u0026#34;, ... } In another example, the eventType is different but userIdentity block structure looks similar.\n{ \u0026#34;eventVersion\u0026#34;: \u0026#34;1.05\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;IAMUser\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;AIDAVBHXPSQ567GPQHO75\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:iam::346263884858:user/admin\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;346263884858\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;admin\u0026#34; }, \u0026#34;eventTime\u0026#34;: \u0026#34;2020-09-12T18:05:04Z\u0026#34;, \u0026#34;eventSource\u0026#34;: \u0026#34;signin.amazonaws.com\u0026#34;, \u0026#34;eventName\u0026#34;: \u0026#34;ConsoleLogin\u0026#34;, \u0026#34;awsRegion\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;eventID\u0026#34;: \u0026#34;0b8f0958-8507-4526-b8f5-d56741ccae77\u0026#34;, \u0026#34;eventType\u0026#34;: \u0026#34;AwsConsoleSignIn\u0026#34;, ... } AssumedRole AssumedRole is when an identity assumes an AWS role. The identity could be IAM user in the same account, user from another AWS account, AWS service or a SAML provider.\nBelow are a few examples.\nUser name: alice@example.com\n{ \u0026#34;eventVersion\u0026#34;: \u0026#34;1.05\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;AssumedRole\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;AROAJKUFA6XAMROQBJRNA:alice@example.com\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:sts::323225952045:assumed-role/assume-admin-role-an-account/alice@example.com\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;323225952045\u0026#34;, ... }, \u0026#34;eventID\u0026#34;: \u0026#34;e7f3be2f-a81b-4a87-975f-eaac58faca9e\u0026#34;, \u0026#34;eventType\u0026#34;: \u0026#34;AwsApiCall\u0026#34;, ... } User name: AutoScaling\n{ \u0026#34;eventVersion\u0026#34;: \u0026#34;1.05\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;AssumedRole\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;AROAJ6TYGYS2TFMOQYEY2:AutoScaling\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:sts::323225952045:assumed-role/AWSServiceRoleForAutoScaling/AutoScaling\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;323225952045\u0026#34;, ... }, \u0026#34;eventID\u0026#34;: \u0026#34;b67837c3-f90c-49c9-8750-02adef205f64\u0026#34;, \u0026#34;eventType\u0026#34;: \u0026#34;AwsApiCall\u0026#34;, ... } User name: bob@example.com\n{ \u0026#34;eventVersion\u0026#34;: \u0026#34;1.05\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;AssumedRole\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;AROAI4O72XO7XFD2BHDUA:bob@example.com\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:sts::323225952045:assumed-role/Sandbox-SSO-PowerUser/bob@example.com\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;323225952045\u0026#34;, ... }, \u0026#34;eventID\u0026#34;: \u0026#34;5921eee9-7a54-4672-84d5-9a64a81822e4\u0026#34;, \u0026#34;eventType\u0026#34;: \u0026#34;AwsApiCall\u0026#34;, ... } User name: test@example.com\n{ \u0026#34;eventVersion\u0026#34;: \u0026#34;1.05\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;AssumedRole\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;AROAVBHXPSQ577YYUL4QC:test@example.com\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:sts::346263884858:assumed-role/AWSReservedSSO_AWSAdministratorAccess_33ca3b9a1184d671/test@example.com\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;346263884858\u0026#34;, ... }, \u0026#34;eventID\u0026#34;: \u0026#34;a1b2f460-0288-4937-b850-12b521a10230\u0026#34;, \u0026#34;eventType\u0026#34;: \u0026#34;AwsApiCall\u0026#34;, ... } User name: AssumeRoleSession\n{ \u0026#34;eventVersion\u0026#34;: \u0026#34;1.07\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;AssumedRole\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;AROAIUHYOXFSUYZJIJQUM:AssumeRoleSession\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:sts::323225952045:assumed-role/CloudHealth/AssumeRoleSession\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;323225952045\u0026#34;, \u0026#34;accessKeyId\u0026#34;: \u0026#34;ASIAUWQOET4WKMRLT5G6\u0026#34;, \u0026#34;sessionContext\u0026#34;: { \u0026#34;sessionIssuer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Role\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;AROAIUHYOXFSUYZJIJQUM\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:iam::323225952045:role/CloudHealth\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;323225952045\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;CloudHealth\u0026#34; }, \u0026#34;attributes\u0026#34;: { \u0026#34;creationDate\u0026#34;: \u0026#34;2020-09-15T13:53:25Z\u0026#34;, \u0026#34;mfaAuthenticated\u0026#34;: \u0026#34;false\u0026#34; } } }, \u0026#34;eventTime\u0026#34;: \u0026#34;2020-09-15T14:08:27Z\u0026#34;, \u0026#34;eventSource\u0026#34;: \u0026#34;dynamodb.amazonaws.com\u0026#34;, \u0026#34;eventName\u0026#34;: \u0026#34;ListTables\u0026#34;, \u0026#34;awsRegion\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;sourceIPAddress\u0026#34;: \u0026#34;34.230.249.2\u0026#34;, \u0026#34;eventID\u0026#34;: \u0026#34;de0f486d-c1ff-4032-9e86-17ba166f687e\u0026#34;, \u0026#34;eventType\u0026#34;: \u0026#34;AwsApiCall\u0026#34;, ... } SAMLUser This type of userIdentity are most commonly seen in AssumeRoleWithSAML event.\nUser name: bob@example.com\n{ \u0026#34;eventVersion\u0026#34;: \u0026#34;1.05\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;SAMLUser\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;6DLJuKNu+27u3kwvB9BKCv71kco=:bob@example.com\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;bob@example.com\u0026#34;, \u0026#34;identityProvider\u0026#34;: \u0026#34;6DLJuKNu+27u3kwvB9BKCv71kco=\u0026#34; }, \u0026#34;eventTime\u0026#34;: \u0026#34;2020-09-08T13:22:03Z\u0026#34;, \u0026#34;eventSource\u0026#34;: \u0026#34;sts.amazonaws.com\u0026#34;, \u0026#34;eventName\u0026#34;: \u0026#34;AssumeRoleWithSAML\u0026#34;, \u0026#34;awsRegion\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;sourceIPAddress\u0026#34;: \u0026#34;72.21.217.22\u0026#34;, \u0026#34;eventID\u0026#34;: \u0026#34;892c55be-2ab1-4e0e-a80f-5e04f05b625d\u0026#34;, \u0026#34;eventType\u0026#34;: \u0026#34;AwsApiCall\u0026#34;, ... } AWSService For this type of userIdentity, it simply does not have a real user. Instead, it\u0026rsquo;s AWS service that is performing an action.\nUser name (blank)\n{ \u0026#34;eventVersion\u0026#34;: \u0026#34;1.05\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;AWSService\u0026#34;, \u0026#34;invokedBy\u0026#34;: \u0026#34;elasticbeanstalk.amazonaws.com\u0026#34; }, \u0026#34;eventTime\u0026#34;: \u0026#34;2020-09-15T13:43:16Z\u0026#34;, \u0026#34;eventSource\u0026#34;: \u0026#34;sts.amazonaws.com\u0026#34;, \u0026#34;eventName\u0026#34;: \u0026#34;AssumeRole\u0026#34;, \u0026#34;awsRegion\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;sourceIPAddress\u0026#34;: \u0026#34;elasticbeanstalk.amazonaws.com\u0026#34;, \u0026#34;eventID\u0026#34;: \u0026#34;93fd006e-a58f-4304-a9a1-04136ca8a1c3\u0026#34;, \u0026#34;eventType\u0026#34;: \u0026#34;AwsApiCall\u0026#34;, ... } Unknown This is commonly seen in AwsServiceEvent event. I\u0026rsquo;ve seen AWS SSO produces this type of event but I\u0026rsquo;m not aware of what else AWS services produce it.\nUser name: test@example.com\n{ \u0026#34;eventVersion\u0026#34;: \u0026#34;1.05\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Unknown\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;90677f325d-ffd9565d-ac85-4753-8dc6-502c67f1c727\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;346263884858\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;test@example.com\u0026#34; }, \u0026#34;eventTime\u0026#34;: \u0026#34;2020-09-15T13:35:04Z\u0026#34;, \u0026#34;eventSource\u0026#34;: \u0026#34;sso.amazonaws.com\u0026#34;, \u0026#34;eventName\u0026#34;: \u0026#34;Authenticate\u0026#34;, \u0026#34;awsRegion\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;sourceIPAddress\u0026#34;: \u0026#34;75.15.150.17\u0026#34;, ... \u0026#34;eventID\u0026#34;: \u0026#34;63e0001b-e2fa-49b0-bf29-b7c92d977266\u0026#34;, \u0026#34;eventType\u0026#34;: \u0026#34;AwsServiceEvent\u0026#34;, \u0026#34;recipientAccountId\u0026#34;: \u0026#34;346263884858\u0026#34; } ","permalink":"https://davidxiao.me/posts/extract-user-identity-from-aws-cloudtrail/","summary":"Whether you are troubleshooting or investigating something on AWS, being able to look up user identity across the Cloudtrail event logs can be very helpful.","title":"Extract User Identity from AWS Cloudtrail"},{"content":"OTP is very common in today\u0026rsquo;s MFA implementation.\nHOTP HOTP stands for HMAC-based One-time Password algorithm. It computes the value with the following inputs:\nA cryptographic hash method, H (default is SHA-1)\nA secret key, K, which is an arbitrary byte string, and must remain private\nA counter, C, which counts the number of iterations\nA HOTP value length, d (6–10, default is 6, and 6–8 is recommended)\nTOTP TOTP stands for Time-based One-time Password algorithm (TOTP). It is an extension of HOTP that generates a one-time password (OTP) by instead taking uniqueness from the current time.\nMore often than not, time is downsampled into larger durations (e.g., 30 seconds) to allow for validity between the parties.\nTo establish TOTP authentication, the authenticatee and authenticator must pre-establish both the HOTP parameters and the following TOTP parameters:\nT0, the Unix time from which to start counting time steps (default is 0)\nTX, an interval which will be used to calculate the value of the counter CT (default is 30 seconds)\n","permalink":"https://davidxiao.me/posts/one-time-password-and-hotp-totp/","summary":"All you need to know about OTP from a security perspective.","title":"One Time Password, HOTP and TOTP"},{"content":"TLDR: This post is my collection of articles related to threat modeling and Microsoft STRIDE threat model.\nWhat Is Threat Modeling At a high level, threat modeling is a process of putting the \u0026ldquo;bad guy\u0026rdquo; hat on and conducting an security assessment over a system (such as a website or a mobile app) to identify and prioritize threats and mitigations.\nA more complete definition can be found on Wikipedia.\nThere are many ways to do threat modeling. Depending on the types of system and workloads that are in scope, the applicable threats can vary a lot.\nFor example, threat modeling over a set of highly scalable workloads deployed on MS Azure might start from a threat library that includes portential threats relevant to Azure services in use, while assessing over a simple web application hosted on on-prem data center might start from a different set of threats relevant to host based security and OWASP Top 10.\nWhat Is STRIDE STRIDE is both a threat model framework and a methodology that developed and adopted in Microsoft over the years.\nSTRIDE stands for:\nSpoofing: Impersonating something or someone else.\nTampering: Modifying data or code.\nRepudiation: Claiming to have not performed an action.\nInformation Disclosure: Exposing information to someone not authorized to see it.\nDenial of Service: Deny or degrade service to users.\nElevation of Privilege: Gain capabilities without proper authorization.\nMicrosoft suggests the following approach when conducting a threat modeling:\nThreat Modeling Process Most Current Articles Threat Modeling. A high level overview.\nAzure Threat Modeling Tool, the framework and the tool. 02/16/2017\nOlder References STRIDE chart 09/11/2007\nThreat Modeling, once again 08/30/2007\nThreat Modeling again. Drawing the diagram 08/31/2007\nThreat Modeling Again, STRIDE 09/04/2007\nThreat Modeling Again, STRIDE Mitigations 09/05/2007\nThreat Modeling Again, What does STRIDE have to do with threat modeling 09/07/2007\nThreat Modeling Again, STRIDE per Element 09/10/2007\nThreat Modeling Again, Threat Modeling PlaySound 09/11/2007\n","permalink":"https://davidxiao.me/posts/threat-modeling-and-stride-model/","summary":"This post is my collection of articles related to threat modeling and Microsoft STRIDE threat model.","title":"Threat Modeling and STRIDE Model"},{"content":"Both TLS and SSH are security protocols aimed to solve a specific set of problems.\nTLS is the transport layer of HTTPS protocol while SSH is designed to replace plaintext Telnet protocol.\nArchitecture wise, TLS is relatively simple: It has a handshake protocol that does the authentication and agrees on a session key that will be used to encrypt the rest of the communication.\nSSH is more complicated than TLS. It has the following main components:\ntransport layer;\nuser authentication layer;\nconnection layer;\nSix SSH related RFC are published in relate to SSH: 4251, 4252, 4253, 4254, 425 and 4256.\nSSH Transport Layer Transport layer handles key exchange, server authentication and sets up encryption, compression and integrity verification. It exposes to the upper layer an programmatic interface for sending and receiving plaintext data. The transport layer also arranges for key re-exchange, usually after 1 GB of data has been transferred or after 1 hour has passed, whichever occurs first.\nUser Authentication Layer It handles client authentication and provides a number of authentication methods. Widely used user-authentication methods include password, publickey, keyboard-interactive, GSSAPI authentication which allows SSH to authenticate using external mechanisms such as Kerberos 5 or NTLM, providing single sign-on capability to SSH sessions.\nConnection Layer It defines the concept of channels in SSH. A single SSH connection can host multiple channels simultaneously, each transferring data in both directions. Standard channel types include: shell for terminal shells; SFTP and exec requests (including SCP transfers); direct-tcpip for client-to-server forwarded connections; forwarded-tcpip for server-to-client forwarded connections etc.\n","permalink":"https://davidxiao.me/posts/ssh-and-tls/","summary":"Review the differences and similarities between the two protocols from an architecture and security perspective.","title":"SSH and TLS: Differences and Similarities"},{"content":"At a high level, the following occurs during a TLS handshake:\ngraph TD; A[Client establishes a TCP connection to the server] --\u0026gt;B[Client sends Hello and list of cipher suites including TLS version] --\u0026gt;C[Server sends Hello, selected suite and certificate] --\u0026gt; D[Client validates certificate] D --\u0026gt; E[Client and server starts key exchange process. \u0026lt;br/\u0026gt;RSA and Diffie-Hellman are two common KEP algogirhtms] E --\u0026gt; F{Key Exchange Protocol} F --\u0026gt;|RSA| G[Both client and server independently \u0026lt;br/\u0026gt;agree on the same secret value with client random,\u0026lt;br/\u0026gt; server random and premaster secret] F --\u0026gt;|DH| H[Both client and server independently \u0026lt;br/\u0026gt;agree on the same secret value over exchanging\u0026lt;br/\u0026gt; a few DH parameters] G --\u0026gt;I[Regardless of which KEP was used, \u0026lt;br/\u0026gt;the rest of the session uses the agreed symmetric key to encrypt \u0026lt;br/\u0026gt;the communication both ways going forward] H --\u0026gt;I Read more about DH on my post and wikipedia\nKey Takeaways About the KEPs 👉 DH achieves forward secrecy while RSA does not.\n👉 DH handshake takes longer than RSA.\nWhat Else You Need To Know about TLS TLS 1.0 and TLS 1.1 are no longer secure and should be avoided. A best practice is to use TLS version is 1.2 or later at the time of writing.\nHTTPS means \u0026ldquo;HTTP over TLS\u0026rdquo;.\nBoth SSH and TLS are purpose-built for secure communication over the Internet, but they are very different in many ways. Check out my another post where I explain the differences between the two.\nGlossary Cipher Suite A cipher suite is a set of algorithms. It usually contain include: a key exchange algorithm, a bulk encryption algorithm, and a message authentication code (MAC) algorithm.\nFor example, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 means:\nECDHE_RSA indicates the key exchange algorithm being used.\nAES_128_GCM indicates the block cipher being used to encrypt the message stream, together with the block cipher mode of operation.\nSHA256 indicates the message authentication algorithm which is used to authenticate a message.\nECDHE_RSA key exchange algorithm In a nutshell, it is ECDHE signed by RSA. Signing defeats man-in-the-middle attack. See detail here\n","permalink":"https://davidxiao.me/posts/tls-handshake-in-a-nutshell/","summary":"A quick overview of TLS handshake","title":"TLS Handshake in a Nutshell"},{"content":"Primer PPL stands for Private Pilot License. In Canada and US, PPL holders can fly an airplane under VFR rules.\nThe type of airplane PPL holder can legally fly depends on a few things. Usually one can legally fly small/light airplanes that he is most familiar with, such as Cessna 172 trainer.\nTo fly complex, high performance or tailwheel airplanes, one need to obtain respective endorsement from a certified instructor first.\nFor heavier airplane or those equipped with turbojet powerplant, type rating is required before he can legally fly one.\nAbove all, PPL holder needs to stay current and competent before he decides to fly as pilot. The old aviation proverb says, \u0026ldquo;Takeoffs are optional, landings are mandatory\u0026rdquo;.\nRegulatory Body and Laws / Regulations In Canada, Transport Canada governs general aviation.\nIn US, FAA governs general aviation.\nCARs: Canadian Aviation Regulations. Link on Transport Canada\nCruising Altitudes When flying VFR above 3,000 AGL, remember \u0026ldquo;East is Odd, West is Even Odder\u0026rdquo; on Magnetic Track (not Magnetic Heading)\n602.34 Cruising Altitudes and Cruising Flight Levels\nCessna 172M procedures Precautionary Landing COWLS check\nCivilization (nearby population)\nObstacle (tower, treeline, powerline)\nWind (headwind on final)\nLength of the surface (2000 ft would be minimal for C172 considering obstacle clearance, touchdown area and distance required for T/O)\nSurface condition (too soft, too much rough, uneven terrain)\nHigh Pass and Low Pass\nHigh pass is completed at circuit altitude: 60kt with flaps 20\nIf satisfied with the surface, a low pass is completed at 500 AGL with same speed and configuration\nIn both high pass and low pass, the key is to trim for straight and level flight at the correct speed and altitude before abeam threshold.\nUse the same pitch for speed and power for decent technique. For C172, 1900rpm is rule of thumb that can keep 60kt without decending.\nOn the downwind for landing, complete 3P: Prelanding checklist, Pan call, and Pax - passenger briefing)\nEnroute Climb Lookout to clear the climb\nAPT - Attitude, Power (control the adverse yaw), Trim\nLevel off from climbing - The same APT procedure applies.\nReference\nClimb\nPitch up and hold the nose on the horizon Peek at A/S as it decreases to 75 Full Power, Mix Rich. Carb Off. Right rudder when needed Trim Level Off from enroute climb\nStarts at 50-100ft below target altitude. Gradually push the nose down to cruise attitude.\nReduce Power to Cruise setting: 2300rpm. Left Rudder when needed\nEnroute Decent Lookout to clear the decent\nPAT - Power (control the adverse yaw), Attitude, Trim\nLevel off from climbing - The same APT applies.\nReference\nNormal Takeoff Take-off Distance is the lump sum of the following three:\nTakeoff roll (ground roll)\nLift-off\nInitial Climb until clear 50ft obstacle\nConfirm Wind Input before takeoff. Use Cross-wind Take-off Checklist if needed.\nElevator neutral. Aileron neutral. No flaps. Full Power. Mix Rich. Carb Cold. Call \u0026#34;Power Set\u0026#34;. Stay on the centerline, use Right Rudder as needed. Check and call \u0026#34;Engine gauges green\u0026#34;. Check and call \u0026#34;Airspeed alive\u0026#34;. Wings level. Rotate at 55. Right Rudder! Call \u0026#34;Rotate\u0026#34;. Nose on the Horizon. Call \u0026#34;____, \u0026lt;callsign\u0026gt; airborne Rwy __\u0026#34; Climb out at 70. Trim. Normal Landing Important: Set your mindset as \u0026ldquo;keep it flying without landing\u0026rdquo;\nOn Down Wind:\nSet Power to 2300rpm. Fly parallel to the landing Runway. Use a Crab angle when needed Pre-landing checklist. (Primer locked, Master ON, Magneto on Both, Circuit Breakers Check, Landing Lights ON, Carb ON, Mix Rich, Engine Green, Fuel on Both, Doors and Windows Locked, Seatbelts fastened, Brakes Check) \u0026#34;Pre-landing Checklist Complete\u0026#34; \u0026#34;Aiming to touch down at one third of the Rwy\u0026#34; At 45 degrees beyond threshold (turn early when having a tailwind), reduce power to 1500rpm, holding the altitude while making a turn to base. Call \u0026#34;___, \u0026lt;callsign\u0026gt;, turning base for Rwy __\u0026#34; (optional) On Base Leg:\nHolding the altitude while bleeding off A/S until it reaches 75 knots Apply flaps 20 (be careful not to ballon up) and trim for 75 Anticipate a 500fpm decent. Look out for Wind *** When having tail winds, turn early and anticipate a bit high Turning Final and call \u0026#34;___, \u0026lt;callsign\u0026gt;, turning final on Rwy __. [Tough-n-Go / Full Stop]\u0026#34; On Final:\nApply full flaps. A/S 65. Trim. Line up early on the extended runway centerline. Fly with a crab angle initially to correct for drift. Pitch for Airspeed, Power for altitude Transition to Sideslip. When getting the vision of \u0026#34;exploding\u0026#34; runway: Power to Idle, Hold control column and gradually level off. Judge the sink rate using peripheral vision. When sinking, do baby steps of \u0026#34;pull-pause-pull\u0026#34; to keep the nose slight below horizon. When not sinking, hold the control column but no pulling. When touches down completely: Wind input - Aileron fully deflected into the wind Opposite Rudder to stay on the centerline Short Field Landing On Down wind:\n\u0026#34;Aiming to touch down at second dash\u0026#34; On Final:\nApproach speed: 60 \u0026#34;There is an 50ft obstacle before threshold\u0026#34; When obstacle is cleared, power to idle and maintain 60 After touching down, HOLD control full back, RETRACT flaps and APPLY maximum brake pressure (from C172 POH) Soft Field Landing On Down wind:\n\u0026#34;Aiming to touch down at one third of the Rwy\u0026#34; On Final:\nPerform a 60kt approach on final Power to Idle and enter cruising attitude as normal landing Immediately after entering cruising attitude but before it starts sinking: Apply small amount of power (100 to 200rpm is enough) Touching down at a more nose up attitude than normal landing (textbook says roughtly the attitude of a power-off stall) During the flaring control column likely has more back travel than normal landing. It can be close to the belly when touching down After touchdown, HOLD control full back and DO NOT apply heavy brake pressure Power-off Stall Entry and Recovery On straight and level flight\nHASEL CHECK (Carb ON, Mix Rich) Power to Idle smoothly while controlling yaw Hold back control pressure to keep airplane straight and level Watch A/S decreases into slow flight range. Call out immiment stall by \u0026#34;STALL\u0026#34; when hearing horn and losing altitude Call out \u0026#34;RECOVER\u0026#34; and immediately nose down and apply full power and control yaw Power-on Stall Entry and Recovery Entry: Power to 1700rpm everything else is similiar to that of a power off stall. Expect more challenging yaw control due to slipstream and asymmetric thrust use RUDDER to control yaw Stall will happen in a more hose-up position it will be judged if heading can be kept during recovery ","permalink":"https://davidxiao.me/posts/pilot-in-the-making/","summary":"This is some of the notes I took when taking PPL flight lessons.","title":"A New Pilot in the Making"},{"content":"A Conflict of Visions: Ideological Origins of Political Struggles. 2007. Thomas Sowell. Amazon link\n[Oct 30 2020 update]\nDiscrimination and Disparities. 2019. Thomas Sowell.\n","permalink":"https://davidxiao.me/posts/what-ive-been-reading/","summary":"My reading list of nonfiction books. I enjoy reading them but you can draw your own conclusions.","title":"What I have Been Reading"},{"content":"TLDR\nAs a recovering C++ developer learning React, I put together some notes along the journey.\nSome of the notes and example code were extracted from the materials listed in the reference. I\u0026rsquo;ve tried to include links but feel free to let me know if I missed something.\nCredit goes to the original authors.\nReference Step-by-step guide. Great learning material. What is React.js React is a declarative, efficient, and flexible JavaScript library for building user interfaces. It has a few building blocks such as elements, components etc.\nReactjs.org\nElements An element describes what you want to see on the screen:\nconst element = \u0026lt;h1\u0026gt;Hello world\u0026lt;/h1\u0026gt;; const div1 = \u0026lt;div /\u0026gt;; Components Elements such as \u0026lt;div /\u0026gt; are defined in HTML5. React extends it by introducing user-defined elements such as\n\u0026lt;Welcome name=\u0026#34;Sara\u0026#34; /\u0026gt; Welcome is a user-defined component. In React, always start component names with a capital letter to follow the naming convention.\nWhy Components Components let you split the UI into independent, reusable pieces, and think about each piece in isolation.\nReact component can be declared as a JS function or a JS class. Either way, it accepts input (called “props”) and return React elements describing what should appear on the screen.\nWhen React sees an element representing a user-defined component, it wraps up JSX attributes and children and passes it as a single object (called props) to the component implicitly without user code.\npropsstands for properties.\nReact function An example:\nfunction Welcome(props) { return \u0026lt;h1\u0026gt;Hello, {props.name}\u0026lt;/h1\u0026gt;; } const element = \u0026lt;Welcome name=\u0026#34;Sara\u0026#34; /\u0026gt;; ReactDOM.render( element, document.getElementById(\u0026#39;root\u0026#39;) ); JS code starts at the top of the file. It gets executed from top to bottom. There is no entry point such as what \u0026ldquo;main()\u0026rdquo; has to do in C++.\nWhat happens in the example above is:\nReact calls ReactDOM.render() with the \u0026lt;Welcome name=\u0026quot;Sara\u0026quot; /\u0026gt;\nReact sees Welcome is a user-defined component and calls its corresponding function with {name: 'Sara'} as the props.\nWelcome component returns \u0026lt;h1\u0026gt;Hello, Sara\u0026lt;/h1\u0026gt; as the result.\nReact takes care of updating the DOM with \u0026lt;h1\u0026gt;Hello, Sara\u0026lt;/h1\u0026gt;.\nReact class The function component example above can be equivalently rewritten as a class component:\nclass Welcome extends React.Component { render() { return \u0026lt;h1\u0026gt;Hello, {this.props.name}\u0026lt;/h1\u0026gt;; } } React class component always \u0026ldquo;inherited\u0026rdquo; from React.Component.\nrender() is the only method that any class \u0026ldquo;inherited\u0026rdquo; from React.Component must declare.\nThink of render() as a C++ pure virtual function in React.Component.\nA derived class would have to implement it before the class can be instantiated.\nCalling a component in another component Let\u0026rsquo;s take function component as an example.\ngraph TD; Comment--\u0026gt;UserInfo--\u0026gt;Avatar; Comment passed the value of its own props.author to UserInfo.\nfunction Comment(props) { return ( \u0026lt;div className=\u0026#34;Comment\u0026#34;\u0026gt; \u0026lt;UserInfo user={props.author} /\u0026gt; \u0026lt;div className=\u0026#34;Comment-text\u0026#34;\u0026gt; {props.text} \u0026lt;/div\u0026gt; \u0026lt;div className=\u0026#34;Comment-date\u0026#34;\u0026gt; {formatDate(props.date)} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; ); } UserInfo\u0026rsquo;s props contains a key value pair \u0026ldquo;user=\u0026hellip;\u0026rdquo; received from Comment. The value then gets passed down to Avatar.\nfunction UserInfo(props) { return ( \u0026lt;div className=\u0026#34;UserInfo\u0026#34;\u0026gt; \u0026lt;Avatar user={props.user} /\u0026gt; \u0026lt;div className=\u0026#34;UserInfo-name\u0026#34;\u0026gt; {props.user.name} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; ); } Props Props gets passed to the constructor of class and stored as a class variable. Any method thereafer can reference it using this.props\nIn a class component, think of this.props as a const reference member variable in C++.\nIt is initiated within constructor(props) where super(props) is called, the superclass always being React.Component.\nWe recommend naming props from the component’s own point of view rather than the context in which it is being used. The rationale being: It doesn’t need to know context such as where it is being rendered.\nThat\u0026rsquo;s a sort of isolation that helps in producing clear code.\nProps are Immutable Whether you declare a component as a function or a class, it must never modify its own props.\nThe following code is valid in javascript syntax but should not be used as React component as it changes its own input:\nfunction withdraw(account, amount) { account.total -= amount; } Think of JS function parameter as pass by reference in C++.\nIn React, treat props as if it is const reference in C++.\nClass, State and Lifecycle One difference between class and function is that class has a special object called this.states. states acts as a \u0026ldquo;container\u0026rdquo; that preserves variables between calls. In other words, class oject is \u0026ldquo;stateful\u0026rdquo; while function is \u0026ldquo;stateless\u0026rdquo;.\nA class has a few built-in methods including:\nconstructor(). See here\ncomponentDidMount(). It is called when the object is rendered to the DOM for the first time. See here\ncomponentWillUnmount(). It is called when the DOM produced by the object is removed. See here\nA diagram to help understand lifecycle:\nReact lifecycle methods diagram For a visual reference, click here\nstate is a built-in object in class.\nClick to see code example and explainations class Clock extends React.Component { constructor(props) { // for a React.Component subclass, you should call // super(props) before any other statement super(props); // in constructor, you should not call setState(), // instead, assign the initial state to this.state directly this.state = {date: new Date()}; } componentDidMount() { this.timerID = setInterval( () =\u0026gt; this.tick(), 1000 ); } componentWillUnmount() { clearInterval(this.timerID); } tick() { this.setState({ date: new Date() }); } render() { return ( \u0026lt;div\u0026gt; \u0026lt;h1\u0026gt;Hello, world!\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;It is {this.state.date.toLocaleTimeString()}.\u0026lt;/h2\u0026gt;\u0026lt;/h2\u0026gt;\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; ); } } ReactDOM.render( \u0026lt;Clock /\u0026gt;, document.getElementById(\u0026#39;root\u0026#39;) ); When \u0026lt;Clock /\u0026gt; is passed to ReactDOM.render(), React calls the constructor of the Clock component. Since Clock needs to display the current time, it initializes this.state with an object including the current time. We will later update this state.\nReact then calls the Clock component’s render() method. This is how React learns what should be displayed on the screen. React then updates the DOM to match the Clock’s render output.\nWhen the Clock output is inserted in the DOM, React calls the componentDidMount() lifecycle method. Inside it, the Clock component asks the browser to set up a timer to call the component’s tick() method once a second.\nEvery second the browser calls the tick() method. Inside it, the Clock component schedules a UI update by calling setState() with an object containing the current time.\n👉 Important: Thanks to the setState() call, React knows the state has changed, and calls the render() method again to learn what should be on the screen. This time, this.state.date in the render() method will be different, and so the render output will include the updated time. React updates the DOM accordingly.\nIf the Clock component is ever removed from the DOM, React calls the componentWillUnmount() lifecycle method so the timer is stopped.\nProps vs State This is a growing list.\n\u0026ldquo;rendered\u0026rdquo; value In React, both this.props and this.state represent the rendered values, i.e. what’s currently on the screen.\nBoth props and state can be accessed by \u0026ldquo;this\u0026rdquo; Both this.state and this.props are valid within the class scope.\nProps are immutable It is not supposed to be modified in any way. If the component needs to be \u0026ldquo;stateful\u0026rdquo; during the calls, always use state.\nAvoid Copying Props into State constructor(props) { super(props); // Don\u0026#39;t do this! this.state = { color: props.color }; } This is a common mistake. It’s unnecessary (use this.props.color instead) and prone to bugs (updates to the color prop won’t be reflected in the state).\nOnly use it if you want to disregard prop updates. In that case, it makes sense to rename the prop to be called initialColor or defaultColor. You can then force a component to “reset” its internal state by changing its key when necessary.\nRead this post on avoiding derived state to learn about what to do if you think you need some state to depend on the props.\nReactjs.org\nState Updates May Be Asynchronous setState() \u0026ldquo;schedules\u0026rdquo; an update to a component’s state object. When state changes, the component responds by re-rendering.\nConsider using setState() that accepts a function rather than an object when you need to update state variables.\nReact may batch multiple setState() calls into a single update for performance. In React, state must only be updated by setState() (only exception is within constructor()).\nThis is because manual updates won\u0026rsquo;t trigger UI updates since React doesn\u0026rsquo;t know state has changed.\nTo fix it, use a second form of setState() that accepts a \u0026ldquo;updater\u0026rdquo; function object as parameter.\n// Wrong. counter may not be updated immediately since setState() may be delayed in execution by React this.setState({ counter: this.state.counter + this.props.increment, }); // Correct. See explaination above this.setState((state, props) =\u0026gt; ({ counter: state.counter + props.increment })); In React, \u0026ldquo;updater\u0026rdquo; function takes the following signature:\n(state, props) =\u0026gt; stateChange The return value of the updater is shallowly merged with state. For example, in the following code, the return value of the function. will be assigned to state.counter:\nthis.setState((state, props) =\u0026gt; { return {counter: state.counter + props.step}; }); State holds all its variables React merges the object you provide into the current state.\nconstructor(props) { super(props); this.state = { posts: [], comments: [] }; } When you update comments with the following code, it leaves this.state.posts intact, but completely replaces this.state.comments.\nState is not accessible to other components State is not accessible to any component other than the one that owns and sets it.\nThink of state as a \u0026ldquo;protected\u0026rdquo; member variable encapsulated in React.Component class. It can be accessed by its derived classed but not from outside. It is the philosophy that neither parent nor child components can know if a certain component is stateful or stateless, and they shouldn’t care its implementation details, such as whether it is defined as a function or a class.\nIn React apps, whether a component is stateful or stateless is considered an implementation detail of the component that may change over time. You can use stateless components inside stateful components, and vice versa.\nBe cautious about using \u0026ldquo;this\u0026rdquo; in a class method Unlike in C++ where this is accessible in any class method including in the constructor, in JS, this is undefined until it is bound.\nFirst, let\u0026rsquo;s take a look at this example where I purposely made an attempt to access this in constructor and get the following error:\nUncaught ReferenceError: Must call super constructor in derived class before accessing \u0026#39;this\u0026#39; or returning from derived constructor at pen.js:-12 Second, in member methods this is undefined until it is bound.\nIn the following example code, if you forget to bind this.handleClick and pass it to onClick, this will be undefined when the function is actually called.\nThis is not React-specific; it is a part of how functions work in JavaScript.\nGenerally, if you refer to a method without () after it, such as onClick={this.handleClick}, you should bind that method.\nThere are two (recommended) ways to do it:\n// bind a method in constructor constructor(props) { super(props); this.state = {isToggleOn: true}; // This binding is necessary to make `this` work in the callback this.handleClick = this.handleClick.bind(this); } // or the following way handleClick = () =\u0026gt; { console.log(\u0026#39;this is:\u0026#39;, this); } In either way, the method is bound and the render() will work:\nrender() { return ( \u0026lt;button onClick={this.handleClick}\u0026gt; Click me \u0026lt;/button\u0026gt; ); } ","permalink":"https://davidxiao.me/posts/notes-from-my-journey-in-reactjs/","summary":"The Good, \u003cdel\u003ethe Bad and the Ugly.\u003c/del\u003e","title":"Notes from My Journey Learning Javascript and React.js"},{"content":"A drop bar road bike in black painting fell off my bike rack between around 4-5pm when I was driving westbound on St.John between Yonge and Bathurst then Southbound on Bathurst towards Richmond Hill. I turned around in ten minutes but did not see it. If anyone has it please DM me. Much appreciated!\nDevinci HATCHET APEX G/57cm standover 774 wheelbase 1061 700c x 45mm / 650b x 2.1 tire clearance http://www.devinci.com/bikes/bike_1274_scategory_332\nGiant TOUGHROAD SLR GX 0 2018 $2,199 Size L standover 802 seat tube length 51cm wheelbase 1071\nNORCO SEARCH XR S 2020 58cm stand over 775mm wheelbase 1050 seat tube 550mm w/o fender it runs up to 700/42 or 650/51\nNiner RLT 9 AL/carbon 59cm standover 805mm seat tube 55cm wheelbase 1051 tire 700/50c or 650/2.0\n***Specialized AWOL is a large fitting bike. Size M is right. standover 794mm wheelbase 1062\nSurly Bridge Club Size M 27.5 1x SRAM SX EAGLE\nSalsa Journeyman\nBrodie Torque 2020 L-19.5\nGiant Sycamore S, 700x50, FlatGuard Deflect 2 crank 172.5 seat tube 495 =667.5\n33 inch inseam=838mm\nseatpost=838-667=171mm out 400mm length=\n","permalink":"https://davidxiao.me/posts/my-bike-notes/","summary":"A drop bar road bike in black painting fell off my bike rack between around 4-5pm when I was driving westbound on St.John between Yonge and Bathurst then Southbound on Bathurst towards Richmond Hill. I turned around in ten minutes but did not see it. If anyone has it please DM me. Much appreciated!\nDevinci HATCHET APEX G/57cm standover 774 wheelbase 1061 700c x 45mm / 650b x 2.1 tire clearance http://www.","title":"My Bike Notes"},{"content":"Onboarding an existing project to GitHub If the project is not yet initialized by git, do the following first\ngit init ; # then create a .gitignore file if needed git add . ; git status -u ; # will show you all the files to be committed. git commit -m \u0026#34;init commit\u0026#34; ; Then connect it to GitHub\ngit remote add origin git@github.com:davxiao/my-proj.git ; git branch --set-upstream-to=origin/master master ; git pull origin master --allow-unrelated-histories ; git push ; ","permalink":"https://davidxiao.me/posts/my-awesome-github-cheatsheet/","summary":"An awesome list of useful git slash github commands I compiled over time.","title":"My Awesome GitHub CheatSheet"},{"content":"If you own a personal site and like to make both your site and visitors secure, read this: Just because you\u0026rsquo;ve enabled HTTPS does not mean it\u0026rsquo;s sound and secure.\nTLS v1.0 and v1.1 are known to be vulnerable and should not be allowed on your site. It is a security best practice to make TLS v1.2 the minimum version allowed on your site.\nFor more detail, check out this post on Google Security Blog and this post on Google Chrome Browser Updates.\nGet a Test on your Site You can use SSLLabs to conduct a quick test on your site.\nInitial test results Click on any of the server will give you a brief explaination on the findings.\nSee explaination here I\u0026rsquo;m using Cloudflare as CDN for api.davidxiao.me, so I went on to the Cloudflare portal and updated the \u0026ldquo;Minimum TLS Version\u0026rdquo; to \u0026ldquo;TLS 1.2\u0026rdquo;.\nUpdating minimum TLS version to 1.2 Then performed a re-scan. It looks much better this time.\nThe new results Hope this is helpful!\n","permalink":"https://davidxiao.me/posts/secure-your-https-ssl-tls/","summary":"To site owners: Just because you\u0026rsquo;ve enabled HTTPS does not mean it\u0026rsquo;s sound and secure. TLS v1.0 and v1.1 is unsecure and phasing out.","title":"Secure Your HTTPS / SSL / TLS"},{"content":"You will need the following to get dynamic DNS working on Cloudflare:\nCloudflare as your DNS provider. Migrating from your current DNS provider over to Cloudflare is easy and free whether you are using Godaddy, Namecheap or another one.\nGet API token set up on Cloudflare.\nInstall cloudflare-cli.\nCloudflare API token Cloudflare Token is preferred way over API key as token enables added security by allowing to specify access level with permissions and resources.\nToken can be disabled when not in use.\nWhen those are met, a one-liner like the following will update A record api.davidxiao.me with your public internet IP.\nNOTE: If you are only using DDNS on Cloudflare and not using its CDN, remove --activate from the command below.\n$ env CF_API_KEY=\u0026#39;your-own-cloudflare-token-NOT-api-key\u0026#39; \\ CF_API_DOMAIN=\u0026#39;your-own-TLD-such-as-davidxiao.me\u0026#39; \\ cfcli --activate --type A edit your-subdomain-such-as-api.davidxiao.me \\ `curl -s \u0026#39;https://ip.seeip.org/\u0026#39;` ; Finally, put the code it into crontab will automate the process.\n","permalink":"https://davidxiao.me/posts/dynamic-dns-on-cloudflare/","summary":"You will need the following to get dynamic DNS working on Cloudflare:\nCloudflare as your DNS provider. Migrating from your current DNS provider over to Cloudflare is easy and free whether you are using Godaddy, Namecheap or another one.\nGet API token set up on Cloudflare.\nInstall cloudflare-cli.\nCloudflare API token Cloudflare Token is preferred way over API key as token enables added security by allowing to specify access level with permissions and resources.","title":"Dynamic DNS on Cloudflare in 5 Minutes"},{"content":"Suppose you plan on hosting multiple API endpoints on one domain, make it api.your-domain.com, one way to do it is to put each endpoint under a distinctive path, each one would look like:\nhttps://api.your-domain.com/myapp1/ Tunneling through a Nginx reverse proxy Configure a reverse proxy such as Nginx and set up rewrite rules can get it done quickly.\nSee below my example:\nDepending on your Linux distro and Nginx, most would need to put the conf file under /etc/nginx/sites-available/ directory and create a symbol link in /etc/nginx/sites-enabled/.\nRestart nginx service by sudo systemctl restart nginx ; and it\u0026rsquo;s working!\n","permalink":"https://davidxiao.me/posts/hosting-multiple-containers-with-nginx-rewrite-rules/","summary":"A reverse proxy such as Nginx will come in handy if you need to host multiple apps on a single domain. Here\u0026rsquo;s a 5-minute how-to.","title":"Hosting Multiple Apps With Nginx Rewrite Rules"},{"content":"TLDR;\n👉 This post talks about hosting remark42 commenting system as Docker container; Leveraging Cloudflare to protect the remark42 endpoint; Integrating remark42 to a static site which is built on Hugo and academic theme.\nBefore getting started, take a look at the posts on my blog at davidxiao.me and see the way commenting works.\nWhat Will be Covered in This Post Deploy remark42 container on Docker\nProtect your remark42 endpoint with Cloudflare CDN\nIntegrate remark42 to your Hugo site\nStep 1. Deploy Remark42 on your Host Remark42 is an open source commenting system that can be deployed as container. It\u0026rsquo;s self-contained with little external dependencies. You can find deployement guide on its Remark42\u0026rsquo;s git repo.\nFeel free to do container your way but if you are interested in what tool I use for docker management, it\u0026rsquo;s Portainer.\nI have the following parameters on my remark42 container:\nREMARK_URL make sure it has the full path if you are using reverse proxy and rewrites, e.g. https://api.davidxiao.me/remark42 SITE\tsite id. For example: davidxiao SECRET\trequired. can be a long and hard-to-guess string DEBUG\ttrue AUTH_GOOGLE_CID your own value AUTH_GOOGLE_CSEC your own value AUTH_FACEBOOK_CID your own value AUTH_FACEBOOK_CSEC your own value AUTH_TWITTER_CID your own value AUTH_TWITTER_CSEC your own value AUTH_GITHUB_CID your own value AUTH_GITHUB_CSEC your own value ADMIN_SHARED_EMAIL mail address that will receive notifications such as new comments NOTIFY_EMAIL_ADMIN\ttrue NOTIFY_TYPE\temail NOTIFY_EMAIL_FROM\tmail address that is in the same domain see Mailgun settings. For example, mine is remark42@davidxiao.me AUTH_EMAIL_FROM\tyour own value ADMIN_SHARED_ID\tOAuth authenticated user id that has admin access. see https://github.com/umputun/remark42#admin-users SMTP_HOST\tsmtp.mailgun.org SMTP_PORT\t465 SMTP_TLS\ttrue SMTP_USERNAME\tSMTP credential from Mailgun SMTP_PASSWORD\tyour own credential For more detail on how to configure email configuration on Remark42, check this out.\nApp Registration on the OAuth Providers Before registering you Remark42 app on google, facebook, twitter and github (the OAuth providers Remark42 supports), you would need to determine the domain name of your Remark42 api endpoint.\nMy app registration page on facebook as an example DDNS(Dynamic DNS) comes in handy whether you are hosting Remark42 container on cloud such as AWS EC2 or on your homelab, since it allows you to update DNS A records whenever your endpoint IP changes.\nSelecting a DDNS provider is important for a few reasons.\nProtection of your host\u0026rsquo;s public IP is important for self-hosting web apps. Using DDNS alone means your domain name gets resolved to your public IP. DDNS + CDN is a better approach.\nEach OAuth provider has its own rules over whether an given OAuth redirect URI is allowed. For example, facebook does not allow any duckdns.org as part of redirect URI at time of writing.\nService availability concern. Your app will become inaccessible when the DDNS it relies on stops working.\nThe security posture of the DDNS provider. If the service provider gets compromised, you DDNS domain name can be \u0026ldquo;hijacked\u0026rdquo;.\nMy approach:\nSet up your public endpoint leveraging a DNS provider such as Cloudflare that has large operating scale, supports DDNS management over API and offers CDN protection over your private endpoint. See here for more.\nOn your private endpoint, Use a reverse proxy such as Nginx to rewrite URLs so that multiple apps can be \u0026ldquo;tunneled through\u0026rdquo; a single domain name when needed. See here for more.\nUse Nginx control policy to restrict access to only Cloudflare IPs and local trusted networks.\nStep 2. Protect(tunnel) your Endpoint You need to use Cloudflare as DNS provider before enabling Cloudflare CDN.\nTo enable CDN, first go to Cloudflare portal and enable CDN for your remark42 subdomain. In my case it\u0026rsquo;s api.davidxiao.me.\nEnabling Proxy by clicking on the Proxy status icon Second, modify caching level to \u0026ldquo;No query string\u0026rdquo;. No Query String means it only delivers files from cache when there is no query string. It\u0026rsquo;s the caching behavior we expect for an API endpoint, isn\u0026rsquo;t it?\nEnabling Proxied by clicking on Proxy status icon Step 2.1 Enable End-to-end HTTPS with Cloudflare There are several SSL options provided by Cloudflare. Check this out to understand the difference.\nBased on my own needs, I\u0026rsquo;ve set up a \u0026ldquo;Full\u0026rdquo; mode in Cloudflare, which ensures a secure connection between both the web browser and Cloudflare and between Cloudflare and my endpoint. This option uses a self-signed certificate at the my endpoint web server.\nEnabling SSL in Full mode Generate and download Cloudflare\u0026#39;s self-signed certificates Save the origin certificat as origin-cert.pem and the private key as priv.key, place both files on your host and make sure they both have ownership of root and have 0600 permissions.\nThen you just need to add the file locations in your nginx configuration file. See below my configruation files for example:\nStep 3. Integrate Remark42 to Your Hugo Site Override the comments.html template by:\n$ cp your-project-root/themes/academic/layouts/partials/comments.html your-project-root/layouts/partials/comments.html and modify the new one as you see fit.\nThe following is my modified version of comments.html.\nConclusion Congrats! You\u0026rsquo;ve got the remark42 commenting system integrated to your Hugo site.\nComments and Feedback are welcome!\n","permalink":"https://davidxiao.me/posts/adding-your-own-commenting-system-to-a-static-site/","summary":"If you are looking for guidance on integrating an open-source commenting system such as Remark42 to your site, here is how I did it.","title":"Integrating a Self-Hosting Commenting System to Your Site"},{"content":"Mailgun is a SaaS that provides email services (both sending and receiving) through both conventional SMTP and RESTful APIs. Find more detail here.\nIn this post we will cover the SMTP option as it can be easily integrated with Postfix.\nWhat\u0026rsquo;s Required You would need the following:\nA custom domain name, something like yourcompany.com. You also need to have admin access to your DNS provider.\nA Mailgun account that allows you to send out mails using your custom domain name. Payment information is required when signing up but no charges will take place at the moment.\nAdmin privilege on a Linux box to set up Postfix server. Outbound internet access is required.\nOnboarding Mailgun Make sure you fill out payment information when signing up a new account on Mailgun. There\u0026rsquo;s no charge for small volume mail delivery but it\u0026rsquo;s a Mailgun requirement if you plan on delivering emails with your custom mail address like yourname@yourdomain.com.\nSign up on Mailgun Mailgun provides multiple plans, the Flex Trial is essentially a pay-as-you-go option that works very well if you don\u0026rsquo;t need to deliver a large volume of emails everyday.\nFlex Trial is pay-as-you-go DNS Settings For Sending on Mailgun to work properly, you\u0026rsquo;ve got to update a few DNS settings. First, you\u0026rsquo;d have to add a subdomain to your domain for Mailgun MX. For example, I\u0026rsquo;ve added mg.davidxiao.me.\nThen you\u0026rsquo;d have to add a few TXT records on DNS.\nAdding TXT records for sending emails What About Receiving Receiving and forwarding incoming mails is a premium feature on Mailgun that requires spending commitments.\nI will write another post talking about using a free-of-charge mail forwading service. Stay tuned.\nSMTP Credential On Mailgun portal, under Settings/Domain Settings and click on SMTP credentials, you will be able to provision new SMTP credentials. The SMTP password will only show one time during user provisioning for security.\nSMTP user credential and SMTP server detail Which SMTP Port to Use Short version: It depends on what your client can support.\nLong version:\nMailgun indicates the following ports are open: 25, 587, and 465 (SSL/TLS)\nPort 25 is not recommended because many enterprise firewalls deny it for security reasons.\nPort 587 is the STARTTLS version. In short, client first initiates a plain TCP connection. When server advises that it supports STARTTLS, client will respond with STARTTLS and proceed with reconnecting to the same 587 port, this time with TLS handshake. The protocol is such that server will deny TLS handshakes in the first place if client did not complete an initial TCP session and ask for STARTTLS.\nPort 465 is the \u0026ldquo;straight-in\u0026rdquo; SSL/TLS in that it deals with every session as TLS.\nPostfix Postfix can be easily installed on almost any Linux distro. On Ubuntu, run:\nsudo apt install libsasl2-modules postfix ; Then create a file /etc/postfix/sasl/sasl_passwd_mailgun. Copy the following line into the file, replacing your-username@mg.yourdomain.com:your-password-here with your actual SMTP credential.\n[smtp.mailgun.org]:587 your-username@mg.yourdomain.com:your-password-here Run:\nsudo postmap /etc/postfix/sasl/sasl_passwd_mailgun ; If all went well, you have a new file as /etc/postfix/sasl/sasl_passwd_mailgun.db. It\u0026rsquo;s what Postfix will be reading from for Mailgun SMTP authentication.\nSecure the passwords Both sasl_passwd_mailgun and sasl_passwd_mailgun.db contain plain text of your app password, to secure the access, you need to:\nsudo chown root:root /etc/postfix/sasl/sasl_passwd_mailgun /etc/postfix/sasl/sasl_passwd_mailgun.db ; sudo chmod 0600 /etc/postfix/sasl/sasl_passwd_mailgun /etc/postfix/sasl/sasl_passwd_mailgun.db ; Setting up Postfix\u0026rsquo;s main configuration file is /etc/postfix/main.cf. See the following my own configuration for your reference.\nmyhostname indicates the hostname of the Postfix\nmynetworks is the IP CIDRs that are allowed to relay mails through Postfix. It should only contain your local networks\nsmtp_sasl_password_maps is where the sasl_passwd file is located\nrelayhost is the server name and port. Replace it with[smtp.mailgun.org]:587\nEdit the values respectively.\nFor security reason, do not expose Postfix service to anywhere outside of your trusted networks.\nIn the example above, I am allowing anonymous SMTP authentication from mynetworks. The argument can be made that attackers will be after more valuable targets than fiddling with Postfix relay service provided the local network is compromised.\nTesting and Troubleshooting When config file is done, reload Postfix configurations without having to restart the service:\nsudo postfix reload ; For SMTP testing, I personally like to use swaks, a purpose-built Perl script.\nWith swaks installed, it will be just a one-liner to send out a testing mail:\n./swaks \\ --server \u0026lt;your-postfix-server-ip\u0026gt; \\ --port 25 \\ --from yourname@yourdomain.com \\ --to anymailbox@wherever.com \\ --h-Subject: \u0026#34;Hello world\u0026#34; \\ --body \u0026#39;Testing some Mailgun awesomness!\u0026#39; If you suspect something went wrong, go check the logs:\nsudo tail -f /var/log/syslog ; # and sudo tail -f /var/log/mail.err ; That\u0026rsquo;s it!\nOn a side note, there are lots of tweaks can be done on the Postfix end depending on what you need to achieve, but this simple guide should be able to get you started quickly.\nLet me know if you have any questions or comments.\n","permalink":"https://davidxiao.me/posts/setting-up-a-mailgun-relay-with-postfix/","summary":"Set up a Postfix server on local network that relays outgoing emails through Mailgun SMTP.","title":"Setting Up a Mailgun Relay With Postfix"},{"content":"You would need the following to set up a gmail relay on Postfix:\nAn app password obtained from your Gmail account. It will be effectively your Gmail SMTP password.\nAdmin privilege on a Linux box to set up Postfix server. Outbound internet access is required.\nGoogle App Password App passwords let you sign in to your Google Account from apps that don\u0026rsquo;t support 2-Step Verification. This post from Google explains how to obtain an app password.\nSign up for an App Password on your Google Account Essentially, first you need to go to Google Account and enable two Factor Authentication. Without enabling 2FA, app password option is disabled on Google.\nThen click on \u0026lsquo;Security\u0026rsquo; from the left, click on \u0026lsquo;App passwords\u0026rsquo; under \u0026lsquo;Signing in to Google\u0026rsquo; and click on \u0026lsquo;Generate\u0026rsquo; button, here you go. Copy the password as we will need it later.\nThe rest of the post is using Ubuntu as an example, but the approach should work for most recent Linux distro in general.\nPostfix Postfix can be easily installed on almost any Linux distro. On Ubuntu, run:\nsudo apt install libsasl2-modules postfix ; Then create a file /etc/postfix/sasl/sasl_passwd. Copy the following line into the file, replacing your@gmail.com with your actual Gmail; abcdefghijk123 with your actual app password.\n[smtp.gmail.com]:587 your@gmail.com:abcdefghijk123 Run:\nsudo postmap /etc/postfix/sasl/sasl_passwd ; If all went well, you have a new file as /etc/postfix/sasl/sasl_passwd.db. It\u0026rsquo;s what Postfix will be reading from for Gmail SMTP authentication.\nSecure the passwords Both sasl_passwd and sasl_passwd.db contain plain text of your app password, to secure the access, you need to:\nsudo chown root:root /etc/postfix/sasl/sasl_passwd /etc/postfix/sasl/sasl_passwd.db ; sudo chmod 0600 /etc/postfix/sasl/sasl_passwd /etc/postfix/sasl/sasl_passwd.db ; Setting up Postfix\u0026rsquo;s main configuration file is /etc/postfix/main.cf. See the following my own configuration for your reference.\nmyhostname indicates the hostname of the Postfix\nmynetworks is the IP CIDRs that are allowed to relay mails through Postfix. It should only contain your local networks\nsmtp_sasl_password_maps is where the sasl_passwd file is located\nrelayhost is the server name and port\nEdit the values respectively.\nFor security reason, do not expose Postfix service to anywhere outside of your trusted networks.\nIn the example above, I am allowing anonymous SMTP authentication from mynetworks. The argument can be made that attackers will be after more valuable targets than fiddling with Postfix relay service provided the local network is compromised.\nTesting and Troubleshooting When config file is done, reload Postfix configurations without having to restart the service:\nsudo postfix reload ; For SMTP testing, I personally like to use swaks, a purpose-built Perl script.\nWith swaks installed, it will be just a one-liner to send out a testing mail:\n./swaks \\ --server \u0026lt;your-postfix-server-ip\u0026gt; \\ --port 25 \\ --from yourmail@gmail.com \\ --to anymailbox@wherever.com \\ --h-Subject: \u0026#34;Hello world\u0026#34; \\ --body \u0026#39;Testing some Mailgun awesomness!\u0026#39; If you suspect something went wrong, go check the logs:\nsudo tail -f /var/log/syslog ; # and sudo tail -f /var/log/mail.err ; That\u0026rsquo;s it!\nOn a side note, there are lots of tweaks can be done on the Postfix end depending on what you need to achieve, but this simple guide should be able to get you started quickly.\nLet me know if you have any questions or comments. On my post I will be discussing setting up a Mailgun relay. Stay tuned!\n","permalink":"https://davidxiao.me/posts/setting-up-a-gmail-relay-with-postfix/","summary":"Set up Postfix that relays outgoing emails through your personal Gmail account.","title":"Setting up a Gmail Relay With Postfix"},{"content":"TL;DR\nInstall and configure Proxmox as KVM hypervisor.\nPlaying [三國志XIII](https://store.steampowered.com/app/363150/ROMANCE_OF_THE_THREE_KINGDOMS_XIII/) on Win10 guest through Remote Desktop ","permalink":"https://davidxiao.me/posts/my-homelab-setup-from-hardware-to-kvm-part-2/","summary":"Install and configure Proxmox as KVM hypervisor.","title":"My Homelab Setup From Hardware to KVM - Part 2"},{"content":"TL;DR\nI plan on writing a few posts about my Homelab project. This is the first one focusing on the hardware spec and networking.\nLast year I decided to set up a homelab to learn technologies, to run some services for my own use and more importantly, to have fun.\nThere is a budget and a few high level requirements on the infrastructure side I worked on:\nBe able to run a hypervisor that supports both Linux and Windows virtual machine\nOne single host. No clustering\nUse open source as long as the it\u0026rsquo;s mature enough\nEasy to manage and operate\nCost effective\nNo dedicated networking hardware\nI plan on writing a few posts with respect to the HW spec I chose, the architecture decisions I made along the way and how I built the whole lab. This is the first post.\nHardware Item Spec Notes CPU AMD Ryzen 7 3700x 65W TDP. 8-core, 16-thread, 7nm Motherboard ASUS PRIME X470-PRO See my notes Memory 16gb x2 ECC UDIMM Unbuffered ECC System Storage 250GB SATA SSD See my notes Data Storage 4TB HDD As temporary data storage Networking On-board Gigabit Ethernet See my notes Graphics Card #1 Asus GeForce GTX 1660 Super model: TUF-GTX1660S-O6G-GAMING See my notes Graphics Card #2 MSI GeForce GT 710 1GB Fanless design Accessories such as case and power supply are not listed but included in the total cost.\nTotal Cost The hardware cost of my homelab is around CA $1,400 when I bought them back in December 2019.\nMotherboard If you need to use ECC memory, you have to pick a motherboard that support ECC. In consumer grade motherboard market, look no further than ASUS and ASROCK. Those two are know for building consumer grade ECC-enabled motherboards.\nIf you need to manage your homelab hypervisor from a remote location, and be able to access it when troubleshooting something low level, e.g. when hypervisor is crashed or configuring BIOS, you have two choices: Phyical Access or IPMI.\nPhyical Access It means hook up a display, a keyboard and a mouse to your server and sit in front of it when troubleshooting.\nIPMI With IPMI, tasks such as power on/off the machine, configure BIOS settings and monitor console output all can be done from remote wia a web portal. Motherboards that support IPMI typically provide a dedicated Ethernet port for IPMI management. Learn more about IPMI here.\nASROCK has some pretty good IPMI-enabled workstartion motherboards. For AMD cpu, take a look at ASROCK X470D4U or x470d4u2-2t with 10G networking. There are also x570d4i-2t to be released in 2020.\nStorage To have flexability and speed, I need two kinds of storage. Direct Attached Storage (DAS) for hypervisor and guest OS images and Network Attached Storage (NAS) for data such as video and photos.\nSSD as DAS M2.SSD is my primary choice for DAS since it\u0026rsquo;s much faster than SATA SSD both on paper and in reality. So I bought a XPG SX8800 PRO 1TB and installed it on my server.\nUnfortunately my server started freezing up randomly every a few hours to a few days. Troubleshooting proved to be one hell of an effort when my server is in the basement without a dedicated display. I\u0026rsquo;d have to take an external display from my home office down to the basement for troubleshooting. IPMI could have saved my day.\nError messages showing I/O error on nvme0n1 It took some time to reproduce the problem and troubleshoot the issues. Finally I was able to conclude that the controller chip that XPG SX8800 PRO uses may have compatibility issue with PRIME X470-PRO. After I replaced the M2.SSD with an old SATA SSD that I have, the server runs smoothly without any issues.\nSynology NAS I have a Synology DS416play. It\u0026rsquo;s a little home NAS with 4 bays and 1GB memory. I\u0026rsquo;ve since upgraded the memory to 8G and have it fitted with 4x10G HDD. In terms of usable space it has around 27T with one disk fault tolerance which is more than enough for me.\nThe downside of this NAS is it only has Gigabit Ethernet. If you think 10G is appealing, I\u0026rsquo;m totally with you. QNAP is offering a number of 10G ready NAS. Check it out here for detail.\nNetworking My home network topology is rather simple. Both homlab server and NAS are connected to the internet router. The router provides basic functionalities such as IP management and port mapping.\nHome Network Diagram Given my home network cables are all Cat5 supporting up to 1 Gigabit, I have to stay with 1G network and the on-board Gigabit Ethernet works well in my setup.\nIf you are considering 10G, you would need to make sure all critial devices on the network (e.g. router, NAS, Wifi etc.) are capable of dealing with the throughput, since any performance bottleneck can be the low watermark that undermines the overall network performance.\nGPU Passthrough The KVM hypervisor supports passing graphics card(s) onto dedicated guest OS.\nOn my setup, GTX1660 is the primary card for tasks such as streaming encoding/decoding whereas the GT710 is pass-through to a Win10 guest. This setup allows me to play a few not-so-demanding games on the Win10 through remote desktop.\nPlaying [三國志XIII](https://store.steampowered.com/app/363150/ROMANCE_OF_THE_THREE_KINGDOMS_XIII/) on Win10 guest through Remote Desktop I will discuss more on GPU passthrough in the next post.\n","permalink":"https://davidxiao.me/posts/my-homelab-setup-from-hardware-to-kvm-part-1/","summary":"I plan on writing a few posts about my Homelab project. This is the first one focusing on the hardware spec and networking","title":"My Homelab Setup From Hardware to KVM - Part 1"},{"content":"TL;DR\nThis post will show you how to build and deploy a voice-activated app on Google Cloud in 10 minutes. The app will respond to your voice command and even play a piece of pre-defined music for you! Not a bad way to celebrate someone\u0026rsquo;s birthday eh? :-)\nCreate a New Project on Google Actions Console The app is built on Google Cloud using Google Actions and Dialogflow. If you don\u0026rsquo;t have a Google Actions account yet, click here to create a new one. It\u0026rsquo;s free.\nWhen the account is ready, go ahead and create a new project. Google Actions allows you to add Actions to any existing GCP projects, but we will create being creating a new project for simplicity.\nCreate a new project Specify a Catchy Name for Your App Determine an activation phrase for the app so that every time when you say it, your app will be activated on the Google Home device.\nGo to the project dashboard, click on \u0026ldquo;Quick setup\u0026rdquo; followed by \u0026ldquo;Decide how your action is invoked\u0026rdquo; and put the app name here. It may reject your phrase name if it\u0026rsquo;s considered too common or ambiguous, e.g. \u0026ldquo;Hello\u0026rdquo; is not a good choice here.\nLet\u0026rsquo;s say the app is called \u0026ldquo;Hello Jukebox\u0026rdquo;.\nSpecify a catchy name for your app Add Actions to Your App An app is only as smart as what it\u0026rsquo;s taught. There is no secret sauce in here. This app will respond to voice commands and act accordingly based on the \u0026ldquo;Intent\u0026rdquo;. \u0026ldquo;Intent\u0026rdquo; is a Google term referring to a combination of voice command and its corresponding response.\nWithin an app, developers can create as many intents as they want so long as no intent is stepping on one another\u0026rsquo;s toes. For example, trying to create two separate intents both responding to the same command \u0026ldquo;what is my favoriate color\u0026rdquo; would be confusing to begin with.\nWithin an intent, developer can decide on the kind of response it needs to provide: it can be as simple as having Google Home say some words or be more complicated with custom logic.\nAdd a new Action to your app Use Case 1: Simple Voice Commands and Text Responses Scroll down the Actions dashboard until the Fulfillment section, click on \u0026ldquo;Edit in Dialogflow\u0026rdquo; and click on the Intents. Start adding intents.\nEdit in Dialogflow Add new Intent For example, you may want to create an intent called \u0026ldquo;special-intent\u0026rdquo;, add \u0026ldquo;Do you know why today is so special\u0026rdquo; as voice command and add \u0026ldquo;Of course I know, David\u0026rdquo; as text responses to the intent. Those are what you would say to the app and what the app will say back respectively.\nA list of intents I added Text Responses Use Case 2: Implement custom logic using Cloud Function The second use case is enabling webhook in an intent and developing a handler for it.\nThis approach allows you to implement custom logic for an intent. GCP supports either running your custom code on a Cloud Function or calling an external web service you specify.\nText Responses I will use the Cloud Function way for this example since we don\u0026rsquo;t need to worry about resource or storage thanks to its serverless nature.\nFirst, you need to enable \u0026ldquo;Webhook\u0026rdquo; on the intent that needs to have custom logic. Second, click on the Fulfillment on the left navbar and enable Inline editor. Last, copy and paste the following code example and click on save. That\u0026rsquo;s it.\nThe code example will first say something then play an audio clip. If you need to play something else, e.g. a peronalized audio clip, you can replace the URL with your own thing, but then you would have to deal with access control.\nTesting Your New App on Simulator Click on \u0026ldquo;Google Assistant\u0026rdquo; on the right bar to open the Simulator on Google Actions. From there you can tinker with your app until you\u0026rsquo;re satisfied with it.\nText Responses Deploying to your Google Home Device Making your app available on Google Assistant Actions Portal sounds like a great idea, however the releasing process could take some time as Google needs to review and approve your app first. My post promised you a 10-minute ride, so let\u0026rsquo;s take a detour to get your freshly baked app deployed onto your own Google Home device as Alpha testing thus bypassing the approval steps.\nOn Actions console, click on \u0026ldquo;Deploy\u0026rdquo;, choose \u0026ldquo;Alpha\u0026rdquo;, click on \u0026ldquo;Manage Alpha Testers\u0026rdquo;, and add your own Google Home device account email here. You can then switch to your Google Home account and use the opt-in link received on the invitation email to accept the invite.\nMake an Alpha Deployment When it\u0026rsquo;s ready, click on \u0026ldquo;Create a release\u0026rdquo;, and wait it to complete. It can take a few minutes.\nCongrats! You\u0026rsquo;ve just built and deployed your first Google Home app in 10 minutes! It\u0026rsquo;s time to say out loud \u0026ldquo;Hey Google, Talk to your_app_name_here\u0026rdquo; and see what will happen next :)\n","permalink":"https://davidxiao.me/posts/building-your-first-app-on-google-home/","summary":"Build and deploy a voice-activated app on Google Home in 10 minutes. The app will respond to your voice command and even play a piece of pre-defined music for you! Not a bad way to celebrate someone\u0026rsquo;s birthday eh? :-)","title":"Build Your First App on Google Home in 10 minutes"},{"content":"A little teaser here: Raspberry Pi is not really ediable :-)\nSo I have a Raspberry Pi Zero W for about one year now.\nIt runs a few software. One of them is called Pi-Star. It\u0026rsquo;s an open source toolkit for digial voice over amateur radio. Find more detail about Pi-Star here.\nAmateur radio is one of my hobbies. Figuring out how digital voice modes work in the amateur radio world took some time for me but it was rewarding. At the end of the day, listening to hams talking about their passions from all over the world on my little handheld radio feels amazing.\nPi-Star works very well in my setup, so when I found out Pi-Star does not display my public IP address on OLED, I decided to write some code for it.\nHere\u0026rsquo;s what it looks like.\nOLED display showing my Public IP 1. Download the Toolchain with Extra Libs and Headers I use Ubuntu 20.04 on my homelab as cross compiling platform, but any recent Linux distro should work.\nMy toolchain is a fork from the original toolchain with extra libs and headers for compiling MMDVMHost.\n$ cd ~/code $ git clone https://github.com/davxiao/tools.git 2. Code is the Easy Part \u0026lt;3 Pi-Star consists of a few components including a PHP frontend and a few programs as backend for data exchange over various digital voice networks.\nMMDVMHost is the program that interfaces to the digital voice modem (MMDVM) on one side, and a suitable network on the other. It\u0026rsquo;s written in standard C++ with dependencies to external libs such as ArduiPi_OLED.\nFor my purpose, I added some code in CNetworkinfo class and COLED class. If you don\u0026rsquo;t know much about C++, no worries, just download all source code from my github repo.\nDownload my repo:\n$ cd ~/code $ git clone https://github.com/davxiao/MMDVMHost.git 3. Prep for the Cross Compilation In MMDVMHost/cmake/CrossCompile.cmake, you wanted to update toolchain paths so that CMake will be able to generate correct Makefile afterwards.\nWhen it\u0026rsquo;s done, run:\n$ cd MMDVMHost/cmake $ cmake ../ -DCMAKE_TOOLCHAIN_FILE=./CrossCompile.cmake If you see warnings like this, try delete CMakeCache.txt and run the cmake command again.\nCMake Warning: Manually-specified variables were not used by the project: CMAKE_TOOLCHAIN_FILE 4. Last Step When Cmake is done, a Makefile was generated under the same directory. Run the following in the same directory:\n$ make ; When the complication is complete, you should see MMDVMHost in the cmake/ directory. You may wish to run file ./MMDVMHost to confirm the target platform is ARM as opposed to amd64. Here\u0026rsquo;s my output:\n./MMDVMHost: ELF 32-bit LSB executable, ARM, EABI5 version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-armhf.so.3, for GNU/Linux 2.6.32, with debug_info, not stripped Congrats! You\u0026rsquo;ve built MMDVMHost using cross compilcation.\nDeployment You can skip this section if you have set up your own deployment pipeline.\nOn my homelab, I mount the same Samba share folder on both the Ubuntu and the Pi, then just copy the MMDVMHost over. Before replacing the MMDVMHost, you need to confirm the SD card is mounted in R/W mode and MMDVMHost service is stopped. Make a backup of the original MMDVMHost is also a good idea.\n$ rpi-rw ; $ sudo systemctl stop mmdvmhost ; $ sudo systemctl stop mmdvmhost.timer ; $ sudo cp ~/nas-dir/MMDVMHost.build /usr/local/bin/MMDVMHost ; $ sudo systemctl start mmdvmhost ; $ sudo systemctl start mmdvmhost.timer ; If you experience mount error(115): Operation now in progress when mounting CIFS on Pi, it might be caused by the iptable rules set by Pi-Star.\nTo troubleshooting the issue, run the following commands on Pi-Star and see if mount works.\nNo worries, the following changes do not persist between restarts.\nsudo iptables -P INPUT ACCEPT; sudo iptables -P FORWARD ACCEPT; sudo iptables -P OUTPUT ACCEPT; sudo iptables -F; sudo iptables -X ; sudo iptables -nvL ; ","permalink":"https://davidxiao.me/posts/cross-compile-mmdvmhost-on-ubuntu/","summary":"A quick \u0026ldquo;How-to\u0026rdquo; guide to compiling raspberry-pi programs on Ubuntu 20.04 using pi toolchain. This post takes Pi Zero W (BCM2708 chip) as an example but the approach would be applicable to other Pi systems.","title":"Cross Compile MMDVMHost on Ubuntu"},{"content":"In this post I will talk about how I built this website from the ground up using Hugo as site generator and Firebase as hosting provider. I wish you\u0026rsquo;ll find it helpful.\nWeb development has come a long way. Back in the late 90\u0026rsquo;s when I first came across Internet, anyone who knows anything about HTML would be considered very technical.\nFast forward into 2020, social media and smart phone is ubiquitous, web hosting services such as Wix and Wordpress have made content creation possible for anyone who wishes to create a website without much headache.\nWhy should you building a website from the ground up as opposed to using a web hosting service?\nMaybe it\u0026rsquo;s for you Building a website with a static site generator is not for everyone. George Cushen who is Hugo theme Academic\u0026rsquo;s main contributor once put:\n\u0026hellip;(it would) require a basic understanding of using the command line in the Terminal (Mac/Linux) or Command Prompt (Windows) app on your computer. If you are not interested in this, perhaps this is not for you\u0026hellip;\nOriginal post\nBut if going beyond content creation and learning more about building a website that is lightweight and secure, cost effective yet no vendor lock-in is your thing, then let\u0026rsquo;s dive right in.\nOverview First and foremost, let\u0026rsquo;s take a look on what will be covered in the post.\nRegister a domain name.\nSet up Hugo as static site generator.\nUse a kick-starter theme to get going quickly.\nDeploy the site to Firebase.\nAdd your domain to the new site.\n1. Register a domain name It\u0026rsquo;s pretty straightforward. You come up with a great domain name. It is better to be concise and easy to remember. Be creative \u0026lt;3.\nThen complete the domain registration on any Domain Name Registrar you prefer. I use Google Domains but there are other good choices such as Namecheap and GoDaddy.\nRegister a domain name on Google Domains June 2020: For a few reasons, one being Google domains does not provide API for dynamic DNS (A records), I\u0026rsquo;ve changed my domain name registrar to Namecheap, my DNS provider to Cloudflare].\n2. Setting up Hugo Hugo is a static site generator. There are many other site generators, I picked Hugo for a few reasons:\nIt\u0026rsquo;s open source and backed by an active developer team and support community. It\u0026rsquo;s a monolithic program with no external dependencies. It\u0026rsquo;s production ready. In a nutshell, Hugo renders content into HTML files and uploads the files onto your choice of hosting provider. Your content is what you write as content creator. Hugo takes content files written in Markdown (.md), a format that is intended to be used by technical and non-technical writers alike. Since inception, Markdown has become the de facto format in content creation and blogging.\nIf you need to learn about Markdown syntax, there are good guides such as Markdown Guide.\nInstalling Hugo On macOS, I recommend using a package manager such as Homebrew to manage third-party packages. With Homebrew installed, to install Hugo, just run:\n$ brew install hugo ; All set. In case you need to check which Hugo version is installed, run hugo version. On my mac it returns Hugo Static Site Generator v0.70.0/extended darwin/amd64\nFor installing Hugo on Windows or Linux, refer to Hugo\u0026rsquo;s documentation.\n3. Use a kick-starter theme Hugo has built-in theme mechanism that allows developers to quickly run a theme and see the results. It also provides all the necessary building blocks for user to personalize the theme. There are many themes available on Hugo, for my own website I use Academic Theme. It also comes with a academic-kickstart repo on github for teasers.\nThe easy way to get started is to just fork the repo, download the code and run it.\nFork the kickstart into your own repo Download the code:\n$ git clone \u0026lt;replace-it-with-your-own-repo-url\u0026gt; ; $ cd \u0026lt;your-repo-root-dir\u0026gt; ; $ git submodule update --init --recursive ; # get the latest Academic theme Run Hugo to serve the test site:\n$ hugo server -D ; Now visit http://127.0.0.1:1313/ on your web browser and you should see the homepage.\nCongrats! You\u0026rsquo;ve got your first Hugo website up and running on your local environment!\nHugo only binds to local network address for security by default. If you need to test the site on another computer in your local network, run:\n$ hugo server -D --bind=0.0.0.0 ; 4. Deploy the new site to Firebase There are many out there: Google Firebase, GitHub Pages, Netlify and AWS Amplify to name a few. Each one has its own offering. I picked Firebase as my hosting provider because they seem to offer a bit more on their free tier.\nFirst, install Firebase CLI and (optional) Google Cloud SDK CLI.\nFirebase CLI. The recommended way is to run npm i -g firebase-tools ; See its github repo for more detail. If you don\u0026rsquo;t have npm installed yet, run: brew install node ;. npm will be installed alongside node.js.\nGoogle Cloud SDK CLI. Run brew cask install google-cloud-sdk ;\nNext, go to Firebase to set up an account and create a new Firebase project. Make sure it uses the default free tier plan which is called Spark. Be noted you need to specify GCP resource location under Project Overview in Firebase Dashboard after project is created. The location can not be changed afterwards, so choose something close to you would be wise.\nSpecify resource location under Project Overview in Firebase Dashboard Set up service account authenication on Firebase Authenticating with a service account allows you to use Firebase CLI to manage your Firebase project. Google has provided a step by step guide here.\nWhen authentication is set up, go to your project root directory and follow the recorded screens below to initialize firebase and deploy the very first version of your site onto firebase.\nCongratulations! Your website is online! You should find your Hosting URL at the end of the Firebase deploy output, it\u0026rsquo;s typically something like: https://your-project-id.web.app\n5. Add your domain to the new site Go to Hosting on Firebase, click on \u0026ldquo;Add custom domain\u0026rdquo;. Typically you wanted to add your root domain name and a sub domain name such as \u0026ldquo;www\u0026rdquo;. For example, I added \u0026ldquo;davidxiao.me\u0026rdquo; for my website and added another entry for redirecting www.davidxiao.me to davidxiao.me\nWhen it\u0026rsquo;s complete, you will be able to visit your website by your custom domain regsitered on step 1.\nAdd custom domain to your website ","permalink":"https://davidxiao.me/posts/building-a-website-in-2020/","summary":"If you are interested in building a personal blog site in a cost efficent way, this post discusses about creating a website from the ground up with minimal vendor dependency and high levels of autonomy using Hugo and Firebase.","title":"Building a website in 2020"}]